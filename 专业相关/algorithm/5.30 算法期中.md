Chapter 1-4
可能会有伪代码
# Chapter 1
## Lecture 1
### Several commonly used models: 
→Random Access Machine (RAM): Measures time and space, typical for traditional serial computers. 
→Parallel Random Access Machine (PRAM): Considers parallel time, number of processors, and read/write constraints. 
→Message Passing Model: Evaluates communication costs (number of messages) and computational costs. 
→Turing Machine: Focuses on time and space, representing an abstract theoretical model
### Algorithm analysis evaluates performance based on models and metrics such as: 
→Input Size: The length or amount of input data. 
→Running Time: The number of primitive operations executed. We typically focus on the worst-case running time unless stated otherwise. 
→Order of Growth: We analyze the growth rate of running time by considering only the leading term in a time formula (e.g., in $𝑛^2$ + 100𝑛 + 5000, the leading term is $𝑛^2$).
→Theoretical analysis often uses asymptotic measures (Big O, Ω, Θ) to estimate performance for large inputs.

### Steps for developing an Algorithm
1. Define the problem: State the problem you are trying to solve in clear and concise terms. 
2. List the inputs (Information needed to solve the problem) and the outputs (what the algorithm will produce as a result). 
3. Describe the steps needed to convert or manipulate the inputs to produce the outputs. Start at a high level first and keep refining the steps until they are effectively computable operations. 
4. Test the algorithm: choose datasets and verify that your algorithm works.

### Algorithm Phases
Design $\Rightarrow$ Analyse $\Rightarrow$ Implement $\Rightarrow$ Experiment

### Representing Algorithms

1. In terms of Natural Language. 
2. In terms of Formal Programming Language. 
3. In terms of Pseudocode. 
4. Flowcharts.

## Lecture 2

### Steps for finding Big-O 
1. Figure out what the input is and what 𝑛 represents. 
2. Express the maximum number of operations, the algorithm performs in terms of 𝑛. 
3. Eliminate all excluding the highest order terms. 
4. Remove all the constant factors.

>例1

$$f(n)=4n^2 + n +12$$

Solution:
- Big O for this function is: $O(n^2)$
- To prove this, we assume that:
$$\begin{align}
C &\ge 1 \\
C &= 5
\end{align}$$
$$\begin{align}
5n^2 &\ge 4n^2+n+12 \\
5n^2 &-4n^2-n-12=0\\
n^2 & -n-12=0 \\
(n - &4)(n+3)=0
\end{align}$$
- when $n_0$ = 4, both equations are equal
- when $n_0 > 4, 5n^2 > 4n^2 +n +12$

>例2

```
void fun(int n){
   int i,j,k,count=0;
   for(i = n/2;i<=n; i++)
      for(j = 1; j<= n;2*j)
         for(k = 1;k<=n;k++)
            count++;
}
```
1. loop i
	Iteration 1        i=n/2
	Iteration 2        i=1+n/2
	Iteration 3        i=2+n/2
	Iteration 4        i=3+n/2
	Iteration k        i=k-1+n/2
k-1+n/2 = n $\Rightarrow$ k=n-n/2 = n/2+1
k = n/2 + 1, by removing the constant 1 and 1/2:
The time complexity of the first loop is **O(n)**
2. loop j
	Iteration 1        j=1
	Iteration 2        j=2
	Iteration 3        j=4
	Iteration 4        j=8
	Iteration k        j=$2^{k-1}$
$2^{k-1}$ = n $\Rightarrow$ k=$\log{n}$ +1, by removing the constant 1:
The time complexity of the second loop is **O(logn)**
3. loop k
	Iteration 1        k=1
	Iteration 2        k=2
	Iteration 3        k=3
	Iteration 4        k=4
	Iteration m       k=m
m = n
The time complexity of the third loop is **O(n)**

整体时间复杂度就是$O(n^2\log{n})$

# Chapter 2
## Part 1
### Probabilistic analysis definition
- 通常用于evaluate算法运行时间，有些算法只能用概率分析，不能只靠deterministic确定性方法（比如输入数据是随机生成的，比如算法本身使用了随机选择，像随机化快速排序这种）
- Instead of只考虑最好情况和最坏情况，probabilistic analysis considers all cases by: 
	1. Assessing the probability distribution of each case.  计算概率 P(X)
	2. Calculating the expected runtime based on this distribution. 期望运行时间 E(X) 
	 - This method can also be used to analyze various quantities.  某次操作次数等
	 - An example is estimating the hiring cost in the Hire-Assistant procedure. 具体例子解释various quantities内容
### Hiring problem
>You are using an employment agency to hire a new assistant for your company. The agency works in such a way that it sends you ==one candidate per day.== You then interview the candidate after which you must immediately ==decide whether to hire== that person. Also, if you hire that person, you must ==fire your current office assistant==. Even if it is someone you have hired recently you must fire them before you hire a new assistant.

#### There are a few requirements to be fulfilled which are:

1. 你任何时候都想拥有截止当前最好的候选人
2. 只要觉得面试的候选人更好，必须雇用这个候选人同时解雇之前的当前的助理
3. 第一位候选人总是被雇用
#### Here are a few points to be considered for the Cost Model:
1. 我们不考虑雇用助手的running time
2. We must determine the total cost of hiring the best candidate.
3. If 𝑛 candidates are interviewed and 𝑚 candidates are hired, then the cost would be $𝑛𝑐_𝑖 + 𝑚𝑐_ℎ$, 其中 $nc_i$ 是我们必须付出的成本，但 $mc_h$ 是可以控制的，m随着候选人等待顺序的改变而改变
#### Analysis
##### First
Cost to interview candidate is $𝑐_𝑖$. 
Cost to hire a candidate is $𝑐_h$.
Assume $𝑐_𝑖$ is much less than $𝑐_h$. 
Total cost: $O(𝑐_𝑖 \times n + c_h \times m)$, where 𝑚=number of hirings

```pseudocode
Hire-Assistant(n) 

1 best = 0   // fictional虚构的 least qualified candidate 
2 for i= 1 to n 
3 interview candidate i       // paying cost ci 
4 if candidate i is better than candidate best 
5 best = i 
6 hire candidate i// paying cost ch
```

##### Second
那么，可以通过检查每个元素和改变the winner 𝑚 times 来find the maximum or minimum in a sequence.

$$cost=
\begin{cases}
\text{Best-case} : &O(c_in) \text{, when the agency sends us the best applicant on the first day}\\ 
\text{Worst-case} : &O(c_hn) \text{ when }c_h > c_i \text{, if each candidate is better than all who came before}\\
\text{Average-case} : & \text{Assume applicants come in random order. Each permutation of applicants is equally likely.}\\
\end{cases}$$

We can use a randomized algorithm to bring more control and ensure randomness in the sequence.


### Randomized Algorithms
- We can force all inputs to be equally likely by randomizing the input, ensuring a more balanced analysis
- An algorithm is randomized when part of its behavior is determined by a random number generator
- 对于hiring problem来说，我们需要去选择面试顺序，那么我们决定去以随机顺序面试他们，thus making the average case the expected value
>In the hire-assistant problem, we can: 
> Randomly permute the list of candidates first. 
> Then, run the hiring algorithm
> This guarantees that for any input, the expected number of hires will be approximately $\ln{𝑛} +𝑂(1)$

### Indicator Random Variables (IRV).
- Indicator Random Variables (IRV) are a useful tool in probabilistic analysis, especially when analyzing dependent events
- definition:
	An Indicator Random Variable is a variable that indicates whether a specific event has occurred. 
	It takes a value of: 
		1 if the event happens
		0 if the event does not happen.
- The expected value of $𝐼_𝐴$ is the probability that event 𝐴 occurs: 𝐸$(𝐼_𝐴)$ =𝑃(𝐴)

>例1 Hiring Problem using IRV

Let’s define $𝐼_𝑖$ as the Indicator Random Variable for hiring candidate 𝑖. 
$𝐼_𝑖$ =1 if candidate 𝑖 is hired, and 0 otherwise.
The expected number of hires is: $$E[\text{number of hires}] = E [\sum\limits_{i=1}^{n}I_i] = \sum\limits_{i=1}^{n}E[I_i]$$
Since $$𝐸[𝐼_𝑖] = 𝑃 \text{ (hiring candidate 𝑖)} = \frac{1}{i}$$, the expected value gives us the average number of hires across all candidates
**Step 3: Total Expected Number of Hires**
$$\begin{align}
E[X] &= 1 + \frac{1}{2} + \frac{1}{3} + \dots + \frac{1}{n} \\
&= \sum\limits_{k=1}^{n}\frac{1}{k}   \text{ (harmonic series)} \\
&=\ln{n} + O(1)
\end{align}$$
where $\ln{n}$ is the natural logarithm of n, and O(1) is a constant.

>例2 flipping a coin

分两类：
1. 投掷1次硬币
only two possible outcomes: heads or tails.
**Step 1: Define the Indicator Random Variable (IRV)**

Let’s define an indicator random variable 𝑋 for the event "the coin lands heads." The IRV will be defined as:
$$X = 
\begin{cases}
1, &\text{if head occurs}\\
0, &\text{if tail occurs}
\end{cases}$$

**Step 2: Compute the Expected Value**
$$E[X] = 1 \times P(heads) + 0 \times P(tails) = \frac{1}{2}$$
2. 投掷 n次硬币
**Step 1: Define the Indicator Random Variable (IRV)**

Let’s define an indicator random variable $𝑋_𝑖$ for each flip 𝑖 (where 𝑖 = 1,2,…,𝑛) to indicate whether heads occurs on the 𝑖 − 𝑡ℎ flip:
$$X_i = 
\begin{cases}
1, &\text{if i-th flip is heads}\\
0, &\text{if i-th flip is tails}
\end{cases}$$
**Step 2: Total Number of Heads**
$$X=X_1 + X_2 + \dots +X_n$$

**Step 3: Compute the Expected Value**
$$\begin{align}
E[X] &= E[X_1 + X_2 + \dots +X_n]=E[X_1] + E[X_2] + \dots + E[X_n]\\
&=\frac{1}{2} + \frac{1}{2} + \dots +\frac{1}{2}\\
&=\frac{n}{2}
\end{align}$$

>例3 Using IRVs in Quicksort to Analyze Expected Comparisons 

In Quicksort, the key operation is comparing elements to a chosen pivot to partition the array into subarrays. The number of comparisons made directly affects the algorithm's performance.
Suppose we are sorting an array of 𝒏 distinct elements using Quicksort.

**Step 1: Define the Indicator Random Variable (IRV)**
Let’s define an Indicator Random Variable $𝐼_{𝑖𝑗}$ for each pair of elements 𝑖 and 𝑗, where 𝑖 < 𝑗: 

$𝐼_{𝑖𝑗}$ = 1 if elements 𝑖 and 𝑗 are compared at some point during the execution of Quicksort. 
$𝐼_{𝑖𝑗}$ = 0 if elements 𝑖 and 𝑗 are never compared.

**Step 2: When Are Two Elements Compared?**
Two elements i and j are compared if and only if one of them is chosen as a pivot from the subarray containing both i and j.
因此，两元素被比较的概率就是两人在同一subarray里且一个被选为pivot的概率
由于每个元素随机成为枢纽，$$P(\text{i and j are compared}) = \frac{2}{j-i+1}$$
Thus, $$E[I_{ij}] = \frac{2}{j-i+1}$$
**Step 3: Total Expected Number of Comparisons**
$$E[\text{total comparisons}] = \sum\limits_{1\le i<j\le n} E[I_{ij}] = \sum\limits_{1\le i<j\le n} \frac{2}{j-i+1}$$
It turns out that this sum simplifies to: $$E[\text{total comparisons}] = 2n \ln{n} + O(n)$$

The expected number of comparisons in Quicksort is $2𝑛 \ln{𝑛}$, which explains its average-case time complexity of $𝑂(𝑛 \log{𝑛})$ .

## Part 2

### Definition of Amortized Analysis 
- It calculates the average running time for each operation over the worst-case scenario.
- not a method to design algorithms, but rather a more accurate way to analyze algorithms under specific conditions
- 不包括概率
$$\begin{cases}
\text{average-case analysis} &\text{considers all possible inputs}\\
\text{probabilistic analysis} &\text{involves all random choices}\\
\text{amortized analysis} &\text{looks at a sequence of operations}\\
\end{cases}$$
- assume worst-case input 并且通常不用randomness
### Three Methods of Amortized Analysis
1. **Aggregate Method**
找到upper limit T(n), 计算平均cost as T(n)/n
2. **Accounting Method**
计算每个操作的成本（既要考虑直接immediate time影响，也要考虑对future operations的影响）
assign 每种operation 不同的amortized cost
对某些操作收取过高的费用（overcharge），并将多余部分储存为credit
这个credit可以后续用来补偿compensate昂贵的操作
3. **Potential Method**
类似方法2但是储存credit当作potential energy
overcharge 早期操作来cover future昂贵操作

### Examples: Dynamic Tables

- 对于the size of a ==hash table== for insertions, 需要平衡两因素：空间和时间
	If 哈希表比较大，找一个item会更快，但需要更多memory（space）
	If 哈希表比较小，占用空间少，但需要更多时间去find/insert a item（冲突太多导致每个桶装了很多元素）
- 解决方法：use a ==dynamic table==, which grows in size when it becomes full

- Steps when the Dynamic table is ==full==: 
	Allocate memory for a table that is double the current size. 
	Copy all the contents from the old table to the new one. 
	Free the memory of the old table. 
	If space is available, simply insert the new item in the empty slot.

#### Simple analysis

If 用==simple analysis==, an insertion的最坏情况是 $O(n)$, total cost is $n\times O(n) = O(n^2)$
This analysis gives an upper bound, but not a tight upper bound for 𝑛 insertions as all insertions do not take 𝜃(𝑛) time.

#### Aggregate Analysis
- let us try solving it by ==Aggregate Analysis==: $$\frac{Cost(\text{n opertaions})}{n} = \frac{Cost(\text{normal opertaions})}{n} + \frac{Cost(\text{expensive opertaions})}{n}$$
这里normal operations是直接插入，expensive operations是double and copy

| No          | Table Size | Total Cost($C_i$) | Cost of Operations | Cost of Doubling and Copying |
| ----------- | :--------: | :---------------: | :----------------: | :--------------------------: |
| 1           |     1      |         1         |         1          |              0               |
| 2           |     2      |         2         |         1          |           1=$2^0$            |
| 3           |     4      |         3         |         1          |           2=$2^1$            |
| 4           |     4      |         1         |         1          |              0               |
| 5           |     8      |         5         |         1          |           4=$2^2$            |
| 6           |     8      |         1         |         1          |              0               |
| 7           |     8      |         1         |         1          |              0               |
| 8           |     8      |         1         |         1          |              0               |
| 9           |     16     |         9         |         1          |          8 = $2^3$           |
| ...         |    ...     |        ...        |        ...         |                              |
| $2^{n-1}$+1 |   $2^n$    |    1+$2^{n-1}$    |         1          |          $2^{n-1}$           |
$$
\begin{gather}
\text{Total cost } = \sum\limits_{i=1}^{n}c_i \\
\le n + \sum\limits_{j=0}^{\lfloor \log{n} \rfloor} 2^j \\
= n + \frac{2^{\lfloor \log{n} \rfloor +1}-1}{2-1} \\
= n + 2 \times 2^{\lfloor \log{n} \rfloor} - 1 \\
\le n + 2n -1 = 3n-1 = \Theta(n)
\end{gather}
$$
Thus, the average cost of each dynamic table operation is  Θ(𝑛)/ 𝑛 = Θ(1)
#### Accounting Method
- Idea: 
	Assign different charges to different operations. 
	The amount of the charge is called amortized cost. 
	Amortized cost is more or less than the actual cost. 
	When amortized cost >actual cost, the difference is saved in specific objects as credits. 
	The credits can be used by later operations whose amortized cost
- As a comparison, in aggregate analysis, all operations have same amortized costs
- The ==sum of all the amortized costs== in a sequence ==must be at least== the sum of ==all the actual costs== since we would like to bound the total cost of the sequence by the sum of amortized costs.
- Charge 𝑖𝑡ℎ operation a fictitious amortized cost $𝑐_𝑖'$ , where $1pays for 1unit of work (i.e., time)

- The bank balance must not go negative, we must ensure that, Let $𝑐_𝑖'$ be the charge of 𝑖𝑡ℎ operation then: $$\sum\limits_{i=1}^{n} c_i \le \sum\limits_{i=1}^{n} c_i'$$
  for all n.
  Thus, the total amortized costs provide an upper bound on the total true costs.
- Charge an amortized cost of $𝑐_𝑖'$ = $3 for the 𝑖𝑡ℎ insertion: 
	$1 pays for the immediate insertion. 
	$2 is stored for later table doubling.
	When the table doubles, $1 pays to insert a recent item, and $1 pays to move an old item.

|      **i**      |  1  |  2  |  3  |  4  |  5  |  6  |  7  |  8  |  9  | 10  |
| :-------------: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
|  **$size_i$**   |  1  |  2  |  4  |  4  |  8  |  8  |  8  |  8  | 16  | 16  |
|    **$c_i$**    |  1  |  2  |  3  |  1  |  5  |  1  |  1  |  1  |  9  |  1  |
| **$\hat{c_i}$** |  2  |  3  |  3  |  3  |  3  |  3  |  3  |  3  |  3  |  3  |
|  **$bank_i$**   |  1  |  2  |  2  |  4  |  2  |  4  |  6  |  8  |  2  |  4  |
The first operation costs only $2, not $3.
#### Potential Method
- Same as accounting method: something prepaid is used later
- Different from accounting method:
	The prepaid work not as credit, but as “potential energy”, or “potential”. 
	The potential is linked to the entire data structure, not to individual elements within it
- The most powerful method ( and the hardest to use), it builds on a physical metaphor of potential energy
- Instead of associating costs or taxes with operations, we represent prepaid work as the potential that can be spent on later operations.
- let $D_i$ denote our data structure after i operations and let $\Phi_i$ denote its potential. Let $c_i$ denote the actual cost of the $ith$ operation (which changes $D_{i-1}$ into $D_1$). Then the amortized cost of the $ith$ operation, denoted $c_i'$ is defined to be the actual cost plus the increase in potential: $$c_i' = c_i + \Phi_i - \Phi_{i-1}$$
  即（actual cost + potential change） ，其中$\Phi_i - \Phi_{i-1}$ 为potential difference
- 我们添加potential是因为我们想要amortized cost总是大于actual cost
- $$\begin{align}
 \sum\limits_{i=1}^{n}c_i' &= \sum\limits_{i=1}^{n}(c_i + \Phi(D_i) - \Phi(D_{i-1})) \\
 &= \sum\limits_{i=1}^{n}c_i + \Phi(D_n) - \Phi(D_0)
 \end{align}$$
    $\Phi(D_i)$ is called the potential of $D_i$
    a potential function is valid if $\Phi_i - \Phi_0 \ge 0$  for every i, then the total actual cost of any sequence of operations is always < the total amortized cost: $$\sum\limits_{i=1}^{n}c_i = \sum\limits_{i=1}^{n}c_i' - \Phi_n \le \sum\limits_{i=1}^{n}c_i'$$
    定义$\Phi(D_0) = 0$ 是很方便的，这样  $\Phi(D_i) \ge 0$ 适用于所有i，$c_i'$ is an overcharge

 - $\Phi = 2 \times \text{no of items in the array - capacity of the array}$
 
| No          | Table <br>Size | Cost of <br>Operations | Cost of <br>Doubling<br>and <br>Copying | Total <br>Cost<br>($C_i$) |           $$\Phi$$           | Amortized Cost<br>$C_i +\Phi_i - \Phi_{i-1}$ |
| ----------- | :------------: | :--------------------: | :-------------------------------------: | :-----------------------: | :--------------------------: | :------------------------------------------: |
| 1           |       1        |           1            |                    0                    |             1             |     2 $\times$ 1 - 1 = 1     |                1 + 1 - 0 = 2                 |
| 2           |       2        |           1            |                    1                    |             2             |     2 $\times$ 2 - 2 = 2     |                2 + 2 - 1 = 3                 |
| 3           |       4        |           1            |                    2                    |             3             |     2 $\times$ 3 - 4 = 2     |                3 + 2 - 2 = 3                 |
| 4           |       4        |           1            |                    0                    |             1             |     2 $\times$ 4 - 4 = 4     |                1 + 4 - 2 = 3                 |
| 5           |       8        |           1            |                    4                    |             5             |     2 $\times$ 5 - 8 = 2     |                5 + 2 - 4 = 3                 |
| 6           |       8        |           1            |                    0                    |             1             |     2 $\times$ 6 - 8 = 4     |                1 + 4 - 2 = 3                 |
| 7           |       8        |           1            |                    0                    |             1             |     2 $\times$ 7 - 8 = 6     |                1 + 6 - 4 = 3                 |
| 8           |       8        |           1            |                    0                    |             1             |     2 $\times$ 8 - 8 = 8     |                 1 + 8 - 6= 3                 |
| 9           |       16       |           1            |                    8                    |             9             |     2 $\times$ 9 - 16= 2     |                9 + 2 - 8 = 3                 |
| ...         |      ...       |          ...           |                                         |            ...            |             ...              |                     ...                      |
| $2^{n-1}$+1 |     $2^n$      |           1            |                $2^{n-1}$                |        1+$2^{n-1}$        | 2 $\times 2^{n-1}+2-2^n$ = 2 |           1+$2^{n-1}+2-2^{n-1}$= 3           |

### Empirical Analysis 

 In practice, 我们经常用经验分析而不是理论分析来比较算法


#### Issues to consider
- introduce many more factors:
	test platform (hardware, language, compiler)
	Measures of performance (what to compare) 
	Benchmark test set (what instances to test on) 
	Algorithmic parameters 
	Implementational details
- Practical considerations prevent complete testing.（资源有限，而因素较多，无法把所有组合都测试一遍）

#### Measures of Performance
- 目前，we focus on sequential algorithms
- 目标：
	比较两算法
	improve改进 the implementation实现方式 of a single algorithm
- 常见的性能衡量方法：
	empirical running time （CPU time, wall-clock）
	representative operation counts（例如排序中的比较次数）

#### Measuring Time

| 类型              | 意义                 | 用途            |
| --------------- | ------------------ | ------------- |
| User Time       | 程序在用户模式下消耗的 CPU 时间 | 一般用于算法性能测量    |
| System Time     | 系统调用占用的时间（内核模式）    | 如果有大量 I/O 要注意 |
| Wall-clock Time | 实际从开始到结束的时间        | 并行算法时有参考价值    |
串行算法更关注 User Time，但要小心它忽略了 I/O 等部分

#### Representative Operation Counts

有些情况下，我们可能想要数操作次数，而不是时间
	identify bottlenecks
	a counterpart to the theoretical analysis
哪些操作可以数：
	function calls and executions of individual lines of code to identify bottlenecks
	we may know a priori先验 知道我们想要测量什么（例如排序中的比较和交换）

#### Test Set
- 选对训练集很重要，to allow proper conclusions to be drawn
- must pay attention to their size, inherent difficulty固有难度 and 其他结构性特性
- 如果我们尝试分清多样算法，那选对训练集很重要

#### Comparing Algorithms

- Given a performance measure and a test set, the question still arises of how to decide which algorithm is “better.” 
- We can do the comparison using some sort of summary statistic. 
	Arithmetic mean 
	Geometric mean 
	Variance 
- These statistics hide information useful for comparison.

#### Scientific Method

- Empirical analysis:  Analyze running time based on observations and experiments. 
- Apply the scientific method: 
	1. Make observations and gather data. 
		- Make observations: 
			Run the program multiple times, systematically increasing the input size for each run.
		- Gather data: 
			Time each program run and record the elapsed time along with the associated input size
	2. Hypothesize an explanation for the observations. 提出假设 
	3. Make predictions based on the hypothesis. 
	4. Formulate指定 an experiment to see if the predictions occur. 
	5. Analyze experimental results to confirm or falsify the hypothesis
#### Empirical versus Theoretical Analysis 

- For sequential algorithms, asymptotic analysis is often good enough for choosing between algorithms. It is less ideal with respect to the tuning of implementation details.
- For parallel algorithms, asymptotic analysis is far more problematic. 
	The details not captured by the model of computation can matter much more. There is an additional dimension on which we must compare algorithms: for example, scalability


# Chapter 3
## Part 1
### General introduction to recursion. 
- A recurrence is an ==equation== that describes a ==function== based on its value at ==smaller inputs==.
- Solving a recurrence means finding a function that works for ==all natural numbers== and satisfies the recurrence
- When solving recurrences, we often ==ignore== floors, ceilings, and boundary conditions
- A recursive function is a function that ==calls itself== until a “==base condition==” is ==true==, and execution stops.
- [x] 整数的阶乘
- [x] 整数的斐波那契
### The complexity of the recursive algorithm 
->simplify recurrence -> Obtain a formula without recurrence
#### Simplifying recurrences 
1. Iteration Method (-> recursion tree method)
2. Substitution Method (aka, induction method)
3. Master Method

#### The iteration method
- to ==expand== the recurrence and express it as a ==summation of terms== dependent only on ==*𝑛*== and the ==initial condition==
- Use back-substitution to express the recurrence in terms of 𝑛 and the initial (boundary) condition.
- [x] 例题练习 （P16-23）
#### The substitution method
- It involves ==guessing the form== of the solution and then using mathematical ==induction== to find the ==constants== and show that the solution works.
- The substitution method can be used to establish either ==upper or lower bounds== on a recurrence.
- Two-steps solution: 
	1. **Guess the exact form of the solution**: Typically, one would precompute a set of numerical results for different input  n’𝑠 and derive a formula. 
	2. **Prove the correctness of the guess**: Mathematical induction is the tool of choice in most cases.
- Making a good guess:
	Some heuristics can help us make a good guess.
	Prove the loose upper and lower bounds on the recurrence and then reduce the range of uncertainty.
	Revising the guess by subtracting a lower-order term often permits the math to go through.
- [x] 例题2道 P27,P28
- [x] 例题、练习 P30-37

## Part 2

### Recursion tree method 
- A recursion tree is a tree where each node represents the cost of a certain recursive subproblem.
>We would usually use a recursion tree to generate possible guesses for the runtime, and then use the substitution method to prove them.

- We always consider the second term in recurrence as the root.
- We sum the costs within each of the levels of the tree to obtain a set of pre-level costs and then sum all pre-level costs to determine the total cost of all levels of the recursion.
- Steps to Solve Recurrence Relations Using Recursion Tree Method:
	1. ==Draw== the recursion tree for the given recurrence relation. 
	2. Calculate the ==height== of the recursion tree formed. 
	3. Calculate the ==cost== (time required to solve all the subproblems at a level) at ==each level==. 
	4. Calculate the ==total number of nodes== at ==each level== in the recursion tree. 
	5. ==Sum up== the ==cost== of all the levels in the recursion tree
- [x]  Example 1 (两种方法) $T(n) = 2T(n/2) +cn$
- [x]  $T(n) = T(n/3) + T(2n/3) + O(n)$
- [x]  $T(n) = 3T(n/4) + cn^2$
- Geometric progression
$$a + ar + ar^2 + ar^3 + \dots = \sum\limits_{k=0}^{\infty}ar^k = \frac{a}{1-r},~{} for ~{}|r|<1$$
### Master Method 
#### Method 1
$$T(n) = aT(\frac{n}{b}) + f(n) \text{  with } a\ge 1 \text{  and } b > 1$$
→𝒂 represents how many recursive calls are made, (Binary search has 1 split, while Merge sort has 2 split, etc.) 
→𝒃 represents the factor by which the work is reduced in each recursive call, (for example, Binary search and merge sort cut input in half). 
→𝒇(𝒏) reparents how much work is done by each call apart from the recursion, as a function of 𝑛. for example 𝑂(𝑛),𝑂(1).

$$\begin{cases}
T(n)= \theta(n^{\log_b{(a)}}) \text{ for }\epsilon > 0 &\text{if }f(n) = O(n^{\log_b{(a)}-\epsilon}) \\
T(n)= \theta(n^{\log_b{(a)}}\log{(n)})  &\text{if }f(n) = \Theta(n^{\log_b{(a)}}) \\
T(n) = \theta(f(n)) &\text{if }f(n) = \Omega(n^{\log_b{(a)}+\epsilon})
\end{cases}$$
>for case 3, f(n) should satisfy the regularity condition: $$af(\frac{n}{b}) \le cf(n), \text{𝑤ℎ𝑒𝑟𝑒 𝑐 <1(This is always holds for polynomials)}$$

- Steps:
	**Extract** a, b and f(n) from the given recurrence. 
	**Determine** $n^{\log_b{(a)}}$ . 
	**Compare** 𝑓(𝑛) and $n^{\log_b{(a)}}$ asymptomatically. 
	**Determine** the appropriate Master Method case and apply it.
- [ ] $T(n) = 2T(\frac{n}{2}) + n$
- [ ] $T(n) = 9T(\frac{n}{3}) + n$
- [ ] $T(n) = 3T(\frac{n}{4}) + n\log{n}$
- [ ] What the runtime of this recursion? (P32)
#### Method 2
$$T(n) = aT(\frac{n}{b}) + f(n)$$
We assume that: $$a\ge 1, b>1, f(n) = \Theta(n^k \log^p{n})$$
Case 1: $\text{if }\log_b{a} > k, \text{ then } \Theta(n^{\log_b{a}})$
Case 2: if $\log_b{a}$ = 𝑘,then we will have 3 sub-cases:
	a) If 𝑝 > −1, then $\Theta(n^k \log^{p+1}{n})$
	b) If p = -1, then  $\Theta(n^k \log{\log{n}})$
	c) If p < -1, then  $\Theta(n^k)$
Case 3: if $\log_b{a}$ < 𝑘,then we will have 2 sub-cases:
	a) If p $\ge$ 0, then $\Theta(n^k \log^p{n})$
	b) If p < 0, then  $\Theta(n^k)$
- [x] $T(n) = 2T(\frac{n}{2}) +1$
- [x] 6道练习题（P37）

# Chapter 4

## Part 1
### Divide and Conquer Algorithms
- In this approach, we solve a problem recursively by applying 3 steps: 
	1. ==DIVIDE==-break the problem into several sub-problems of smaller size. 
	2. ==CONQUER==-solve the problem recursively. 
	3. ==COMBINE==-combine these solutions to create a solution to the original problem
-  top-down approach
-  Recursion is usually adopted
- Algorithms using Divide and conquer Technique:
	1. Binary Search
	2. Merge Sort
	3. Quick Sort
	4. Finding maximum and minimum
	5. Strassen's matrix multiplication

### Linear (Sequential) Search
- 不是分而治之算法
- 3 popular search algorithms:
	linear search
	binary search
	jump search
- Every item is checked and if a match is found then that item is returned, otherwise, the search continues till the end of the data collection.
- input:
	search value(key)
	data set
- time complexity:
	best case O(1)
	worst case O(n)
	average case O(n/2)
### Binary Search
- 需要the list of elements 有序排列
##### Pros and cons
- advantage:
	1. searches several times faster than the linear search
	2. In each iteration, it reduces the number of elements to be searched from n to n/2
- Disadvantage:
	只能用于有序list
##### Analysis
let 𝑇(𝑛) denote the number of comparisons performed by Algorithm binary-search in the worst case on an array of size 𝑛, then 𝑇(𝑛) can be expressed by the recurrence: $$T(n) = \begin{cases}
1 & n = 1\\
T(\frac{n}{2})+1 & n> 1
\end{cases}$$
##### Rewrite
- [ ] 伪代码
- [ ] C++ program
### Merge Sort
- Efficient sorting is important for optimizing优化 the efficiency of other algorithms(such as search and merge algorithms)that require input data to be in sorted lists
- The output of any sorting algorithm must satisfy two conditions: 
	1. The output is in nondecreasing order(each item is no smaller than the previous item). 递增或递增趋势
	2. The output is a permutation(a reordering yet retaining all the original items)of the input. 不丢失不添加元素
- top-down approach
- [ ] 伪代码
- [ ] C++ program
- implicit binary tree 隐式二叉树 不需要指针来记录元素间关系，元素间关系是有元素本身决定的，隐藏的
- This chain of calls corresponds to a ==preorder== traversal of the tree: Visit the **root**, the **left subtree**, and then the **right subtree**.
- The computation, **however**, corresponds to a ==post-order== traversal of the tree: Visit the **left subtree**, the **right subtree**, and then the **root**.
- 实施 traversal, 需要用stack
- mergesort 左&右 然后merge两个结果
##### Time complexity
- the number of comparison: at least n1? at most n-1
- the number of assignment: 2n
- 和二元查找一样，basic operation也是element comparison，也就是说运行时间是和元素比较次数成正比
- we wish to compute the number of ==element comparisons== 𝐶(𝑛)：最小比较数？$$C(n) = \begin{cases}
 0 &\text{if }n = 1, \\
2C(n/2)+n/2 &\text{if }n \ge 2
\end{cases}$$
   根据Corollary 1.2，d=0, a=c=2, and b=1/2 可得$$C(n) = \frac{n\log{n}}{2}$$
	Corollary 1.2 $\text{Let a and c be nonnegative integers, b and d nonnegative}$ $\text{constants, and let n = }c^k,\text{ for some nonegative integer k. Then, the}$ $\text{solution to the recurrence}$
	$$f(n) = \begin{cases}
	d & \text{if n = 1,} \\
	af(n/c)+bn &\text{if n}\ge 2
	\end{cases}$$
	$$\begin{align}
	&f(n) = bn\log_{c}{n} + dn  \text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ } \text{ if }a = c \\
	&f(n) = (d + \frac{bc}{a-c})n^{\log_{c}{a}} - (\frac{bc}{a-c})n \text{ }\text{ }\text{ if a} \ne c
	\end{align}$$
- The ==maximum number of comparisons== done by the algorithm is given by the recurrence $$C(n) = \begin{cases}
 0 &\text{if }n = 1, \\
2C(n/2)+n-1 &\text{if }n \ge 2
\end{cases}$$
  用iteration method，
   $$\begin{align}
   C(n)&=2C(n/2)+n-1 \\
   C(n)&=2[2C(n/4)+n/2+1]+n-1 \\
   C(n)&=4C(n/4)+n+2+n-1 \\
   &=2^k C(n/2^k)+ kn -(2^k-1)\\
   (n = &2^k -> k = \log{n})\\
   &=nC(1)+n\log{n}-n+1\\
   &=n\log{n}-n+1
   \end{align}$$
- The total number of element comparisons performed by algorithm mergesort to 排序一个大小为n的数组，where n is a power of 2, is between 上面两种结果（$\frac{n\log{n}}{2}$, $n\log{n} -n +1$）之间
- 如果 n 是任意arbitrary正整数（不一定是 2 的幂）,比较次数满足以下递推关系：$$C(n) = \begin{cases}
 0 &\text{if }n = 1, \\
C(\lfloor n/2\rfloor)+C(\lceil n/2\rceil)+ bn &\text{if }n \ge 2
\end{cases}$$
where b is nonnegative
那么用主定理即可得$C(n) = \Theta(n\log{n})$
- 由于元素比较的操作是basic operation，因此mergesort算法的运行时间是：$$T(n) = \Theta(n \log{n})$$
- 归并排序的运行时间(最坏情况下)达到了所有比较排序算法的最优下界 $\Omega(n \log n)$，因此它是**理论上最优的比较排序**(Insertion, Bubble, Quicksort, Mergesort, Heapsort)**算法之一**
##### Space complexity
- not a ==in-place== sort,需要额外空间
- total number of extra array items is $n + \frac{n}{2} + \frac{n}{4} +\dots =2n$
- the space needed for the recursive calls is Θ(𝑛)
##### Pros and Cons
1. advantage:
	Suitable for large-size lists. 
	Suitable for linked lists. (Can sort and merge 2 linked lists to create 1 linked list) Extended sorting.(Partitioning large data and sort it if the space not enough in the memory) 
	Stable.(Works in case of there is data duplicate in two files, the algorithm will preserve their order)
2. Disadvantage:
	需要extra space
	No small problem except when we have only one element.
	recursive

- [ ] C++练习

## Part 2
### Quicksort
- the basis for Algorithm quicksort 就是partition算法
- the action of arrangement is also called splitting or partitioning around x, which is called the pivot or splitting element
- [ ] partition 算法伪代码
- number of element comparisons of algorithm SPLIT is exactly n-1, 时间复杂度为 $\Theta(n)$
- The only extra space the algorithm uses is needed to hold its local variables. Therefore, the space complexity of the algorithm is Θ(1).
- [ ] quicksort 算法伪代码
- split算法和Quicksort算法的关系类似merge和mergesort算法关系。但是，merging属于分而治之的combining部分，spliting属于dividing部分
- The combine step in Algorithm quicksort is nonexistent.
#### pivot to be the first element
- [ ] quicksort 伪代码
- [ ] partition 伪代码
- [ ] 举例说明
#### pivot to be the last element
- [ ] quicksort 伪代码
- [ ] partition 伪代码
- [ ] 举例说明
#### pivot to be the middle element
- [ ] quicksort 伪代码
- [ ] partition 伪代码
- [ ] 举例说明
#### time complexity
##### Best case
- Quicksort's best case occurs when the partitions are as evenly balanced as possible: their sizes either are equal or are within 1 of each other.
	第一种是subarray元素个数为奇数，pivot正好在中间after partitioning，每个partition里有（n-1)/2 个元素
	第二种情况发生在子数组元素个数为偶数，one partition has n/2 elements with the other having 𝑛/ 2 −1
##### Worst case
- only need to find one situation in which the algorithm exhibits the longest running time for each value of 𝑛.也就是input array是以nondecreasing顺序排列的
- total up the partitioning times for each level： $$n+(n-1)+(n-2)+\dots +2+1 = \frac{n(n+1)}{2}$$
- In big-O notation, quicksort's worst-case running time is $𝑂(𝑛^2)$.
- In this case, the **smallest element** will always be chosen as the pivot
- The worst-case running time can be improved to ==Θ(𝑛log𝑛)== by always selecting the **median** as the pivot.
##### average case
- 最坏情况不多见
- on the average, the time complexity is $\Theta(n\log{n})$ 
- for simplicity，我们假设input elements are distinct,排列的概率相同
- any element will be picked as the pivot is 1/n
- Let 𝐶(𝑛) denote the number of comparisons done by the algorithm on the average on an input of size n $$C(n) = (n-1) + \frac{1}{n} \sum\limits_{w=1}^{n}(C(w-1)+C(n-w))$$
  since, $$\frac{1}{n} \sum\limits_{w=1}^{n} C(n-w) = C(n-1) + C(n-2) +\dots +C(0) = \sum\limits_{w=1}^{n}C(w-1)$$
  the equation can be simplified to $$C(n) = (n-1) \frac{2}{n} \sum\limits_{w=1}^{n}C(w-1)$$
 $$C(n) =(n+1)D(n) \approx 1.44n\log{n}$$
- The average number of comparisons performed by algorithm QUICKSORT to sort an array of n elements is $\Theta(n\log{n})$ 
- 尽管最坏情况下，Quicksort时间复杂度比很多其他排序算法大，但是实际应用更快，因为inner loop can be efficiently implemented on most architectures, and in most real-world data
- 同时可以通过改变pivot的选择来以不同方式实施，因此对于既定类型的数据，最坏情况很少发生
- ==However==，当数据很大且储存in external storage, mergesort通常被认为更好
##### avoid the worst case
 选择一个appropriate pivot
	 1. 中间元素
	 2. 随机选择
#### space complexity
- in-place
- index of the pivot item is created in each recursion call
- using a stack
- the space taken depends on the stack size, the maximum size of the stack in the worst case is *n* and in the average and best case is *log n*

#### Pros and Cons
Quicksort is a representative of three types of sorting algorithms: in-place, divide and conquer, and unstable
1. in-place:
	- does not create any copies of the array or any of its subarrays. 
	- It does however require stack memory for all recursive calls it makes

2. divide and conquer:
	 splits the array into smaller arrays until it ends with an empty array, or one that has only one element, before recursively sorting the larger arrays

3. unstable:
	一个stable的算法相同值的元素在排序后的数组中出现的relative order与排序前的顺序相同（如果有两个具有相同值的元素，则原始数组中的第一个元素将在排序后的数组中首先出现）
	不稳定的排序算法不保证这一点，但是这种情况可能发生

- [ ] C++写Quicksort

### Strassen’s Matrix Multiplication Algorithm
- The standard method of matrix multiplication of two (𝑛 × n) matrices takes $O(n^3)$ operations
- Strassen's algorithm is a Divide-and-Conquer algorithm that is asymptotically faster
- 通常$2\times 2$ 矩阵乘法需要8 multiplications & 4 additions
- 本算法using only 7 multiplications and 18 additions
- 乘法操作需要 $O(n^3)$，加法操作需要  $O(n^2)$
- multiplications are much more expensive, and it makes sense to trade one multiplication operation for 18 additions
- [ ] The pseudo code to multiply two Matrices
- Parallelizing the Algorithm: 把矩阵分成四份 $$T(n) = 8T(n/2) + O(n^2)$$
  by the master theorem, $T(n) = O(n^{\log_{2}{8}}) = O(n^3)$
- To apply divide and conquer the matrices dimensions should be a power of 2
- 如果n不是2的power，那么one simple modification is to 加上足够行和列的0
- Useful in practice for large matrices but would be slower than the fastest known algorithms for extremely large matrices
#### time complexity of matrix multiplication
$$T(n) = \begin{cases}
1&n\le 2 \\
8T(\frac{n}{2})+ n^2 &n >2
\end{cases}$$
 根据主定理， $T(n) = \Theta(n^3)$
用分而治之或for loop in matrix multiplication 会有相同的时间复杂度
#### How it works？
假设我们想multiply two matrices A and B $$A=\begin{pmatrix}
a_{11} &a_{12} \\
a_{21} &a_{22}
\end{pmatrix} \text{ }\text{ }\text{ }\text{ }\text{ }
B=\begin{pmatrix}
b_{11} &b_{12} \\
b_{21} &b_{22}
\end{pmatrix} $$
$$c=\begin{pmatrix}
c_{11} &c_{12} \\
c_{21} &c_{22}
\end{pmatrix}\text{ } = \begin{pmatrix}
a_{11} &a_{12} \\
a_{21} &a_{22}
\end{pmatrix} \text{ } \begin{pmatrix}
b_{11} &b_{12} \\
b_{21} &b_{22}
\end{pmatrix}$$
we first compute the following products:
$$\begin{align}
d_1 &= (a_{11} + a_{22})(b_{11} + b_{22}) \\
d_2 &= (a_{21} + a_{22})b_{11} \\
d_3 &= a_{11}(b_{12} - b_{22}) \\
d_4 &= a_{22}(b_{21} - b_{11}) \\
d_5 &= (a_{11} + a_{22})b_{22} \\
d_6 &= (a_{21} - a_{11})(b_{11} + b_{22}) \\
d_7 &= (a_{12} - a_{22})(b_{21} + b_{22}) 
\end{align}$$
Next, we compute C from the equation $$C=\begin{pmatrix}
d_1 +d_4 -d_5+d_7 &d_3 + d_5 \\
d_2+d_4 &d_1 + d_3 -d_2 +d_6
\end{pmatrix}$$
- [ ] Strassen’s Matrix Multiplication – Pseudocode
#### Time complexity
In order to count the number of scalar operations, let 𝑎 and 𝑚 denote the costs of scalar addition and multiplication, respectively 
the total cost of multiplying two 𝑛 × 𝑛 matrices is governed by the recurrence
$$T(n) = \begin{cases}
m&n=1 \\
7T(\frac{n}{2})+ 18(\frac{n}{2})^2a &n \ge2 \\
\text{ }=mn^{\log{7}}+6an^{\log{7}}-6an^2
\end{cases}$$
 the running time is Θ($𝑛^{\log{7}} = Θ(𝑛^{2.81})$
 - [ ] 习题一份

## Part 3
### Tiling a defective chessboard Algorithm
#### Definition
- A chessboard is an 𝑛×𝑛 grid, where 𝑛 is a power of 2.
- A defective chessboard is a chessboard that has one unavailable (defective) position.
- A triomino is an L-shaped object that can cover three squares of a chessboard. （4 orientations）

- 在 $n \times n$ 的defective chessboard里放入 $(n^2-1)/3$ 的triominos so that all $n^2-1$ 的无缺陷的位置被覆盖
#### given conditions
- We have a chessboard of size 𝑛 × 𝑛, where 𝑛 = $2^𝑘$. 
- Exactly one square is defective in the chessboard. 
- The tiles(triominoes) are in L-shape i.e., 3 squares. 
#### Objectives
- Cover all the chessboard with L-shape tiles(triominoes), except the defective square

#### 𝟖×𝟖 Defective Chessboard
1. One of the cells is defective
2. We divide the chessboard into equal sub-halves.
3. Trick to cover the chessboard with tiles
4. Again creation of defective boxes as we divide the chessboard.
5. As we have finally divided the problem into 2 × 2 board we will put the tiles.
6. The procedure will continue until all the sub board are covered with the tiles.
7. The final chessboard has been covered by all the tiles and only left the defectives which we created.
8. Here we will cover the defectives which we have created as in the last, there should be only one defective left

- [ ] Pseudocode

#### Time complexity
The recurrence equation of the defective chessboard problem is given as follows: $$T(n) = \begin{cases}
c & for \text{ }k = 0 \\
4T(\frac{n}{2}) + c & for \text{ }k > 0 
\end{cases}$$
  根据主定理，the time complexity of this algorithm is 𝑶($𝒏^𝟐$)

### Finding minimum and Maximum Element Algorithm
#### Problem Description:
Given an array `A[]` of size 𝑛, you need to find the maximum and minimum element present in the array.
#### This problem can be solved by these methods: 
1. Searching linearly 
2. Divide and Conquer

#### searching linearly
- [ ] 伪代码
##### Complexity Analysis
- At every step of the loop, we are doing 2 comparisons in the worst case. 
- Total number of comparisons (in worst case) = 2 ×(𝑛 −1) = 2𝑛 − 2 
- Time complexity = O(n), Space complexity = O(1)
#### Divide and Conquer algorithm
将数组二分，分别递归找最大最小值,然后合并两个子问题的结果
##### time complexity
$$T(n) = \begin{cases}
0 &if \text{ } n=1\\
1 &if \text{ } n=2\\
2T(\frac{n}{2}) + 2 &if \text{ } n >2
\end{cases}$$
using iteration method: $$\begin{align}
T(n)&=2T(\frac{n}{2}) + 2 \\
T(\frac{n}{2})=2(2T(\frac{n}{4})+2)&+2 =2^2T(\frac{n}{2^2})+2^2+2
\end{align}$$
the last element is k, thus: $$\begin{align}
T(n) &= 2^kT(\frac{n}{2^k})+\sum\limits_{i=1}^{k}2^i \\
= &2^kT(\frac{n}{2^k})+2^k-2\\
& \text{ } Let \text{ } 2^k = n \\
T(n) &=nT(1)+n-2 \\
=nT(1) + n-&2=n\times 1+n-2=2n-2
\end{align}$$
Thus, T(n)=O(n)
- [ ] exercise
## Part 4
### Sorting algorithm
- A sorting algorithm is used to arrange elements of an array/list in a specific order.
- Different Sorting Algorithms:
	Bubble Sort 
	Selection Sort 
	Insertion Sort 
	Merge Sort 
	Quicksort 
	Counting Sort 
	Radix Sort 
	Bucket Sort 
	Heap Sort 
	Shell Sort

#### Complexity analysis of different sorting algorithms

- The efficiency of any sorting algorithm is determined by the time complexity and space complexity of the algorithm. 
- **Time** Complexity: Time complexity refers to the ==time== taken by an algorithm to complete its execution with ==respect to the size of the input==. 
- It can be represented in different forms: 
	Big-O notation (O) 
	Omega notation (Ω) 
	Theta notation (Θ)
- **Space** Complexity: Space complexity refers to the ==total amount of memory== used by the algorithm for a complete execution. It includes both the ==auxiliary memory== and the ==input==. 
- The auxiliary memory is the ==additional space== occupied by the algorithm apart from the input data. 
- Usually, auxiliary memory is considered for calculating the space complexity of an algorithm.

| Sorting Algorithm | Time Complexity-Best | Time Complexity-Worst | Time Complexity-Average | Space Complexity |
| ----------------- | :------------------: | :-------------------: | :---------------------: | :--------------: |
| Bubble Sort       |          n           |         $n^2$         |          $n^2$          |        1         |
| Selection Sort    |        $n^2$         |         $n^2$         |          $n^2$          |        1         |
| Insertion Sort    |          n           |         $n^2$         |          $n^2$          |        1         |
| Merge Sort        |        nlogn         |         nlogn         |          nlogn          |        n         |
| Quicksort         |        nlogn         |         $n^2$         |          nlogn          |       logn       |
| Counting Sort     |         n+k          |          n+k          |           n+k           |       max        |
| Radix Sort        |         n+k          |          n+k          |           n+k           |       max        |
| Bucket Sort       |         n+k          |         $n^2$         |            n            |       n+k        |
| Heap Sort         |        nlogn         |         nlogn         |          nlogn          |        1         |
| Shell Sort        |        nlogn         |         $n^2$         |          nlogn          |        1         |
#### Stability of Sorting Algorithm
- A sorting algorithm is considered stable if the ==two or more items== with the ==same value== maintain the ==same relative positions== even ==after sorting==.

| Sorting Algorithm | Stability |
| ----------------- | :-------: |
| Bubble Sort       |    Yes    |
| Selection Sort    |    No     |
| Insertion Sort    |    Yes    |
| Merge Sort        |    Yes    |
| Quicksort         |    No     |
| Counting Sort     |    Yes    |
| Radix Sort        |    Yes    |
| Bucket Sort       |    Yes    |
| Heap Sort         |    No     |
| Shell Sort        |    No     |
### Radix Sort
- 不是分而治之算法
- Radix = “The base of a number system”
- In practice, radix sort is fast for large inputs, as well as simple to code and maintain
- Digit-by-digit sort. 
- Hollerith’s original (==bad==) idea: sort on the **most-significant digit** first. 
- ==Good idea==: Sort on the **least-significant digi**t first with an ==auxiliary stable sort==每轮使用一个稳定排序算法
- This sorting method is called radix sort because the information used to sort the keys is a particular radix(base).
- radix可以是任何数字基底或者是字母
- 排序过程中，会用“桶”（pile）来分类不同的位数。The number of piles is the same as the radix.
	If we were sorting numbers represented in hexadecimal, the number of piles would be 16. 
	If we were sorting alpha keys represented in the English alphabet, the number of piles would be 26
#### Sorting by the most significant digit
- We have two choices. The **first** choice is to sort each list using ==another sorting algorithm== and then ==concatenate the resulting lists== into one sorted list. 
- Observe that in the **worst case** ==all== numbers may have the ==same most significant digit==, which means that they will all end up in one list and the other nine lists will be empty.
- Hence, if the sorting algorithm used runs in Θ(𝑛 log𝑛) time, the running time of this method will be Θ(𝑛 log𝑛). 
- **Another** possibility is to ==recursively sort each list== on digit $d_k$ − 1. But this approach will result in the addition of more and more lists, which is undesirable.
#### Sorting by the least significant digit
- If the numbers are first distributed into the lists by their least significant digit, then a very efficient algorithm results.
- This algorithm is commonly known as radix sort. It is straightforward to derive the algorithm using induction on k.
- The implementation of the algorithm does ==not require== any ==other sorting algorithm==. Nor does it require ==recursion==.
#### How does it work?
1. First, distribute the numbers into 10 lists 𝐿0, 𝐿1 , … , 𝐿9 according to digit 𝑑1, so that those numbers with 𝑑1 = 0 constitute list 𝐿0, those with 𝑑1 = 1 constitute list 𝐿1, and so on. 
2. Next, the lists 合并coalesce in the order 𝐿0, 𝐿1 , …, 𝐿9. 
3. Then, they are distributed into 10 lists according to digit 𝑑2, coalesced in order, and so on. 
4. After distributing them according to $d_k$ and collecting them in order, all numbers will be sorted.
- [ ] 伪代码
#### Time and Space complexity
- There are 𝑘 passes, and each pass costs Θ(𝑛) time. 
- Thus, the running time of the algorithm is Θ(𝑘𝑛). 
- If 𝑘 is constant, the running time is simply Θ(𝑛). 
- The algorithm uses Θ(𝑛) space, as there are 10 lists needed and the overall size of this list is $\Theta(n)$
### Bubble Sort
#### How it works?
- Suppose we are trying to sort the elements in ascending order.
- First Iteration (Compare and Swap) 
	Starting from the first index, compare the first and the second elements. 
	If the first element is greater than the second element, they are swapped. 
	Now, compare the second and the third elements. Swap them if they are not in order.
	The above process goes on until the last element.
#### Optimized Bubble Sort Algorithm
- In the previous algorithm, all the comparisons are made even if the array is already sorted. This increases the execution time. 
- To solve this, we can introduce an extra variable swapped.
- The value of swapped is set true if there occurs swapping of elements. Otherwise, it is set false. After an iteration, if there is no swapping, the value of swapped will be false. This means elements are already sorted and there is no need to perform further iterations. 
- This will reduce the execution time and helps to optimize the bubble sort
- [ ] 伪代码
- [ ] 练习题
#### Complexity
1. Time complexity：
	Best  O(n) →If the array is already sorted, then there is no need for sorting
	Worst  $O(n^2)$   →If we want to sort in ascending order and the array is in descending order then the worst case occurs.
	Average  $O(n^2)$  →It occurs when the elements of the array are in jumbled order (neither ascending nor descending).
2. Space complexity： O(1) 
	Space complexity is O(1) because an extra variable is used for swapping. 
	In the optimized bubble sort algorithm, two extra variables are used. Hence, the space complexity will be O(2).
3. Stability： Yes
### Selection Sort
- an in-place comparison-based algorithm
- Initially the sorted part is empty, and the unsorted part is the entire list. 
- The smallest element is selected from the unsorted array and swapped with the leftmost element, and that element becomes a part of the sorted array. 
- This process continues moving unsorted array boundary by one element to the right.
- [ ] 伪代码
#### complexity
- It is easy to see that the number of element comparisons performed by the algorithm is exactly: $$\sum\limits_{i=1}^{n-1}(n-i)=(n-1)+(n-2)+\dots +(n-n+1)=\sum\limits_{i=1}^{n-1}i=\frac{(n-1)n}{2}$$
- It is also easy to see that the number of element interchanges is between 0 and 𝑛 − 1. 
- Since each interchange requires three element assignments, the number of element assignments is between 0 and 3(𝑛 − 1)

1. Time complexity：
	Best $O(n^2)$   →It occurs when the array is already sorted
	Worst  $O(n^2)$  →If we want to sort in ascending order and the array is in descending order then, the worst case occurs
	Average  $O(n^2)$  →It occurs when the elements of the array are in jumbled order (neither ascending nor descending).
2. Space complexity： O(1) 
	an extra variable temp is used
3. Stability： No
- [ ] C++ program









