[è¯¾ä»¶1](https://l.xmu.edu.my/pluginfile.php/1154515/mod_resource/content/1/Chapter%202%20%28i%29-%20Probabilistic%20Analysis.pdf)
[è¯¾ä»¶2](https://l.xmu.edu.my/pluginfile.php/1156210/mod_resource/content/1/Chapter%202%20%28ii%29-%20Amortized%20and%20Empirical%20Analysis.pdf)
### Lecture (1) Probabilistic Analysis
#### definition
- often used to evaluate an algorithm's running time
- åŒ…æ‹¬åº”ç”¨æ¦‚ç‡è®ºåˆ†æé—®é¢˜
- æœ‰äº›ç®—æ³•éœ€è¦æ¦‚ç‡åˆ†æï¼Œè€Œä¸æ˜¯ä¾èµ–ç¡®å®šæ€§æ–¹æ³• deterministic methods 
- instead ofåªfocus on best-caseæˆ–worst-caseï¼Œè€Œæ˜¯é€šè¿‡ä»¥ä¸‹æ–¹å¼è€ƒè™‘æ‰€æœ‰æƒ…å†µï¼š
	1. è¯„ä¼°assessæ¯ç§æƒ…å†µçš„æ¦‚ç‡åˆ†å¸ƒprobability distribution
	2. æ ¹æ®æ­¤åˆ†å¸ƒè®¡ç®—é¢„æœŸexpectedè¿è¡Œæ—¶é—´
- ä¹Ÿèƒ½ç”¨äºåˆ†æå„ç§æ•°é‡various quantities
>ä¼°ç®—estimateæ‹›è˜åŠ©ç†Hire-Assistantæµç¨‹ä¸­çš„æ‹›è˜æˆæœ¬hiring cost
#### hiring problem
- a few requirements to be fulfilled:
	1. ä½ æ€»æ˜¯å¸Œæœ›æ‹¥æœ‰æœ€å¥½çš„å€™é€‰äººcandidate
	2. wheneverä½ é¢è¯•ä¸€ä½å€™é€‰äºº & è§‰å¾—å€™é€‰äººæ¯”ç°ä»»åŠ©ç†ä¼˜ç§€ï¼Œä½ å¿…é¡»è§£é›‡fireç°ä»»åŠ©ç†ï¼Œç„¶åè˜ç”¨hireè¯¥å€™é€‰äºº
	3. ä½ åº”è¯¥å§‹ç»ˆè˜ç”¨é¢è¯•çš„ç¬¬ä¸€ä½å€™é€‰äºº
- a few points to be considered for the Cost Model:
	1. æˆ‘ä»¬ä¸å…³å¿ƒconcernedæ‹›è˜åŠ©ç†çš„è¿è¡Œæ—¶é—´running time
	2. æˆ‘ä»¬å¿…é¡»å†³å®šé›‡ç”¨æœ€å¥½çš„å€™é€‰äººçš„æ€»æˆæœ¬total cost
	> If nä½å€™é€‰äººè¢«é¢è¯•ï¼Œmä½å€™é€‰äººè¢«é›‡ç”¨ï¼Œåˆ™æˆæœ¬ä¸º $nc_i + mc_h$
	> æˆ‘ä»¬å¿…é¡»æ”¯ä»˜ $nc_i$ çš„æˆæœ¬æ¥é¢è¯•å€™é€‰äººï¼Œæ— è®ºå½•å–äº†å¤šå°‘å€™é€‰äºº
	> å› æ­¤ï¼Œéœ€é‡ç‚¹åˆ†ææ‹›è˜æˆæœ¬ $mc_h$  
	> m varies with å€™é€‰äººçš„é¡ºåºorder
- ä¸ºäº†è¯´æ˜illustrateæ¦‚ç‡åˆ†æï¼Œè¯·è€ƒè™‘ä»¥ä¸‹å†…å®¹ï¼š
	1. é‡‡è®¿ä¸€åå€™é€‰äººæˆæœ¬ $c_i$
	2. é›‡ç”¨ä¸€åå€™é€‰äººæˆæœ¬ $c_h$
	3. å‡è®¾assume $c_i << c_h$
	4. æ€»æˆæœ¬total cost: $O(c_i \times n + c_h \times m)$, where m = number of hirings
 ```Pseudocode:
 Hire-Assistant(n)
1 best = 0//fictional least qualified candidate
2 for i = 1 to n
3     interview candidate i//paying cost c_i
4     if candidate i is better than candidate best
5         best = i
6         hire candidate i //paying cost c_h
 ```
$$\begin{cases}
\text{best case} & \text{cost: }c_in + c_h \text{ and } O(c_in+c_h)=O(c_in) \\
\text{worst case} & O(c_in+c_hn) = O(c_hn) \text{ since }c_h > c_i\\
\text{average case} & \text{å‡è®¾ç”³è¯·äººæŒ‰éšæœºé¡ºåºå‡ºç°ï¼Œç”³è¯·äººçš„æ¯ä¸ªæ’åˆ—permutationéƒ½æœ‰åŒç­‰çš„å¯èƒ½equally likely}
\end{cases}$$
#### Hiring Problem Probabilistic Analysis
- total number of permutations for nä½å€™é€‰äººæ˜¯ $n!$ 
- æ¯ä¸ªæ’åˆ—éƒ½æœ‰ç›¸åŒçš„æ¦‚ç‡æˆä¸ºé¢è¯•çš„é¡ºåºsequence
- å‡è®¾å€™é€‰äººæ ¹æ®å…¶æ’årankæˆ–è€…ä¼˜å…ˆçº§priorityä»¥éšæœºé¡ºåºå‡ºç°
- åœ¨ç°å®ä¸­ï¼Œå€™é€‰äººå¯èƒ½ä¸ä¼šéšæœºå‡ºç°ï¼Œwhichä¼šå¯¼è‡´åˆ†æå‡ºç°é—®é¢˜issues
- ä¸ºäº†è§£å†³ä¸Šæ¡æ‰€è¯´é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨éšæœºç®—æ³•randomized algorithmæ¥å¢å¼ºæ§åˆ¶å¹¶ç¡®ä¿é¡ºåºçš„éšæœºæ€§
##### randomized algorithm
- åœ¨average-case analysisé‡Œï¼Œæˆ‘ä»¬é€šå¸¸å‡è®¾æ‰€æœ‰inputsæ˜¯equally likely
- in realityï¼Œæœ‰äº›inputå¯èƒ½æ€§æ›´é«˜
- if unluckyï¼Œå¯èƒ½æ€§æœ€é«˜çš„inputsæ˜¯most costlyï¼Œå°±åƒåœ¨quicksortç®—æ³•é‡Œè§åˆ°çš„é‚£æ ·
>æˆ‘ä»¬å¯ä»¥force all inputs to be equally likelyé€šè¿‡randomizing the inputï¼Œç¡®ä¿ä¸€ä¸ªæ›´balancedåˆ†æ
- æˆ‘ä»¬å¯èƒ½ä¸çŸ¥é“æˆ–è€…æ— æ³•æ¨¡æ‹Ÿmodelæ¦‚ç‡åˆ†å¸ƒ
- ç›¸åï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ç®—æ³•ä¸­ç”¨éšæœºåŒ–randomization to å¯¹è¾“å…¥æ–½åŠ impose a distribution
- an algorithm is randomized whenå…¶éƒ¨åˆ†è¡Œä¸ºç”±éšæœºæ•°ç”Ÿæˆå™¨random number generatorå†³å®š
>åœ¨hiring problemé‡Œï¼Œè¿™å¯¼è‡´äº†ä¸€ä¸ªå˜åŒ–ï¼š
	1. employment agency æå‰æä¾›äº†a list of nä½å€™é€‰äºº
	2. we get to é€‰æ‹©é¢è¯•interviewé¡ºåºï¼Œå¹¶ä¸”æˆ‘ä»¬å†³å®šä»¥éšæœºé¡ºåºé‡‡è®¿å€™é€‰äºº
  è¿™è®©æˆ‘ä»¬èƒ½é€šè¿‡å¼ºåˆ¶æ‰§è¡Œenforce random orderæ¥take control over æ˜¯å¦éšæœºæ’åºï¼Œä»è€Œä½¿average caseæˆä¸ºé¢„æœŸå€¼expected value

>åœ¨hire-assistant problemé‡Œï¼Œwe canï¼š
>	 >1. éšæœºæ’åˆ—permute å€™é€‰äººåå•list first
>	 >2. thenï¼Œ run the hiring algorithm
>ä¿è¯äº†å¯¹äºä»»ä½•inputï¼Œthe expected number of hires will be approximately $\ln{n} +O(1)$.
>
>é—®é¢˜å°±æˆäº†ï¼šæˆ‘ä»¬æ€ä¹ˆæ‰“ä¹±shuffle the listä½¿æ¯ä¸€ä¸ªæ’åˆ—æœ‰ç›¸åŒçš„å¯èƒ½æ€§ï¼Ÿ
>	  >æˆ‘ä»¬å‡è®¾access to a good éšæœºæ•°ç”Ÿæˆå™¨
>	  >ç›®æ ‡æ˜¯ç¡®ä¿æ¯ä¸ªpossible permutationæœ‰ç›¸åŒçš„è¢«é€‰ä¸­çš„å¯èƒ½æ€§
>å½“è¾“å…¥æ˜¯éšæœºçš„ï¼Œthe running time is referred to as the expected running time
#### Probabilistic Analysis with Indicator Random Variables (IRV)
- åœ¨probabilitieså’Œexpectationä¸­è½¬æ¢
	Random variable (X): ä¸€ä¸ªæ ¹æ®æ¦‚ç‡åˆ†å¸ƒå¯ä»¥å»ä»»ä½•å€¼çš„å˜é‡
	Expected value $(E[X])$: å¦‚æœæˆ‘ä»¬åå¤é‡‡æ ·sampleè§‚å¯Ÿobserveçš„å¹³å‡å€¼
- Introduction
	è¿™ç§æŠ€æœ¯ä½¿æˆ‘ä»¬èƒ½å¤Ÿè®¡ç®—éšæœºå˜é‡çš„é¢„æœŸå€¼ï¼Œå³ä½¿there is ä¾èµ–å…³ç³»dependence between variables
- IRVs are a useful tool in probabilistic analysis, ç‰¹åˆ«æ˜¯å½“åˆ†ædependent events
- definition
	an IRV is a variable that indicates whether a specific event has occurred
	it takes a value of:
		->1 if the event happens, and
		->0 if the event doesn't happen
- expected valueçš„åº”ç”¨
	for a given event A, the corresponding Indicator Random Variable $I_A$ is defind as:
		$I_A$ = 1 if event A occurs.
		$I_A$ = 0 if event A does not occur.
	The expected value of $I_A$ is the probability that event A occurs:
		E$[I_A]$ = P(A)
- practical use: IRV are particularly useful when åˆ†æmultiple eventsçš„é—®é¢˜ where:
	we define an IRV for each event
	by summing the IRVs, we å¯ä»¥è®¡ç®—hiresæ€»æ•°é‡çš„æœŸæœ›å€¼

>Let's define $I_i$ as the Indicator Random Variable for hiring candidate i
>$I_i$ = 1 if candidate i is hired, and 0 otherwise
>æœŸæœ›å€¼ï¼š
>$$E[\text{number of hires}] = E[\sum\limits_{i=1}^{n}I_i] = \sum\limits_{i=1}^{n}E[I_i]$$
>	ç”±äº$E[I_i] = P(\text{hiring candidate i})$, the expected value gives us the average number of hires across all candidates
>è¿™ä¸ªæŠ€æœ¯simplifies æ¦‚ç‡åˆ†æ by æŠŠé—®é¢˜åˆ†è§£æˆæ›´å°çš„ç‹¬ç«‹æˆ–ä¾èµ–events å¹¶è®¡ç®—æœŸå¾…å€¼ in a manageable way
##### Example: Applying Indicator Random Variables (IRVs) to the scenario of flipping a coin
1. Flipping a Coin Once
	Step 1: Define the Indicator Random Variable (IRV)
	define event X for the event "the coin lands heads" , the IRV will be defined as:$$X = \begin{cases}
	1, & \text{if heads occurs} \\
	0, & \text{if tails occurs} 
	\end{cases}$$
	Step 2: Compute the Expected Value
	$$E[X] = 1Â·P(heads) + 0Â·P(tails) = \frac{1}{2}$$
2. Flipping a Coin n times
	Step 1:  Define the Indicator Random Variables (IRVs)
	define event $X_i$ for each flip i (where i =1, 2, ..., n) to indicate whether heads occurs on the i-th flip:$$X_i = \begin{cases}
	1, & \text{if the i-th flip is heads} \\
	0, & \text{if the i-th flip is tails} 
	\end{cases}$$
	Step 2: Total Number of Heads
	$$X = X_1 + X_2 + ... + X_n$$
	Step 3: Compute the Expected Value
	$$\begin{align*}
	E[X] &= E[X_1 + X_2 + \dots +X_n] = E[X_1] + E[X_2] + \dots + E[X_n] \\
	&= \frac{1}{2} + \frac{1}{2} + \dots + \frac{1}{2} \\
	&= nÂ·\frac{1}{2} \\
	&= \frac{n}{2} 
	\end{align*}$$
	Thus, the expected number of heads in n flips is  $\frac{n}{2}$  
3. Conclusion: the use of IRVs simplifies the calculation of expected values, as it breaks down the problem into smaller, manageable components where each event can be treated independently. This technique can be applied to a wider range of probabilistic analyses.
##### Example: Using IRVs in Quicksort to Analyze Expected Comparisons
- in quicksort, the key operation is comparing elements to a chosen pivot to partition the array into subarrays. The number of comparisons made directly affects the algorithm's performance.
- Setup:
	suppose we're sorting an array of n distinct elements using Quicksort
	we want to compute the expected number of comparisons made by the algorithm during the sorting process
- Step 1: Defining Indicator Random Variables
	define an IRV $I_{ij}$ for each pair of element i and j, where i < j:
		$I_{ij}$ = 1 if elements i and j are compared at some point during the execution of Quicksort
		$I_{ij}$ = 0 if element i and j are never compared
- Step 2: When Are Two Elements Compared?
	if and only if one of them is the first to be chosen as a pivot from the subarray containing both i and j. ä¸€æ—¦ pivotè¢«é€‰å‡ºæ¥ï¼Œthe elements are placed in different subarrays, and no further comparisons are made between them.
	$$P(\text{i and j are compared}) = \frac{2}{j - i+1}$$
	Thus,
	$$E[I_{ij}] = \frac{2}{j-i+1}$$
- Step 3: Total Expected Number of Comparisons
	for all pairs of elements i and j:
	$$E[\text{total comparisons}] = \sum\limits_{1\le i\le j\le n}E[I_{ij}] = \sum\limits_{1\le i\le j\le n} \frac{2}{j-i+1}$$
	it turns out that this sum simplifies to:
	$$E[\text{total comparisons}] = 2n \ln{n} + O(n)$$
- Conclusion
	the expected number of comparisons in Quicksort is $2n \ln{n}$, which explains its average-case time complexity of $O(n\log{n})$ 
##### Sequential (Linear) Search Example
```
1: function SEQSEARCH(A[],n,K)
2:    for i = 1 to n do
3:      if A[i] = K then
4:        return i
5:      end if
6:    end for
7:    return -1
8: end function
```
- For this algorithm: 
$$\begin{cases}
\text{best case: A[1] = K, T(n)}\in O(1) \\
\text{worst case: K} \notin A, T(n)\in O(n) \\
\text{expected case:}\\
\text{ ->assume } K \in  A \text{ and that all positions have equal likehood of containing }K \\
\text{ ->assume } A\text{ has no duplicates } \\
\text{ ->all permutations of A are equally likely}
\end{cases}$$
- The expected run time is:
$$E[T(n)] = \sum\limits_{q=1}^{n}t (A[q] = k) \times p_r (A[q] = k)$$
	then,
	$$\begin{align}
	E(T(n)) &= \sum\limits_{q=1}^{n}qc \times \frac{1}{n} \\
	E(T(n)) &= \frac{c}{n}\sum\limits_{q=1}^{n}q \\
	 = \frac{c}{n} \times &\frac{n(n+1)}{2}  \\
	 = &\frac{c(n+1)}{2} \\
	 &\approx \frac{2}{n}  \\
	 \text{Expected case} &\text{ runtime is:} \\
	 E(T(n)) &\in O(n)
	\end{align}$$

#### Hiring Problem Revisited: Applying IRVs to the Hire-Assistant Problem
- ä½ æƒ³è¦å†³å®šhow many times you'll hire a new assistant based on the qualifications of candidates ï¼ˆä»¥éšæœºé¡ºåºåˆ°è¾¾çš„ï¼‰
##### Setup
- suppose you interview n candidates, one by one
- the algorithm works by hiring a candidate if they are more qualified than all previously interviewed candidates.
- we want to compute the expected number of hires
##### Step 1: Defining Indicator Random Variables
- define an IRV $I_i$ for each candidate i (where $1 \le i \le n$ ):
	$I_i = 1$ if candidate i is hired.
	$I_i = 0$ if candidate i is not hired.
##### Step 2: Expected Value of an Indicator Random Variable
- ç”±äºå€™é€‰äººæŒ‰éšæœºé¡ºåºåˆ°è¾¾ï¼Œå€™é€‰äººi åœ¨å‰iä½å€™é€‰äººä¸­ most qualified çš„æ¦‚ç‡æ˜¯ $\frac{1}{i}$ 
- Thus,
$$E[I_i] = P(\text{candidate i is hired}) = \frac{1}{i}$$
##### Step 3: Total Expected Number of Hires
- for all candidates:
$$E[\text{total hires}] = E [\sum\limits_{i = 1}^{n}I_i] = \sum\limits_{i=1}^{n}E[I_i] = \sum\limits_{i=1}^{n} \frac{1}{i}$$
- the sum is known as the harmonic series, and it approximates to:
$$\begin{align*}
H_n &= 1 + \frac{1}{2} + \frac{1}{3} +\frac{1}{4} + \dots + \frac{1}{n} \\
&= \sum\limits_{k=1}^{n} \frac{1}{k} \\
&= \ln{n} + O(1)\\
\\
E[\text{total hires} ] \approx \ln{n} + O(1)
\end{align*}$$
##### Conclusion
- expected number of hires is approximately ln n
- for large n, æœŸæœ›å€¼å¢é•¿ç¼“æ…¢
- IRVs é€šè¿‡å®šä¹‰specific eventså‘ç”Ÿä¸å¦æ¥å¸®æˆ‘ä»¬åˆ†è§£å¤æ‚çš„æ¦‚ç‡é—®é¢˜

### Lecture (2) Amortized Analysis and Empirical Analysis
#### Amortized Analysis
##### definitions
- a strategy for analyzing a sequence of operations to show the average cost per operation is small, even though a single operation within the sequence might be expensive
- calculates the average running time for each operation over the worst-case scenario
- not a method to design algorithms, but rather a more accurate way to analyze algorithms under specific conditions
- doesn't involve probabilities, unlike average-case performance
- $$\begin{cases}
\text{average-case analysis }& \text{considers all possible inputs} \\
\text{probabilistic analysis }& \text{involves all random choices} \\
\text{amortized analysis }&\text{looks at a sequence of operations}
\end{cases}$$
##### three methods
###### aggregate method
1. find the upper limit $T(n)$ for the total cost of n operations (worst case)
2. calculates the average cost as $T(n)$ / n
###### accounting method
1. calculate the cost of each operation, considering both its immediate time and its impact on future operations
2. assign each type of operation a different amortized cost
3. overcharge some operations and store the extra as a "credit"
4. this credit can be used later to compensateè¡¥å¿ for expensive operations
###### potential method
1. similar to the accounting method but stores the credit as "potential energy"
2. overcharge early operations to cover the cost of future expensive operations
##### Amortized Analysis & Probabilistic analysis - Summary
###### probabilistic analysis
1. average case running time: average over all possible inputs for one algorithm (operation)
2. if using probability, called expected running time
###### amortized analysis
1. no involvement of probability
2. average performance on a sequence of operations, even some operation is expensive
3. guarantee average performance of each operation among the sequence in worst case
##### examples
###### Dynamic Tables
- to balance the size of a hash table for insertions, you need to balance two factors: space and time
	If the hash table is larger: searching for an item will be faster, but the table will require more memory (space)
	If ------------------smaller: save space but may take more time to find or insert an item
- the solution is to use a dynamic table, which grows in size when it becomes full
- steps when the Dynamic table is full:
	1. allocate memory for a table that is double the current table
	2. copy all the contents from the old table to the new one
	3. free the memory of the old table
	4. if space is available, simply insert the new item in the empty slot
- initially table is empty and size is 0

| operation                                    | hash table |       |        |     |     |     |     |     |
| -------------------------------------------- | ---------- | ----- | ------ | --- | --- | --- | --- | --- |
| insert item 1 (overflow)                     | 1          | \     | \      | \   | \   | \   | \   | \   |
| insert item 2 (overflow)                     | 1          | 2     | \      | \   | \   | \   | \   | \   |
| insert item 3                                | 1          | 2     | 3      | \   | \   | \   | \   | \   |
| insert item 4 (overflow)                     | 1          | 2     | 3      | 4   | \   | \   | \   | \   |
| insert item 5                                | 1          | 2     | 3      | 4   | 5   | \   | \   | \   |
| insert item 6                                | 1          | 2     | 3      | 4   | 5   | 6   | \   | \   |
| insert item 7                                | 1          | 2     | 3      | 4   | 5   | 6   | 7   | \   |
| next overflow would happen when we insert 9, | table size | would | become | 16  |     |     |     |     |
- simple analysis gives an upper bound, but not a tight upper bound for n insertions as all insertions do not take $\theta(n)$ time
- so, let's try solving it by **Aggregate Analysis**:
$$\frac{\text{Cost(n operations)}}{n} = \frac{\text{Cost(normal operations) + Cost(Expensive operations)}}{n}$$
- $$\begin{align}\text{Total Cost } = \sum_{i=1}^nc_i \\
\le n + \sum\limits_{j=0}^{[\log{n}]}2^j \\
= n + \frac{2^{[\log{n}]+1}-1}{2-1} \\
= n + 2\times 2^{[\log{n}]} -1 \le n+2n-1 &\le 3n-1 = \theta(n)
\end{align}$$
- Thus, the average cost of each dynamic table operation is $\theta(n)/n = \theta(1)$ 
- **2- accounting method**
	assign different charges to different operations
	the amount of the charge is called amortized cost
	amortized cost is more or less than the actual cost
	when amortized cost > actual cost, the difference is saved in specific objects as credits
	credits can be used by later operations whose amortized cost < actual cost 
	in **aggregate analysis**, all operations have same amortized costs
- A charge of 2 per insertion again is not enough, but a charge of 3appears to be best
- The bank balance must not go negative,$$\sum\limits_{i=1}^{n}c_i \le \sum\limits_{i=1}^{n}c_i'$$ for all n
- thus, the total amortized costs provide an upper bound on the total true costs
- **3-potential method**
- different from **accounting method** 
	the prepaid work not as credit, but as "potential energy", or "potential"
	the potential is linked to the entire data structure, not to individual elements within it
	the most powerful method(and the hardest to use)
$$\begin{align}
\text{(actual cost + potential change)}\\
c_i'=c_i + Î¦_i - Î¦_{i-1}
\end{align}$$
$$\begin{align}
&\text{Initial data structure } ğ·_0,\\
 &n \text{ operations, resulting in }D_0,ğ·_1,\dots,D_n \text{ with costs } c_1,C_2,\dots,c_n. \\
 &Î¦(D_i) \text{is called the potential of }D_i.\\
 &\text{amortized cost }c_i' \text{ of the ith operation is:}\\
&  ->c_i' = c_i + Î¦(D_i)-Î¦(D_{i-1})\text{ (actual cost + potential change)} \\  
\end{align}$$
- So, the total amortized cost of n operations is the actual cost plus the total increase in potential:$$\begin{align}
\sum\limits_{i=1}^{n}c_i'&=\sum\limits_{i=1}^{n}(c_i + \Phi(D_i) - \Phi(D_{i-1}) \\
&= \sum\limits_{i=1}^{n}c_i + \Phi(D_n) -\Phi(D_0)
\end{align}$$
- A potential function is valid if $\Phi_i - \Phi_n \ge 0$ for every ğ‘–. If the potential function is valid, then the total actual cost of any sequence of operations is always less that the total amortized cost:$$\sum\limits_{i=1}^{n}c_i = \sum\limits_{i=1}^{n}c_i' - \Phi_n \le \sum\limits_{i=1}^{n}c_i'$$
- $\Phi = 2 \times$ no of items in the array - capacity of the array
#### The Empirical Analysis of an Algorithm
##### definitions
- in practice, we will often need to resortå€ŸåŠ© to empirical rather than theoretical analysis to compare algorithms
##### Issues to consider
- many more factors that need to be controlled in some way
	test platform (hardware, language, compiler)
	measures of performance (what to compare)
	benchmark test set (what instances to test on)
	algorithmic parameters
	implementational details
- Practical considerations prevent complete testing.
##### Measures of Performance
- for the time being, we focus on sequential algorithms
- goal:
	compare two algorithms
	improve the implementation of a single algorithm
- possible measures:
	empirical running time (CPU time, wall-clock)
	representative operation counts
##### Measuring Time
- three relevant measures of time taken by a process:
	user time measures the amount of time (number of cycles taken by a process in "user mode")
	system time the time taken by the kernel execution on behalf of the process
	wall-clock time is the total "real" time taken to execute the process
- generally, user time is the most relevant, though it ignores some crucial operations (I/O, etc.)
- wall-clock times should be used cautiously/sparingly but many be necessary for assessment of parallel codes
##### Representative Operation Counts
- What operations should we count?
	profilers can count function calls and executions of individual lines of code to identify bottlenecks
	we may know a prioriå…ˆéªŒ what operations we want to measure (for example comparisons and swaps in sorting)
##### Test Set
- The instances must be chosen carefully in order to allow proper conclusions to be drawn
- we must pay close attention to their size, inherent difficulty, and other important structural properties
- this is especially important if we are trying to distinguish among multiple algorithms
- Example: set
##### Comparing Algorithms
- we can do the comparison using some sort of summary statistic:
	arithmetic mean
	geometric mean
	variance
##### Scientific Method
- Empirical analysis: analyze running time based on observations and expriments
- apply the scientific method:
	1. make observations and gather dataï¼š
		run the program multiple times, systematically increasing the inout size for each run
		time each program run and record the elapsed time along with the associated input size
	2. hypothesize an explanation for the observations
	3. make predictions based on the hypothesis
	4. formulate an experiment to see if the predictions occur
	5. analyze experimental results to confirm or falsifyä¼ªé€  the hypothesis
**è¯¾å ‚é—®é¢˜ç­”æ¡ˆï¼Ÿ**

##### Empirical versus Theoretical Analysis
- for sequential algorithms, asymptotic analysis is often good enough for choosing between algorithms. It is less ideal with respect to the tuningè°ƒæ•´ of implementation details
- for parallel algorithms, asymptotic analysis is far more problematic
	the details not captured by the model of computation can matter much more
	there is an additional dimension on which we must compare algorithms: for example, scalability