
# Part 1

### Introduction
- a powerful algorithm design technique that is widely used to solve optimizationä¼˜åŒ– problems
- ä¹Ÿè¢«ç”¨æ¥improve the time complexity of the brute-force methods to solve some of the NP-hard problems
- not recursive, but the underlying solution of the problem is usually stated in the form of a recursive function
- ä¸åƒåˆ†è€Œæ²»ä¹‹ç®—æ³•ï¼Œimmediate implementation of the recurrence results in identical recursive calls that are executed more than once 
- å› æ­¤ï¼Œè¯¥æŠ€æœ¯evaluates the recurrence in a bottom-up manner, saving intermediate results to be used later to compute the desired solution
>traveling salesman problem
>$O(n^22^n)$ using dynamic programming
>-- is superior to the $\Theta(n!)$ bound that enumeratesæšä¸¾ all possible tours

- `Programming` refers to a tabularè¡¨æ ¼çš„ method, è€Œä¸æ˜¯æŒ‡å†™ç”µè„‘ä»£ç 
- $$\begin{cases}
\text{divide-and-conquer algorithm} &\text{åˆ’åˆ†ä¸ºä¸ç›¸äº¤çš„å­é—®é¢˜ï¼Œé€’å½’åœ°è§£å†³ï¼Œç„¶åç»„åˆè§£å†³æ–¹æ¡ˆæ¥è§£å†³åŸå§‹é—®é¢˜} \\
\text{dynamic programming} &\text{é€‚ç”¨äºå­é—®é¢˜é‡å overlapçš„æƒ…å†µï¼Œä¹Ÿå°±æ˜¯å­é—®é¢˜å…±äº«çš„æ—¶å€™} \\
&\text{ä¹Ÿå°±æ˜¯åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåˆ†è€Œæ²»ä¹‹ç®—æ³•ä¼šåšæ›´å¤šå·¥ä½œï¼Œé‡å¤è§£å†³common subproblems}
\end{cases}$$
- dynamic-programming algorithm æ¯ä¸ªsubproblemåªè§£å†³ä¸€æ¬¡ï¼Œç„¶åæŠŠå›ç­”ä¿å­˜åˆ°ä¸€ä¸ªè¡¨æ ¼ï¼Œå› æ­¤é¿å…äº†æ¯æ¬¡æ±‚è§£æ¯ä¸ªå­é—®é¢˜æ—¶é‡æ–°è®¡ç®—ç­”æ¡ˆçš„å·¥ä½œ
- optimization problemsæœ‰è®¸å¤špossible solutionsï¼Œæˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°a solution with the optimal (minimum or maximum) value
- æˆ‘ä»¬ç§°å‘¼ä¸€ä¸ªè§£å«an optimal solution to the problem å› ä¸ºå¯èƒ½å­˜åœ¨several solutions that achieve the optimal value
- summary:
	1. åœ¨æ‰€æœ‰è§£å†³æ–¹æ¡ˆä¸­æ‰¾åˆ°æœ€ä½³è§£å†³æ–¹æ¡ˆï¼Œå¹¶é€‰å‡ºæœ€ä¼˜çš„ä¸€ä¸ª
	2. The principle of optimality introduced by Richard bellman in 1955
	3. `dynamic` reflects the time-varying aspect of the problems
	4. `programming` refers to the tabular method to solve a problem
	5. solves every subproblem just once and savesè§£å†³æ–¹æ¡ˆåˆ°ä¸€ä¸ªè¡¨æ ¼é‡Œï¼Œé¿å…æ¯æ¬¡é‡åˆ°å­é—®é¢˜éƒ½é‡æ–°è®¡ç®—è§£
	6. åŠ¨æ€è§„åˆ’ä¸æ˜¯ä¸€ç§ç®—æ³•ï¼Œåªæ˜¯ä¸€ç§è®¾è®¡ç®—æ³•çš„techniqueæŠ€æœ¯/strategyç­–ç•¥
### Properties
Q: ä½ æ€ä¹ˆå°±çŸ¥é“ä¼˜åŒ–é—®é¢˜å¯ä¸å¯ä»¥ç”¨åŠ¨æ€è§„åˆ’æ¥è§£å†³å‘¢
A: Dynamic programming works if a problem exhibitså…·æœ‰ the following two properties:
	1. Optimal sub-structure: An optimal solution to the problem contains within it, optimal solutions to sub-problems
	2. Overlapping sub-problems: When é€’å½’ç®—æ³•é‡å¤è®¿é—®åŒä¸€ä¸ªé—®é¢˜æ—¶ï¼Œthe optimization problem has overlapping sub-problems

#### The principle of optimality
- Bellman's principle of optimality
	An optimal policy (set of decisions) has the property that, regardless of the initial state and decisions, the subsequent decisions must from an optimal policy relative to the state resulting from the first decision
	æœ€ä¼˜ç­–ç•¥ï¼ˆå†³ç­–é›†ï¼‰å…·æœ‰è¿™æ ·çš„ç‰¹æ€§ï¼šæ— è®ºåˆå§‹çŠ¶æ€å’Œå†³ç­–å¦‚ä½•ï¼Œåç»­å†³ç­–éƒ½å¿…é¡»å½¢æˆç›¸å¯¹äºç¬¬ä¸€ä¸ªå†³ç­–æ‰€äº§ç”Ÿçš„çŠ¶æ€çš„æœ€ä¼˜ç­–ç•¥
- Mathematically, $$\begin{align}
&f_N(x)=max. [r(d_n)+f_N-1T(x,d_n)]d_n \in x \\
&\text{where }f_N(x)= \text{optimal return from an N-stage process when initial state is x} \\
& \text{ }\text{ } \text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }r(d_n)= \text{immediate return due to decision }d_n \\
& \text{ } \text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }T(x,d_n) = \text{the transfer function which gives the resulting state}\\
&\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{} \{x\}=\text{set of admissible decisions}\\
\end{align}$$
This equation is also known as a dynamic programming equation.
- It writes the value of a decision problemå†³ç­–é—®é¢˜ at a certain point in time in terms of the payoffæ”¶ç›Š from some initial choices and the value of the remaining decision problem that results from those initial choices
- this breaks a dynamic optimization problem into simpler subproblems
#### Steps to develop a Dynamic programming algorithm

1. Characterizeæè¿° the structure of an optimal solution
2. Recursively define the value of an optimal solution
3. compute the value of an optimal solution, typically in a bottom-upè‡ªä¸‹è€Œä¸Š fashion
4. construct an optimal solution from computed information

 - 1-3æ­¥æ„æˆäº†åŠ¨æ€è§„åˆ’è§£å†³é—®é¢˜åŸºç¡€
 - If æˆ‘ä»¬åªéœ€è¦æœ€ä¼˜è§£optimal solutionçš„å€¼è€Œä¸æ˜¯è§£æœ¬èº«ï¼Œæˆ‘ä»¬å¯ä»¥çœç•¥omitç¬¬4æ­¥
 - Whenç¬¬4æ­¥ï¼Œæˆ‘ä»¬æœ‰æ—¶ä¼šåœ¨ç¬¬3æ­¥maintainä¿ç•™é¢å¤–ä¿¡æ¯ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥è½»æ¾construct an optimal solution
#### Example: Fibonacci Sequence
- Consider the inductive definition of this sequence: $$f(n)=\begin{cases}
1 & if \text{ } n=1 \text{ or } n=2, \\
f(n-1)+f(n-2) & if \text{ }n\ge3.
\end{cases}$$
- This definition suggests a recursive procedure that looks like the following (å‡è®¾è¾“å…¥æ’ä¸ºæ­£): 
	1. procedure $f(n)$
	2. if (n=1) or (n=2) then return 1
	3. else return $f(n-1) + f(n-2)$
è™½ç„¶è¿™æ ·å‡†ç¡®ï¼Œæ˜“å†™ï¼Œæ˜“debugï¼Œå¤§å¤šæ•°æŠ½è±¡ï¼›ä½†æ˜¯æ•ˆç‡ä¸é«˜ï¼Œè¿‡ç¨‹ä¸­å¤ªå¤šé‡å¤recursive calls
- If we assume that computing f(1) or f(2) requires a unit amount of time, then the time complexity of this procedure can be stated as: $$T(n)=\begin{cases}
1 & if \text{ } n=1 \text{ or } n=2, \\
T(n-1)+T(n-2) & if \text{ }n\ge3.
\end{cases}$$
æ˜¾è€Œæ˜“è§ï¼Œè¿™ä¸ªrecurrenceè§£æ˜¯ $T(n) = f(n)$ (ä¹Ÿå°±æ˜¯è¯´è®¡ç®—$f(n)$ æ‰€éœ€æ—¶é—´æ˜¯$f(n)$ itself)
æ¢å¥è¯è¯´ï¼Œè®¡ç®—$f(n)$æ‰€éœ€æ—¶é—´is exponential in the value of n

### Memoization
- top-down approach to solving a problem with dynamic programming
- è¢«å«åšè®°å¿†åŒ–memoizationï¼Œå› ä¸ºwe will create a memo, or a "note to self", for the values returned from solving each problem.
- è¿™æ ·ï¼Œå½“æˆ‘ä»¬å†æ¬¡é‡åˆ°åŒæ ·çš„é—®é¢˜ï¼Œæˆ‘ä»¬simply check the memo, è€Œä¸æ˜¯å†æ¬¡è§£å†³é—®é¢˜
#### Example: Top-Down Fibonacci

```python
const fibDown = (n,memo[ ])=>{
if (n==0 || n==1){
        return n;
}
    if (memo[n] == undefined){
        memo[n] = fibDown(n-1,memo) + fibDown(n-2,memo);
    }
    return memo[n];
}
```
Memoization changed the complexity from O($n^2$) to O(n), from exponential to linear

Why? Because we donâ€™t solve the overlapping subproblems. This is a big improvement over the naive implementation.

### Tabulation: Down-up
- with bottom-up, or åˆ¶è¡¨tabulation, we start with the smallest problems and use the returned values to calculate larger values
- æˆ‘ä»¬å¯ä»¥ç†è§£ä¸ºåœ¨tableè¡¨æ ¼æˆ–spreadsheetä¸­è¾“å…¥å€¼ï¼Œç„¶åapplying a formula to these values
#### Example: Fibonacci
```c
int fib(int n)
{
   if(n<=1)
	   return n;
   f[0]=0;
   f[1]=1;
   for(int i=2;i<=n;i++)
   {
	   f[i]=f[i-2]+f[i-1];
   }
   return f(n);
}
```
æ—¶é—´å¤æ‚åº¦ä¾æ—§ä¸ºO(n)
Whyï¼Ÿå› ä¸ºç”¨äº†forå¾ªç¯ï¼Œä¸è¿‡ç©ºé—´å¤æ‚åº¦ä¸ºO(1)ï¼Œå› ä¸ºno recursion

## Assembly Line Scheduling Problem

### Assembly Line Scheduling

>Automobile factory has two assembly lines
> ->æ¯æ¡çº¿æœ‰nä¸ªç«™ç‚¹ï¼š $S_{1,1}, \dots ,S_{1,n}$ and $S_{2,1}, \dots ,S_{2,n}$
> ->corresponding stations $S_{1,j}$ and $S_{2,j}$ æ‰§è¡Œç›¸åŒçš„function ä½†æ˜¯èŠ±è´¹ä¸åŒæ—¶é—´$a_{1,j}$ and $a_{2,j}$
> ->entry times are $e_1$ and $e_2$ and exit times are $x_1$ and $x_2$
>After going through a station, the car can either:
>-> stay on the same line at no cost, or
>->transfer to other line: cost after $S_{i,j}$ is $t_{i,j}, j=1,\dots ,n-1$
>Problem: what stations should be chosen from line 1 and line 2 in order to minimize the total time through the factory for one car?
### Brute force solution
- A brute force algorithm solves a problem through exhaustion: it goes through all possible choices until a solution is found.
- æ—¶é—´å¤æ‚åº¦é€šå¸¸æ˜¯proportional to the input size
- ç®€å•ä¸”ä¸€è‡´ï¼Œä½†æ˜¯éå¸¸æ…¢

- enumerate all possibilities for selecting stations
- compute how long it takes in each case and choose the best one
- æœ‰ $2^n$ ä¸ªå¯èƒ½å»é€‰æ‹©ç«™å°ï¼Œä¸å¯è¡ŒInfeasibleå½“n is large
### The steps to design an algorithm by using Dynamic programming

#### 1.Structure of the Optimal Solution
	
- å‡è®¾the fastest way through $S_{1,j}$ is through $S_{1,j-1}$
	an optimal solution to the problem â€œFind the fastest way through $S_{1,j}$â€œ contains within it an optimal solution to subproblems: â€ Find the fastest way through $S_{1,j-1}$ or $S_{2,j-1}$â€œ. 
	
	This is referred to as the optimal substructure property.
	
	Same is the case with $S_{2,j-1}$
#### 2.Recursively define the value of an optimal solution

- We define the value of an optimal solution in terms of the optimal solution to subproblems. 
- Definitions:

	$f^*$:  The fastest time to get through the entire factory.
	$f_i[j]$: the fastest time to get from the starting point through station $S_{i,j}$
	$l^*$: the line number which is used to exit the factory from the $n^{th}$ station
	$l_i[j]$: the line number (1 or 2) whose $S_{i,j-1}$ is used to reach $S_{i,j}$
  The objective function $$f^* = min(f_i[n]+x_1,f_2[n]+x_2)$$
- Base case: j=1, i=1,2 (getting through station 1)
	$f_1[1]$ = $e_1$ + $a_{1,1}$
	$f_2[1]$ = $e_2$ + $a_{2,1}$

- General case: j=2,3, ... ,n and i=1,2
The fastest way through $S_{1,j}$ is either:
	The way through $S_{1,j-1}$ then directly through $S_{1,j}$ or $f_1[j-1] + a_{1,j}$
	The way through $S_{2,j-1}$, transfer from line 2 to line 1, then through $S_{1,j}$ or $f_2[j-1] + t_{2,j-1} + a_{1,j}$
$f_1[j]=min(f_1[j-1]+a_{1,j}, f_2[j-1]+t_{2,j-1}+a_{1,j})$ 

- å…¬å¼æ¥è¡¨ç¤ºï¼š $$f_1[j] = \begin{cases}
e_1 +a_{1,1} &if \text{ }j = 1\\
min(f_1[j-1]+a_{1,j}, f_2[j-1]+t_{2,j-1}+a_{1,j}) & if \text{ }j\ge 2
\end{cases}$$
$$f_2[j] = \begin{cases}
e_2 +a_{2,1} &if \text{ }j = 1\\
min(f_2[j-1]+a_{2,j}, f_1[j-1]+t_{1,j-1}+a_{2,j}) & if \text{ }j\ge 2
\end{cases}$$
#### 3.Compute the optimal solution
- using the bottom-up approach, first found optimal solutions to subproblems, and then use them to find an optimal solution to the problem
- for j>=2, each value $f_i[j]$ depends on the values of $f_1[j-1]$ and $f_2[j-1]$
- in increasing order of j
------------------------------------------->

|          |    1     |    2     |    3     |    4     |    5     |
| -------- | :------: | :------: | :------: | :------: | :------: |
| $f_1[j]$ | $f_1(1)$ | $f_1(2)$ | $f_1(3)$ | $f_1(4)$ | $f_1(5)$ |
| $f_2[j]$ | $f_2(1)$ | $f_2(2)$ | $f_2(3)$ | $f_2(4)$ | $f_2(5)$ |
# Part 2
### Longest Common Subsequence (LCS) Problem
#### Introduction
>Given ä¸¤ä¸ªstring Aå’ŒBï¼Œåˆ†åˆ«é•¿nå’Œmï¼Œå†³å®šä¸€ä¸‹the length of the longest subsequence that is common to both A and B

A subsequence of A = $a_1a_2,\dots,a_n$ is a string of the form $a_{i1}a_{i2},\dots,a_{ik}$ , where each $i_j$ is between 1 and n and $1\le i_1 < i_2 < \dots <i_k \le n$ 
ä¸¾ä¾‹æ¥è¯´ï¼Œ if S={`x, y, z`}, A=`zxyxyz`, and B=`xyyzx`, then `xyy` is a subsequence of length 3 of both A and B, ä½†ä¸æ˜¯æœ€é•¿çš„ (LCS), å› ä¸º`xyyz`ä¹Ÿæ˜¯common subsequence of length 4 of both A and B

#### Common Subsequences
å‡è®¾Xå’ŒYare two sequences over a set of S={A,B,C,D}
	X:ABCBDAB
	Y:BDCABA
	Z:BCBA
Z is a common subsequence of X and Y if and only if: 
	â†’Z is a subsequence of X 
	â†’Z is a subsequence of Y
LCS problem asks to find a common subsequence of X and Y that is of maximal length.
#### Example
X= {A B C B D A B }, Y= {B D C A B A}
The brute force algorithm would compare each subsequence of X with the symbols in Y
#### A Poor Approach to the LCS Problem
- ä¸€ä¸ªè§£å†³é—®é¢˜çš„æ–¹æ³•å°±æ˜¯ç”¨brute-force methodï¼š
	enumerate all the $2^n$ subsequence of A
	for each subsequence determine if it is also a subsequence of B in $\Theta(m)$ time
- Clearly, the running time of this algorithm is $\Theta(m2^n)$, which is exponential
#### Applying Dynamic Programming Recursive formula for LCS
- To use the dynamic programming, we first find a recursive formula for the length of the LCS
- A = $a_1a_2,\dots,a_n$ and B = $b_1b_2,\dots,b_m$
- L`[i,j]` å®šä¹‰LCSçš„é•¿åº¦
- æ³¨æ„iæˆ–jå¯èƒ½ä¸º0ï¼Œä¹Ÿå°±æ˜¯è¯´Aæˆ–è€…Bå¯èƒ½ä¸ºç©ºstring
- If i=0 or j=0, then $L[i,j]$ = 0
- $\text{Suppose that both i and j are greater than 0. Then}$
	- If $a_i = b_j$, $L[i,j] = L[i-1,j-1]+1$
	- If $a_i \ne b_j$, $L[i,j] = max{L[i,j-1],L[i-1,j]}$
- $$L[i,j]=
  \begin{cases}
  0 & if\text{ }i=0 \text{ }or\text{ }j=0, \\
  L[i-1,j-1]+1 &if\text{ } i>0, j>0, \text{ }and \text{ }a_i=b_j, \\
  max\{ L[i,j-1],L[i-1,j]\} &if\text{ }i>0, j>0,\text{ }and\text{ }a_i \ne b_j
  \end{cases}$$
#### Developing a dynamic programming solution to the LCS problem
- straightforward
- ç”¨ä¸€ä¸ª (ğ‘› + 1) Ã— (ğ‘š + 1) è¡¨æ ¼è®¡ç®—`L[i,j]`å€¼ for each pair of values of i and j, $0 \le i \le n$ and $0 \le j \le m$
- We need to fill the table $ğ¿[0\dots ğ‘›, 0\dots ğ‘š]$ row by row using the formula
Steps:
	1. Find the length of LCS
	2. modify the algorithm to find LCS itself
	3. å®šä¹‰ $X_i,Y_j$ to be the prefixes of X and Y of length i and j respectively
	4. å®šä¹‰ `c[i,j]` to be the length of LCS of $X_i$ and $Y_j$
	5. Then X and Yçš„LCSé•¿åº¦å°±æ˜¯ `c[m,n]`
$$c[i,j] = 
\begin{cases}
c[i-1,j-1]+1 &if \text{ }x[i]=y[j],\\
max(c[i,j-1],c[i-1,j]) &otherwise \\
\end{cases}$$
- steps to find actual LCS:
	ä»`c[m,n]`å¼€å§‹å¹¶å¾€å›å€’
	whenever $c[i,j] = c[i-1,j-1] + 1$, $x[i]$ is a part of LCS
	å½“i=0 or j = 0ï¼ˆä¹Ÿå°±æ˜¯åˆ°è¾¾å¼€å§‹ï¼Œè¾“å‡º showed letters in reverse order)
- complexity:
	time complexity: å°±æ˜¯è¡¨æ ¼å¤§å°ï¼Œ$\Theta(nm)$,å› ä¸ºå¡«æ¯ä¸ªentry éœ€è¦ $\Theta(1)$ æ—¶é—´
	ä½†æ˜¯è¿™ä¸ªalgorithm can easily be modified so that it requires only Î˜(min{ğ‘š,ğ‘›}) space. 

# Part 3
### Matrix Chain Multiplication Problem
#### Introduction
å‡è®¾æˆ‘ä»¬éœ€è¦è®¡ç®—ä¸‰ä¸ªçŸ©é˜µä¹˜ç§¯ï¼ŒM1 M2 M3åˆ†åˆ«æ˜¯ $2\times 10 \text{ } 10 \times 2 \text{ }2 \times 10$
If å…ˆä¹˜ M1 M2, ç»“æœå†ä¹˜M3, 
	the number of scalar multiplications will be 2Ã—10Ã—2+2Ã—2Ã—10 = 80
Else if å…ˆä¹˜ M3 M2, ç»“æœå†ä¹˜M2,
	the number of scalar multiplications becomes 10 Ã— 2 Ã— 10 + 2 Ã— 10 Ã— 10 =400
æ™®é€šçŸ©é˜µä¹˜æ³•å°±æ˜¯éœ€è¦aÃ—bÃ—cæ¬¡ä¹˜æ³•ï¼ˆçŸ©é˜µå¤§å° aÃ—b bÃ—cï¼‰
#### The ways to multiply chain of ğ’ matrices
- ä¹˜ä¸¤ä¸ªçŸ©é˜µåªæœ‰ä¸€ç§æ–¹æ³•ï¼Œä¹˜ä¸‰ä¸ªçŸ©é˜µæœ‰ä¸¤ç§é€”å¾„
  ä¹Ÿå°±æ˜¯ $f(2)=1 \text{ and } f(3)=2$
- to make sense if the recurrence, we let f(1) = 1
- é‚£ä¹ˆfirst 10 termsï¼š
	1, 1, 2, 5, 14, 42, 132, 429, 1430, 4862, 16796, .....
	æœ‰4862 wayså»ä¹˜10ä¸ªçŸ©é˜µ
- Let f(n) be the number of ways to parenthesize a product of ğ‘› matrices
	æˆ‘ä»¬å¯ä»¥åˆ†å¼€æ‰§è¡Œä¹˜æ³• $$(M1M2\dots Mk)\times(Mk+1Mk+2\dots Mn)$$
	æœ‰ $f(k)$ æ–¹æ³•å»ä¹˜å‰kä¸ªçŸ©é˜µ
	å¯¹äºæ¯ä¸ª $f(k)$ æ–¹æ³•ï¼Œæœ‰ $f(n-k)$ ways å»parenthesize the remaining n-k çŸ©é˜µ
	the overall number of ways to parenthesize the n matrices is given by the summation $$f(n) = \sum\limits_{k=1}^{n-1}f(k)f(n-k)$$
#### Brute-force method
- The cost of multiplying a chain of ğ‘›matrices ğ‘€1ğ‘€2...ğ‘€ğ‘› depends on the order in which the ğ‘›âˆ’1 multiplications are carried out. 
- The order which minimizes the number of scalar multiplications can be found in many ways. 
- Brute-force method tries to compute the number of scalar multiplications of every possible order.
Brute force method of exhaustive search takes time exponential in ğ‘›.
#### Dynamic Programming Approach
- Dynamic programming method can be used to efficiently solve the matrix chain problem in time $\Theta(n^3)$.
1. The structure of an optimal solution:
	ç”¨ $A_{i\dots j}$ è¡¨ç¤º $A_i A_{i+1}\dots A_j$ ä¹˜ç§¯
	An optimal parenthesization of the product $A_1 A_2\dots A_n$ splits the product between $A_k$ and $ğ´_{ğ‘˜+1}$ for some integer ğ‘˜ where $1 \le k < n$
	å…ˆè®¡ç®—çŸ©é˜µ $A_{1\dots k}$ å’Œ $A_{k+1\dots n}$ï¼Œ ç„¶åæŠŠå®ƒä»¬ç›¸ä¹˜å¾—åˆ° $A_{1\dots n}$
 ä¸ºå•¥è¦è¿™æ ·åˆ†å¼€å†åˆå¹¶ï¼Ÿå› ä¸ºä¸€ä¸ªå¤§çš„é—®é¢˜çš„æœ€ä¼˜è§£åŒ…å«å­é—®é¢˜çš„æœ€ä¼˜è§£
2. Recursive definition of the value of an optimal solution
	let `c[i,j]` be the minimum number of scalar multiplications necessary to compute $A_{i\dots j}$ 
	minimum cost to compute $A_{i\dots n}$ is `m[1,n]`
	å‡è®¾ optimal parenthesization of $A_{i\dots j}$   splits the product between $A_k$ and $ğ´_{ğ‘˜+1}$ for some integer ğ‘˜ where $1 \le k < j$
	Cost of computing $A_{i\dots j}$ = cost of computing $A_{i\dots k}$ + cost of computing $A_{k+1\dots j}$ + cost of multiplying $A_{i\dots k}$ and $A_{k+1\dots j}$ ($d_{i-1}d_kd_j$)
	$$c[i,j]=
	\begin{cases}
	0 &if \text{ }i=j \\
	c[i,j]=min\{\ c[i,k]+c[k+1,j]+d_{i-1}\times d_k \times d_j \}\ &if \text{ }i<j \\
	\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }i\le k < j
	\end{cases}$$
3. Construct Optimal Solution
	Our algorithm computes the minimum-cost table ğ‘and the split table k
	æ¯ä¸ª entry `k[i,j]` shows where to split the product $A_iA_{i+1}\dots A_j$ for the minimum cost
	éœ€è¦åŠ¨æ€è§„åˆ’ä¸¤ä¸ªæ€§è´¨ï¼šOptimal Substructureï¼ˆæœ€ä¼˜å­ç»“æ„ï¼‰ï¼ŒOverlapping Subproblemsï¼ˆå­é—®é¢˜é‡å ï¼‰
- Number of ways for parenthesizing the matricesï¼š
	is very closely tied to the number of ways to write valid nested parentheses
	This sequence is called the Catalan numbers
	Now there are 'L' ways of parenthesizing the left sub-list and 'R' ways of parenthesizing the right sub-list then the Total will be L.R: $$p(n) = \begin{cases}1 &if \text{ } n=1\\ \sum\limits_{k=1}^{n-1}p(k)p(n-k) &if \text{ } n \ge 2\end{cases}$$
	Also, p(n)=c(n-1) where c(n) is the nth Catalan number: $$c_n = \frac{1}{n+1}\dbinom{2n}{n} = \frac{(2n)!}{(n+1)!n!}$$
- complexity:
	$\Theta(n^3)$ time and $\Theta(n^2)$ space
# Part 4
### The 0-1 Knapsack Problem
#### Knapsack Problem-Introduction
- Let U ={ $u_1,u_2,\dots ,u_n$ } be a set of n items to be packed in a knapsack of size C
- for $1\le j \le n$, let $s_j$ and $v_j$ be the size and value of the jth item, respectively
- here C and $s_j$, $v_j$, $1 \le j \le n$, are positive integers
- objective: fill the knapsack with some item from U whose total size is at most C and such that their total value is maximum
- assume æ¯ä¸ªitemå¤§å°æ²¡è¶…è¿‡C
- more formally, given U of n items, we want to find a subset S $\subseteq$ U such that $$\sum_{u_i\in S}v_i$$
  is maximized subject to the constraint $$\sum_{u_i \in S}s_i \le C$$
- given weights and values (profits) of n items, put these items in a knapsack of capacity W to get the maximum total value in the knapsack
- two versions of the problem:
	"0-1" knapsack problem: items are indivisible, you either take an item or not
	fractional knapsack problem: items are divisible,  you can take any fraction of an item
#### 0-1 Knapsack Problem
- å·²çŸ¥ä¸€ä¸ªèƒŒåŒ…æœ€å¤§å®¹é‡ Wï¼Œä¸”ä¸€ä¸ªç”±nä¸ªitems ç»„æˆçš„set S
- æ¯ä¸ªitem i has some weight $w_i$, and benefit value $P_i$ (All $w_i$ and P and W are integer values)
- é—®é¢˜å°±æ˜¯ï¼šæ€ä¹ˆæ‰“åŒ…èƒŒåŒ…æ‰èƒ½å®ç°æ‰“åŒ…ç‰©ä»¶æ€»ä»·å€¼æœ€å¤§å‘¢ï¼Ÿ
- naive wayç®€å•æ–¹æ³•: cycle through all $2^n$ subsets of the n items and pick the subset with a legal weight that maximizes the value of the knapsack
- running time is O($2^n$)
- ä¸è¿‡æˆ‘ä»¬å¯ä»¥æ‰¾ä¸€ä¸ªåŠ¨æ€è§„åˆ’çš„ç®—æ³•that will usually do better than this brute force technique
#### Recursive formula of the problem
- We derive a recursive formula for filling the knapsack as follows:
	let `V[i,j]` denote the value obtained by filling a knapsack of size j with items taken from the first i items { $u_1, u_2,\dots ,u_i$ } in an optimal way
	here the range of i is from 0 to n and the range of j is from 0 to C
	Thus, what we seek is the value `V[n,c]`

	`V[0,j]` is 0 for all values of i, å› ä¸ºèƒŒåŒ…é‡Œä»€ä¹ˆä¹Ÿæ²¡æœ‰
	`V[i,0]` is 0 for all values of j, å› ä¸ºæ²¡æœ‰ä¸œè¥¿å¯ä»¥æ”¾åˆ°å¤§å°ä¸º0çš„èƒŒåŒ…é‡Œ
	When both ğ‘– and ğ‘— are greater than 0, we have the following observation, which is easy to prove: $$V[i,j]=\begin{cases}0 &if \text{ }i=0 \text{ or } j=0,\\ V[i-1,j] &if \text{ } j<s_i,\\ max \{V[i-1,j],V[i-1,j-s_i]+v_i \} &if \text{ }i>0 \text{ and } j\le s_i.   \end{cases}$$
	`V[i-1,j]`: The maximum value obtained by filling a knapsack of size j with items taken form { $u_1,u_2,\dots ,u_{i-1}$ } only in an optimal way
	`V[i-1,j-`$s_i$`]` + $v_i$: The maximum value obtained by filling a knapsack of size j-$s_i$ with items taken from { $u_1,u_2,\dots ,u_{i-1}$ } in an optimal way plus the value of item $u_i$. This case applies only if j $\ge$ s, and it amounts to adding item $u_i$ to the knapsack
#### Dynamic Programming Approach
- first attempt might be to characterize a sub-problem as follows:
	let $S_i$ be the optimal subset of elements from { $l_0,l_1,\dots,l_i$ }
	æˆ‘ä»¬èƒ½å‘ç°çš„å°±æ˜¯ the optimal subset from the elements { $l_0,l_1,\dots,l_{i+1}$ } may not correspond to the optimal subset of elements from { $l_0,l_1,\dots,l_i$ } in any regular pattern
	The solution to the optimization problem for $S_{i+1}$ might not contain the optimal solution from problem $S_i$
- we must re-work the way we used to build the previous sub-problem:
	let `V[i,w]` represent the maximum total value of a subset $S_i$ with weight w
	ç›®æ ‡å°±æ˜¯æ‰¾åˆ° `V[n,W]`, where næ˜¯itemsæ€»æ•°é‡ï¼ŒWæ˜¯æœ€å¤§weightï¼Œè¿™ä¸ªèƒŒåŒ…å¯ä»¥æ‰¿è½½çš„
- recursive formula for subproblems: $$\begin{align}
V[i,w] &= V[i-1,w],\text{ if } w_i > w
\\
&=max \{ V[i-1,w],V[i-1,w-w_i]+P_i \}
\end{align}$$
- The best subset of $ğ‘†_ğ‘–$ that has the total weight ğ‘¤, either contains item ğ‘– or not.
	First case: $w-w_i <0$ 
		Item ğ‘– canâ€™t be part of the solution! If it was the total weight would be < 0, which is unacceptable.
	Second case: $w-w_i \ge 0$ 
		Then the item ğ‘– can be in the solution, and we choose the case with greater value.
	$$V(i,w) = \begin{cases}V(i-1,w)&if\text{ }w-w_i <0\\max(V(i-1,w),V(i-1,w-w_i)+P_i) &if \text{ }w-w_i \ge 0 \end{cases}$$
- We use an $(n+1) \times (C+1)$ table to evaluate the values of `V[i,j]`
- We only need to fill the table ğ‘‰ `[0..ğ‘›,0..ğ¶]` row by row using the formula.
- The time complexity of the algorithm is exactly the size of the table, Î˜ (ğ‘›C) or Î˜ (ğ‘›ğ‘Š) ,as filling each entry requires Î˜(1) time. $$T(n) = O(nÃ—C)$$
- It can also be easily modified so that it requires only Î˜(ğ¶) of space, as only the last computed row is needed for filling the current row.
### Floyd's Algorithm
#### Floyd's Algorithm-Introduction
- Floyd-Warshall Algorithm æ˜¯between all the pairs of vertices in a weighted graphæ‰¾æœ€çŸ­è·¯å¾„çš„ç®—æ³•
- for both the directed and undirected weighted graphs
- does not work for the graphs with negative cycles (Where the sum of the edges in a cycle is negative)(may contain negative edges)
- Let G = (V, E) be a directed graph in which each edge (ğ‘–, ğ‘—) has nonnegative length `ğ‘™[ğ‘–, ğ‘—]`.
- if there is no edge from vertex ğ‘– to vertex ğ‘—, then $ğ‘™[ğ‘–, ğ‘—] = âˆ$
- For simplicity, we will assume that ğ‘‰ = {1,2,...,ğ‘›}. 
- Let ğ‘– and ğ‘— be two different vertices in V . 
- Define $ğ‘‘_{ğ‘–,ğ‘—}^ğ‘˜$ to be the length of a shortest path from ğ‘– to ğ‘— that does not pass through any vertex in {ğ‘˜ + 1,ğ‘˜ + 2,...,ğ‘›}.
- Then, by definition, $ğ‘‘_{ğ‘–,ğ‘—}^n$  is the length of a shortest path from ğ‘– to ğ‘— , i.e., the distance from ğ‘– to ğ‘— .
- $$d^k_{i,j}=\begin{cases}
l[i,j] &if\text{ }k=0 \\
min \{ d_{i,j}^{k-1}, d_{i,k}^{k-1}+d_{k,j}^{k-1}\} &if\text{ }1\le k \le n
\end{cases}$$
- Summary
	a weight matrix, where:
		$l(i,j) = 0 \text{ }if \text{ }i=j$
		$l(i,j) = \infty \text{ }if\text{ there is no edge between i and j}$  
		$l(i,j) =$ "weight of edge"
	Principle of optimality (ä¸€ä¸ªé—®é¢˜çš„æœ€ä¼˜è§£åŒ…å«å…¶å­é—®é¢˜çš„æœ€ä¼˜è§£) is applies to shortest-path problems.
#### The subproblem
- How can we define the shortest distance $ğ‘‘_{ğ‘–,ğ‘—}$ in terms of â€œsmallerâ€ problems?
	One way is to restrict the paths to only include vertices from a restricted subset. â†’Initially, the subset is empty. 
	â†’Then, it is incrementally increased until it includes all the vertices
- Let$D^{(k)}[ğ‘–,ğ‘—]$ =weight of a shortest path from $ğ‘£_ğ‘–$ to $ğ‘£_ğ‘—$ using only vertices from { $ğ‘£_1,ğ‘£_2,â€¦,ğ‘£_ğ‘˜$ }as intermediate vertices in the path. 
	â†’D(0)=W 
	â†’D(n)=D which is the goal matrix 
- How do we compute D(k) from D(k-1)?
#### The Recursive Formula
- Case 1: A shortest path from $ğ‘£_ğ‘–$ to $ğ‘£_ğ‘—$ restricted to using only vertices from { $ğ‘£_1, ğ‘£_2, â€¦,ğ‘£_ğ‘˜$ } as intermediate vertices does not use $ğ‘£_ğ‘˜$. 
	â†’Then $D^{(k)}[ğ‘–,ğ‘—] = D^{(k-1)}[ğ‘–,ğ‘—]$. 
- Case 2: A shortest path from $ğ‘£_ğ‘–$ to $ğ‘£_ğ‘—$ restricted to using only vertices from { $ğ‘£_1, ğ‘£_2, â€¦,ğ‘£_ğ‘˜$ } as intermediate vertices does use $ğ‘£_ğ‘˜$. 
	â†’Then $D^{(k)}[ğ‘–,ğ‘—] = D^{(k-1)} [ğ‘–,ğ‘˜]+ D^{(k-1)}[ğ‘˜, ğ‘—]$.
- We conclude: $$D^{(k)}[i,j] = min \{D^{(k-1)}[i,j],  D^{(k-1)} [ğ‘–,ğ‘˜]+ D^{(k-1)}[ğ‘˜, ğ‘—]\}$$
#### The Algorithm
- solves the given recurrence in a bottom-up fashion
- uses n + 1 matrices $ğ·_0, ğ·_1, â€¦,ğ·_ğ‘›$ of dimension ğ‘› Ã— ğ‘› to compute the lengths of the shortest paths. 
- Initially, we set $ğ·_0[ğ‘–, ğ‘–]$ = 0 and $ğ·_0[ğ‘–,ğ‘—] = ğ‘™[ğ‘–,ğ‘—]$ if ğ‘– â‰  ğ‘— and (ğ‘–,ğ‘—) is an edge in G; otherwise, $ğ·_0[ğ‘–, ğ‘—] = âˆ$. 
- We make ğ‘› iterations such that after the ğ‘˜ğ‘¡â„ iteration, $ğ·_ğ‘˜[ğ‘–,ğ‘—]$ contains the value of a shortest length path from vertex ğ‘– to vertex ğ‘— that does not pass through any vertex numbered higher than ğ‘˜. 
- In the ğ‘˜ğ‘¡â„ iteration, we compute $ğ·_ğ‘˜[ğ‘–,ğ‘—]$ using the formula: $$D_k[i,j] = min \{D_{k-1}[i,j] ,D^{(k-1)} [ğ‘–,ğ‘˜]+ D^{(k-1)}[ğ‘˜, ğ‘—]\} $$
- The overall time complexity of the Floyd Warshall algorithm is $ğ‘‚(ğ‘›^3)$ where ğ‘› denotes the number of nodes in the graph.
- Its space complexity is $ğ‘‚(ğ‘›^2)$

# Part 5
### Optimal Binary Search Tree
#### Introduction
- A binary search tree is one of the most important data structures in computer science. A binary Tree is defined as a tree data structure where each node has at most 2 children, the left and right child.
- An optimal binary search tree (OBST) is a binary search tree which provides the smallest possible search time (or expected search time) for a given sequence of accesses (or access probabilities).
- One of its principal applications is to implement a dictionary, a set of elements with searching, insertion, and deletion operations.
- It focuses on reducing the cost of the search of the Binary Search Tree (BST)
- å¯èƒ½æ²¡æœ‰lowest height
- needs 3 tables to record probabilities, cost, and root.

- OBST has ğ‘› keys (representation $ğ‘˜_1,ğ‘˜_2,â€¦,ğ‘˜_ğ‘›$ ) in sorted order (so that $ğ‘˜_1 < ğ‘˜_2 < â‹¯ < ğ‘˜_ğ‘›$ ), and we wish to build a binary search tree from these keys. 
- For each $ğ‘˜_ğ‘–$ , we have a probability $ğ‘_ğ‘–$ that a search will be for $ğ‘˜_ğ‘–$. 
- Some searches may be for values not in $ğ‘˜_ğ‘–$, so we have ğ‘› + 1 â€œdummy keysâ€ $d_0, ğ‘‘_1, â€¦,ğ‘‘_ğ‘›$ not in $ğ‘˜_ğ‘–$. 
- $ğ‘‘_0$ represents all values less than $ğ‘˜_1$, and $ğ‘‘_ğ‘›$ represents all values greater than $ğ‘˜_ğ‘›$, for i =1,2,â€¦,ğ‘› âˆ’1, the dummy key $ğ‘‘_ğ‘–$ represents all values between $ğ‘˜_ğ‘–$ and $ğ‘˜_ğ‘–$+1. 
- ä¹Ÿå°±æ˜¯è¯´ $d_0$ ä»£è¡¨ ( $-\infty ,k_1$ ), $d_1$ ä»£è¡¨ï¼ˆ $k_1, k_2$ ï¼‰, $\dots$, $d_i$ ä»£è¡¨ ( $k_i, +\infty$ ) 
- The dummy keys are leaves (external nodes), and the data keys mean internal nodes.
- Successful Search: Finding the element within the tree. 
- Unsuccessful Search: not Finding the element within the tree and it is represented by dummy nodes. 
- Search could be successful or unsuccessful, in both cases we need to know the number of comparisons that we are doing, the number of comparisons is the cost of searching an element in BST
- The total number of possible Binary Search Trees with ğ‘› different keys can be calculated by:  $CountBST(n))=Catalan \text{ } number \text{ }C_n = \frac{(2n)!}{((n+1)!\times n!)}$
#### Formula & Prove
- The case of search is two situations, one is a success, and the other, is a failure. ïƒ¾We can get the first statement : $$\sum\limits_{i=1}^{n}p_i +\sum\limits_{i=1}^{n}q_i =1$$
  $p_i -> success$,   $q_i -> failure$
- å·²çŸ¥probabilities of searches for each key and each dummy keyï¼Œæˆ‘ä»¬å¯ä»¥determineæœŸæœ›cost of a search
- å‡è®¾the actual cost of a search is the number of nodes examined, i.e., the depth of the node found by the search in T plus 1.
- Then the expected cost of a search in T is: $$\begin{align} E[\text{search cost in T}]&= (i=1 \sim n)\sum p_i \times (depth_T(k_i)+1)  \\
+ (i=0 \sim &n)\sum q_i \times (depth_T(d_i)+1)  \\
 = 1 +(i&=1 \sim n)\sum p_i \times depth_T(k_i)  \\
+ (i=&0 \sim n)\sum q_i \times depth_T(d_i) \end{align}$$
  Where $depth_T$ denotes a nodeâ€™s depth in the tree T
- heightæœ€å°ä¸ä¸€å®šæœ‰æœ€å°costï¼ŒåŒæ—¶æœ€å¤§æ¦‚ç‡ä¸åœ¨leaveï¼Ÿ
#### Dynamic Approach
##### Step1: The structure of an OBST
- To characterize the optimal substructure of OBST, we start with an observation about subtrees.
- Consider any subtree of a BST. It must contain keys in a contiguous range $ğ‘˜_ğ‘–,â€¦,ğ‘˜_ğ‘—$, for some 1â‰¤ğ‘–â‰¤ğ‘—â‰¤ğ‘›. 
- A subtree that contains keys  $ğ‘˜_ğ‘–,â€¦,ğ‘˜_ğ‘—$ must have as leaves the dummy keys $ğ‘‘_{ğ‘–âˆ’1},â€¦,ğ‘‘_ğ‘—$
- Given keys  $ğ‘˜_ğ‘–,â€¦,ğ‘˜_ğ‘—$, å…¶ä¸­ä¸€ä¸ª $k_r (i\le r\le j)$ will be the root of ä¼˜åŒ–å­æ ‘ containing these keys
- left subtree of the root $k_r$ ä¼šåŒ…å« keys ( $k_i,\dots ,k_{r-1}$) and dummy keys ( $d_{i-1},\dots ,d_{r-1}$ )
- right subtree of the root $k_r$ ä¼šåŒ…å« keys ( $k_{i+1},\dots ,k_j$) and dummy keys ( $d_r,\dots ,d_j$ )
- If we examine all roots ğ‘˜ğ‘Ÿ,and determine all optimal binary search trees containing  $k_i,\dots ,k_{r-1}$ and those containing $k_{i+1},\dots ,k_j$, we will guarantee the finding of an OBST
##### Step2: A recursive solution
- We pick the subproblem domain as finding an OBST containing the keys $ğ‘˜_ğ‘–,â€¦,ğ‘˜_ğ‘—$, where ğ‘–â‰¥1,ğ‘—â‰¤ğ‘›, and ğ‘—â‰¥ğ‘–âˆ’1. (When ğ‘—= ğ‘–âˆ’1,there are no actual keys; we have just the dummy key $ğ‘‘_{ğ‘–âˆ’1}$)
- è®© $e[i,j]$ the expected cost of searching an OBST containing the keys $ğ‘˜_ğ‘–,â€¦,ğ‘˜_ğ‘—$. Ultimately, we wish to compute $ğ‘’[1,ğ‘›]$
- The easy case occurs when ğ‘—=ğ‘–âˆ’1. Then we have just the dummy key $ğ‘‘_{ğ‘–âˆ’1}$. The expected search cost is $ğ‘’[ğ‘–,ğ‘–âˆ’1]=ğ‘_{ğ‘–âˆ’1}$.
- For a subtree with keys $ğ‘˜_ğ‘–,â€¦,ğ‘˜_ğ‘—$ let us denote the sum of probabilities as: $$w(i,j) = (l = i \sim j) \sum p_l + (l = i-1 \sim j)\sum q_l$$
- Thus, if $ğ‘˜_ğ‘Ÿ$ is the root of an optimal subtree containing keys $ğ‘˜_ğ‘–,â€¦,ğ‘˜_ğ‘—$, we have: $$e[i,j] = p_r + (e[i,r-1] + W(i,r-1) + (e[r+1,j]+w(r+1,j))$$
- We rewrite $ğ‘’[ğ‘–,ğ‘—]$ as: $ğ‘’[ ğ‘–, ğ‘—] = ğ‘’ [ğ‘–, ğ‘Ÿ âˆ’ 1] +ğ‘’[ ğ‘Ÿ +1,ğ‘—] +ğ‘¤(ğ‘–,ğ‘—)$
- è¿™ä¸ªrecursive equationæ˜¯å‡è®¾æˆ‘ä»¬çŸ¥é“é‚£ä¸ªnodeè¢«ç”¨ä½œroot
- æ‰€ä»¥æˆ‘ä»¬é€‰æ‹©gives the lowest expected search costçš„rootï¼Œä¹Ÿå› æ­¤å¾—åˆ°final recursive formula: 
	$e[i,j]$ = 
		case 1: if $i\le j, i\le r\le j$
		$e[i,j] = min\{  e[i,r-1]+e[r+1,j]+w(i,j) \}$
		case 2: if j = i-1;
		$e[i,j]=q_{i-1}$
- To help us keep track of the structure of OBST, we define $rğ‘œğ‘œğ‘¡[ğ‘–,ğ‘—]$,for 1â‰¤ğ‘–â‰¤ğ‘—â‰¤ğ‘› to be the index ğ‘Ÿ for which $ğ‘˜_ğ‘Ÿ$ is the root of an OBST containing keys $ğ‘˜_ğ‘–,â€¦,ğ‘˜_j$
##### Step3: Computing the expected search cost of an OBST
- We store $ğ‘’[ğ‘–,ğ‘—]$ values in a table $ğ‘’[1. . ğ‘› + 1,0..ğ‘›]$.
	The first index needs to run to n+1 è€Œä¸æ˜¯nï¼Œå› ä¸ºè¦æœ‰åªå«dummy key $d_n$ çš„subtreeï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—å’Œå­˜å‚¨ $e[n+1,n]$
	The second index needs to start from 0 to have a subtree containing only the dummy key $ğ‘‘_0$, we need to compute and store $ğ‘’[1,0]$
	æˆ‘ä»¬åªç”¨entries $ğ‘’[ğ‘–, ğ‘—]$ for which ğ‘— â‰¥ ğ‘– âˆ’ 1
- We also use a table $ğ‘Ÿ[ğ‘–, ğ‘—]$, for recording the root of the subtree containing keys $ğ‘˜_ğ‘–, â€¦,ğ‘˜_ğ‘—$
	This table uses only the entries for which 1 â‰¤ ğ‘– â‰¤ ğ‘— â‰¤ ğ‘›
- We will need one other table for efficiency. Rather than compute the value of ğ‘¤(ğ‘–,ğ‘—) from scratch every time we are computing $ğ‘’_{ğ‘–,ğ‘—} ,we store these values in a table $ğ‘¤[1..ğ‘›+1,0..ğ‘›]$.
	For the base case, we compute for $1\le i \le n$: $$w[i,i-1]=q_{i-1}$$
	For $j \ge i$, we compute: $$w[i,j]=w[i,j-1]+p_j+q_j$$
##### Formulas
1. to calculate $e[i,j]$ $$e[i,j]=\begin{cases}
q_{i-1} &if \text{ }j=i-1 \\
min \{ e[i,r-1]+e[r+1,j]+w(i,j) \} \\
i\le r\le j \} & if \text{ }i\le j
\end{cases}$$
2. to calculate $w[i,j]$ $$w(i,j)=\sum\limits_{m=i}^{j}p_m + \sum\limits_{m=i-1}^{j}q_m$$
3. æŠŠæ‰€æœ‰ `r âˆˆ [i, j]` çš„ä»£ä»·ï¼ˆå°±æ˜¯ä¸Šé¢ç®—eçš„æ—¶å€™é‚£ä¸ªå…¬å¼ï¼‰éƒ½ç®—ä¸€éï¼Œè°æœ€å°ï¼Œå°±æŠŠ `r` å­˜è¿› `root[i][j]`
##### Complexity
- Running time $$\begin{align}
T(n)&=\sum\limits_{m=1}^{n}\sum\limits_{i=1}^{n-m+1}\sum\limits_{j=i}^{n-1+1}\Theta(1) \\
&=\sum\limits_{m=1}^{n}\sum\limits_{i=1}^{n-m+1}n=\sum\limits_{m=1}^{n}n^2\\
&\Theta(n^3)
\end{align}$$
  $T(n)=O(n^3)$
- The algorithmâ€™s space efficiency is quadratic; $O(ğ‘›^2)$













































