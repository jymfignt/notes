# Part 1
## Greedy Algorithm
### Greedy Algorithm- Introduction
- As dynamic programming algorithms, greedy algorithms are usually designed to solve optimization problemsï¼ˆæ‰¾æœ€å¤§å€¼æˆ–è€…æœ€å°å€¼ï¼‰ 
- Unlike dynamic programming algorithms, greedy algorithms consist of an iterative procedure that tries to find a local optimal solution.
	åŠ¨æ€è§„åˆ’ä¼šè®°å½•å†å²ï¼Œæ¯”è¾ƒæ‰€æœ‰å¯èƒ½çš„è·¯å¾„ï¼ˆä»£ä»·æ›´å¤§ï¼Œä½†æ›´ç¨³å¦¥ï¼‰
	è´ªå¿ƒç®—æ³•åªçœ‹å½“å‰å“ªä¸ªæœ€å¥½ï¼Œæ¯æ¬¡éƒ½åšå‡ºå½“å‰æœ€ä¼˜çš„å†³å®šï¼Œå®Œå…¨ä¸ç®¡åæœ
- In some instancesï¼ˆæ¯”å¦‚æ´»åŠ¨é€‰æ‹©ã€æœ€å°ç”Ÿæˆæ ‘ï¼‰, these local optimal solutions translate to global optimal solutions. However, in othersï¼ˆæ¯”å¦‚ 0-1 èƒŒåŒ…é—®é¢˜ï¼‰, they fail to give optimal solutions.  æœ‰äº›æƒ…å†µä¸‹ï¼Œæ¯æ­¥æ˜¯æœ€ä¼˜è§£ä¹Ÿä¸èƒ½ä¿è¯æ•´ä½“æ˜¯æœ€ä¼˜è§£ï¼Œå› ä¸ºæœ‰çš„æ˜¯éœ€è¦ç»„åˆæ¥çœ‹çš„
- A greedy algorithm makes a correct guess on the basis of little calculation without worrying about the future. Thus, it builds a solution step by step.
- Each step increases the size of the partial solution and is based on local optimization. 
- The choice made is that which produces the largest immediate gain while maintaining feasibility. 
- Since each step consists of little work based on a small amount of information, the resulting algorithms are typically efficient. é«˜æ•ˆä½†æœªå¿…å‡†ç¡®
- The hard part in the design of a greedy algorithm is proving that the algorithm does indeed solve the problem it is designed for éš¾ç‚¹å°±æ˜¯è¯æ˜è¿™ä¸ªç®—æ³•ç¡®å®å¯ä»¥è§£å†³é—®é¢˜ï¼ˆåˆ«æ¯ä¸€æ­¥é€‰æ‹©æå¾—æ›´å¤æ‚ï¼‰
- For many optimization problems, using dynamic programming to determine the best choices is overkill; simpler, more efficient algorithms will do.
- A greedy algorithm makes the choice that looks best at the moment. It makes a locally optimal choice in the hope this choice will lead to a globally optimal solution. **è´ªå¿ƒé€‰æ‹©æ€§è´¨ï¼ˆgreedy-choice propertyï¼‰**
- Greedy algorithms do not always yield optimal solutions, but for many problems they do.
### Greedy Algorithm- Summary
- It works in a top-down approach
- It may not produce the best result for all the problems, because it goes for the local best choice to produce the global best result.
### Greedy Algorithm- Problem Properties

æˆ‘ä»¬å¯ä»¥ç¡®å®šè¿™ä¸ªç®—æ³•èƒ½ä¸èƒ½ç”¨äºè§£å†³é—®é¢˜if the problem has the following properties:
1. Greedy Choice Property 
	If an optimal solution to the problem can be found by choosing the best choice at each step without reconsidering the previous steps once chosen, the problem can be solved using a greedy approach. ä¸Šæ–‡æåˆ°è¿‡ï¼Œno backtrackingï¼Œæ¯ä¸€æ­¥é€‰äº†å½“å‰æœ€å¥½çš„ï¼Œæœ€åå°±èƒ½å¾—åˆ°æœ€ä¼˜è§£
2. Optimal Substructure 
	If the optimal overall solution to the problem corresponds to the optimal solution to its subproblems, then the problem can be solved using a greedy approach. èƒ½æŠŠé—®é¢˜æ‹†æˆå­é—®é¢˜ï¼Œå¹¶ä¸”â€œå­é—®é¢˜çš„æœ€ä¼˜è§£â€ç»„æˆäº†â€œæ•´ä½“çš„æœ€ä¼˜è§£â€ã€‚è¿™ä¸ªæ€§è´¨ä¹Ÿæ˜¯åŠ¨æ€è§„åˆ’çš„å‰æï¼Œ
### Elements of the greedy strategy
The process of developing a greedy algorithm will go through the following steps: 
	1. Determine the optimal substructure of the problem. èƒ½æ‹†æˆå­é—®é¢˜æ‰èƒ½é€’å½’ç„¶ååç»­è§£å†³
	2. Develop a recursive solution. æ— æ‰€è°“å®ç°å¿«æ…¢ï¼Œå†™å‡ºä¸€ä¸ªé€’å½’è§£æ³•
	3. Show after making the greedy choice, only one subproblem remains to be solved, which maintains the structure of the original problem and ensures the greedy strategy is effective. è¦è¯æ˜ä¸€æ—¦åšå‡ºäº†è´ªå¿ƒé€‰æ‹©ï¼Œå‰©ä¸‹çš„é—®é¢˜è¿˜æ˜¯â€œåŸé—®é¢˜çš„ç¼©å°ç‰ˆæœ¬â€ï¼Œä½ å¯ä»¥ç»§ç»­ç”¨ç›¸åŒçš„è´ªå¿ƒç­–ç•¥è§£å†³ã€‚
	4. Prove that it is always safe to make the greedy choice. (Steps 3 and 4 can occur in either order. ä¸¤è€…æ˜¯ç‹¬ç«‹åˆ¤æ–­æ ‡å‡†ï¼Œæ— æ‰€è°“å…ˆå)  å°±æ˜¯æ‰€è°“çš„**è´ªå¿ƒé€‰æ‹©æ€§è´¨**
	5. Develop a recursive algorithm that implements the greedy strategy.  å†™å‡ºå…·ä½“é€’å½’å®ç°
	6. Convert the recursive algorithm to an iterative algorithm. æ”¹æˆè¿­ä»£ç®—æ³•
### Advantages of the Greedy Approach
1. The algorithm is easier to describe. 
2. This algorithm can perform better than other algorithms (but, not in all cases).
### Drawback of the Greedy Approach
- ä¸»è¦åŠ£åŠ¿å°±æ˜¯è´ªå¿ƒç®—æ³•ä¸æ˜¯æ€»æ˜¯produceæœ€ä¼˜è§£
ã€éœ€æ€»ç»“å·²ç»å­¦äº†çš„å“ªäº›é—®é¢˜é€‚ç”¨ã€‘
### Steps of the Greedy Approach
1. the solution set is empty è§£çš„é›†åˆä¸ºç©º
2. At each step, an item is added to the solution set until a solution is reached. æ¯ä¸€æ­¥é€‰å‡ºä¸€ä¸ªå€™é€‰è§£ï¼Œåˆ¤æ–­æ˜¯å¦æ»¡è¶³æ¡ä»¶
3. If the solution set is feasible, the current item is kept.  æ˜¯->åŠ å…¥
4. Else, the item is rejected and never considered again.  å¦->æ‹’ç»ä¸”æ°¸ä¸å†é€‰
### Different Types of Greedy Algorithm
1. Activity Selection Problem (Our next topic). 
2. Selection Sort 
3. Knapsack Problem 
4. Minimum Spanning Tree 
5. Single-Source Shortest Path Problem 
6. Job Scheduling Problem 
7. Prim's Minimal Spanning Tree Algorithm 
8. Kruskal's Minimal Spanning Tree Algorithm 
9. Dijkstra's Minimal Spanning Tree Algorithm 
10. Huffman Coding 
11. Ford-Fulkerson Algorithm
## Activity Selection Problem
- The problem of scheduling several competing activities requires exclusive use of a common resource, with a goal of selecting a maximum-size set of mutually compatible activities. å…±ç”¨ä¸€ä¸ªèµ„æºï¼Œé€‰å‡ºæœ€å¤šä¸ªäº’ç›¸ä¸å†²çªçš„æ´»åŠ¨ï¼Œå³ä¸é‡å 
>Suppose we have a set ğ‘† = { $ğ‘_1,ğ‘_2,â€¦,ğ‘_ğ‘›$ }of ğ‘› proposed activities that wish to use a resource, such as a lecture hall, which can serve only one activity at a time. 
>Each activity $ğ‘_ğ‘–$ has a start time $ğ‘ _ğ‘–$ and a finish time $ğ‘“_ğ‘–$, where 0 â‰¤ $ğ‘ _ğ‘– < ğ‘“_ğ‘– < âˆ$. 
>
>If selected, activity $ğ‘_ğ‘–$ takes place during the half-open time interval $[ğ‘ _ğ‘–, ğ‘“_ğ‘–]$. 
>Activities $ğ‘_ğ‘–$ and $ğ‘_ğ‘—$ are compatible if the intervals $[ğ‘ _ğ‘–, ğ‘“_ğ‘–]$ and $[ğ‘ _ğ‘–, ğ‘“_ğ‘–]$ do not overlap. ä¹Ÿå°±æ˜¯$ğ‘_ğ‘–$ and $ğ‘_ğ‘—$ are compatible if $ğ‘ _ğ‘– â‰¥ ğ‘“_j$ or $ğ‘ _ğ‘— â‰¥ ğ‘“_ğ‘–$. 
>In the activity-selection problem, we assume that the activities are sorted in an increasing order of finish time: $ğ‘“_1 â‰¤ ğ‘“_2 â‰¤ ğ‘“_3 â‰¤ â‹¯â‰¤ğ‘“_{ğ‘›âˆ’1} â‰¤ğ‘“_ğ‘›$.

å†æ¯”å¦‚Given a resource such as a CPU and ğ‘› set of activities, such as tasks that want to utilize the resource
The activity ğ‘– have the starting and finishing time which is denoted by $ğ‘ _ğ‘–$ and $ğ‘“_ğ‘–$.
### Steps of Activity Selection Algorithm
Step-1: Set $ğ‘“_ğ‘–$ i.e., finish time into non-decreasing order.  
Step-2: Select the next activity ğ‘– to the solution set if ğ‘– is compatible with each activity in the solution set. 
Step 3: Repeat step-2, until all the activities get examined.
### Activity Selection Problem-Time Complexity
GREEDY-ACTIVITY-SELECTOR schedules a set of ğ‘› activities in ğ‘‚(ğ‘›) time, assuming that the activities were already sorted initially by their finish times. ğ‘‡(ğ‘›)=ğ‘‚(ğ‘›) 
If the activities are not sorted, they should be sorted by using any sorting algorithm such as merge sort, or quick sort. 
In this case, the time complexity of the sorting algorithm it should be considered
# Part 2
## Knapsack Problem â€“ Greedy Algorithm
- Both greedy and dynamic programming strategies exploit optimal substructure, which can lead to mistakenly choosing a dynamic programming solution when a greedy one suffices, or vice versa
- 0-1 Knapsack ä¸æ»¡è¶³è´ªå¿ƒé€‰æ‹©æ€§è´¨ï¼Œåªèƒ½ç”¨åŠ¨æ€è§„åˆ’
### Fractional Knapsack Problem
- you can take fractions of items
- Both knapsack problems exhibit the optimal-substructure property.
	For the 0-1 problem, If we exclude item ğ‘— from the load, the remaining load should be the most valuable subset weighing at most ğ‘¤ âˆ’ $ğ‘¤_ğ‘—$ , chosen from the ğ‘›âˆ’1 original items excluding ğ‘—. 
	For the fractional problem, if we remove item ğ‘— weighing ğ‘¤ from the optimal load, the remaining load should be the highest-value subset weighing up to ğ‘Š âˆ’ ğ‘¤, selected from the ğ‘› âˆ’ 1 original items plus $ğ‘¤_ğ‘—$ âˆ’ ğ‘¤ pounds of item ğ‘—.
- Although the problems are similar, we can solve the fractional knapsack problem by a greedy strategy, but we cannot solve the 0-1 problem by such a strategy.

- To solve the fractional problem, we first compute the value per pound $ğ‘_ğ‘– / ğ‘¤_ğ‘–$ for each item. 
- in the greedy strategy, you can take as much as possible of the item with the greatest value per pound. 
- If the supply runs out of one item but can carry more, he takes as much as can of the item with the next highest value per pound, continuing this process until he reaches his weight limit ğ‘Š. 
- Thus, by sorting the items by value per pound, the greedy algorithm runs in O(ğ‘›ğ‘™ğ‘œğ‘”ğ‘›) time
### Knapsack Problem (Greedy Approach) â€“Time Complexity
ğ‘‡(ğ‘›) =ğ‘‚(ğ‘›ğ‘™ğ‘œğ‘”ğ‘›)
## Huffman Code
### File Compression
- Suppose we are given a file, which is a string of characters. 
- We wish to compress the file in such a way that the original file can easily be reconstructed. 
- Let the set of characters in the file be ğ¶ = { $ğ‘_1, ğ‘_2, .. . , ğ‘_ğ‘›$ }. 
- Let also ğ‘“( $ğ‘_ğ‘–$ ),1 â‰¤ ğ‘– â‰¤ ğ‘›, be the frequency of character $ğ‘_ğ‘–$ in the file, i.e., the number of times $ğ‘_ğ‘–$ appears in the file. 
- Using a fixed number of bits to represent each character, called the encoding of the character, the size of the file depends only on the number of characters in the file.
- Since the frequency of some characters may be much larger than others, it is reasonable to use variable-length encodings. 
- The characters with large frequencies should be assigned short encodings, whereas long encodings may be assigned to those characters with small frequencies. 
- When encodings vary in length, one character's encoding must not be the prefix of another's; these are called prefix codes.
**ç¦»æ•£è¿˜æ˜¯æ•°æ®ç»“æ„è®²è¿‡**
 - Once the prefix constraint is met, decoding is unambiguousæ²¡æ­§ä¹‰; the bit sequence is scanned until a character's encoding is found
 - To parse a bit sequence, use a full binary tree where each internal node has two branches labeled 0 and 1. 
 - The leaves in this tree correspond to the characters. 
 - Each sequence of 0â€™s and 1â€™s on a path from the root to a leaf corresponds to a character encoding. 
 - In follows we describe how to construct a full binary tree that minimizes the size of the compressed file.
### Encoding messages
- Encode a message composed of a string of characters. 
- Codes used by computer systems: 
	ASCII 
		â†’uses 8 bits per character 
		â†’can encode 256 characters. 
	Unicode 
		â†’16 bits per character 
		â†’can encode 65536 characters 
		â†’includes all characters encoded by ASCII 
- ASCII and Unicode are fixed-length codes 
	â†’all characters represented by the same number of bits
#### Drawbacks of fixed-length codes
æµªè´¹ç©ºé—´
	Unicode uses twice as much space as ASCII 
		â†’inefficient for plain-text messages containing only ASCII characters
Potential solution: use variable-length codes 
	â†’variable number of bits to represent characters when the frequency of occurrence is known 
	â†’short codes for characters that occur frequently
#### Advantages of variable-length codes
- The advantage of variable-length codes over fixed-length is short codes can be given to characters that occur frequently 
	â†’The length of the encoded message is less than the fixed-length encoding
- Potential problem: how do we know where one-character ends and another begins? 
	â†’Not a problem if the number of bits is fixed
### Huffman Coding
- The code constructed by the algorithm satisfies the prefix constraint and minimizes the size of the compressed file
- The algorithm consists of repeating the procedure until ğ¶ consists of only one character. 
- Let $ğ‘_ğ‘–$ and $ğ‘_ğ‘—$ be two characters with minimum frequencies. 
- Create a new node ğ‘ whose frequency is the sum of the frequencies of $ğ‘_ğ‘–$ and $ğ‘_ğ‘—$, and make $ğ‘_ğ‘–$ and $ğ‘_ğ‘—$ the children of ğ‘. 
- ğ¿ğ‘’ğ‘¡ ğ¶ = ğ¶ âˆ’ { $ğ‘_ğ‘–,ğ‘_ğ‘—$ } âˆª {ğ‘}.
- Huffmanâ€™s greedy algorithm uses a table of character frequencies to optimally represent each character as a binary string. 
- Huffman Coding is a Greedy Algorithm: 
	â†’It is used for the lossless compression of data. 
	â†’It uses variable length encoding. 
	â†’It assigns variable length code to all the characters. 
- The code length of a character depends on how frequently it occurs in the given text. 
- Prefix Rule: 
	â†’Huffman Coding implements a rule known as a prefix rule. 
	â†’This is to prevent ambiguities while decoding. 
	â†’It ensures that the code assigned to any character is not a prefix of the code assigned to any other character
### Huffman coding tree
- Binary tree 
	â†’Each leaf contains a symbol (character). 
	â†’Label the edge from the node to the left child with 0. 
	â†’Label the edge from node to the right child with 1 
- Code for any symbol obtained by the path from the root to the leaf containing the symbol. 
- Code has prefix property 
	â†’Leaf node cannot appear on the path to another leaf. 
	â†’Note: fixed-length codes are represented by a complete Huffman tree and have the prefix property
### Building a Huffman tree
1. Find the frequencies of each symbol occurring in the message. 
2. Begin with a forest of single-node trees. 
	â†’Each contains a symbol and its frequency 
3. Do recursively. 
	â†’Select two trees with the smallest frequency at the root. 
	â†’produce a new binary tree with the selected trees as children and store the sum of their frequencies in the root. 
4. Recursion ends when there is one tree 
	â†’This is the Huffman coding tree
### Huffman Coding Algorithm
- The main operations required to construct a Huffman tree are insertion and deletion of characters with minimum frequency, a min-heap is a good candidate data structure that supports these operations. 
- The algorithm builds a tree by adding n - 1 internal vertices one at a time; its leaves correspond to the input characters. 
- Initially and during its execution, the algorithm maintains a forest of trees. 
- After adding n-1 internal nodes, the forest is transformed into a tree: the Huffman tree. 
- Algorithm Huffman gives a more precise description of the construction of a full binary tree corresponding to a Huffman code.
### Time complexity
T(ğ‘›) =O($n\log{n}$)

# Part 3
## Dijkstraâ€™s Algorithm-Shortest Path Problem

The shortest path problem is about finding a path between two vertices in a graph such that the total sum of the weights of the edges is minimum. 
	â†’Minimization or Optimization problem. 
There are different methods to find the shortest path in a graph, for example: 
	â†’Depth-First Search (DFS). 
	â†’Breadth-First Search (BFS). 
	â†’Bidirectional Search. 
	â†’Dijkstraâ€™s Algorithm. 
	â†’Bellman-Ford Algorithm. 
The choice of the proper algorithm is based on the use-case.
### Dijkstraâ€™s Algorithm-Introduction
- Let G = (V, E) be a directed or undirected graph in which each edge has a nonnegative length, and there is a vertex s called the source. 
- The single source shortest path problem is to determine the distance from s to every vertex in V, where the distance from vertex s to vertex x is the length of a shortest path from s to x. 
- For simplicity, we will assume that V= {1, 2, . . . , n} and s= 1. 
- This problem can be solved by a greedy technique known as Dijkstraâ€™s algorithm. 
- Initially, the set of vertices is partitioned into two sets X = {1} and Y = {2, 3, . . . , n}
- The set X contains the vertices whose distance from the source has already been determined. 
- At each step, we select a vertex y âˆˆ Y whose distance from the source vertex has already been found and moved to X. 
- Associated with each vertex y in Y is a label `Î»[y]`, which is the length of a shortest path that passes only through vertices in X. 
- Once a vertex y âˆˆ Y is moved to X, the label of each vertex w âˆˆ Y that is adjacent to y is updated, indicating that a shorter path to w via y has been discovered. 
- For any vertex v âˆˆ V , `Î´[v]` will denote the distance from the source vertex to v. 
- At the end of the algorithm, `Î´[v] = Î»[v]` for each vertex v âˆˆ V
### Dijkstraâ€™s Algorithm-Greedy Algorithm
- Greedy strategy can be applied: 
	â†’Problem is solved in stages by taking one step and considering one output at a time to get the optimal solution. 
- Main Idea: 
	â†’Shortest path vertex is selected next. 
	â†’Update the shortest path of other vertices. 
- Any vertex can be selected as a source. 
- It can be applied to directed and undirected connected Graphs. 
- Dijkstra does not work for Graphs with negative weight edges. å¦‚æœæƒ³å¤„ç†è´Ÿè¾¹ï¼Œå¯ä»¥è€ƒè™‘ **Bellman-Ford** ç®—æ³•ã€‚
### Dijkstraâ€™s Algorithm-Algorithm Steps
1. Mark your selected initial node with a current distance of 0 and the rest with infinity. 
2. Set the non-visited node with the smallest current distance as the current node. 
	â†’For each neighbor N of your current node C: 
		â†’Add the current distance of C with the weight of the edge connecting C to N; if it is smaller than the current distance of N, set it as the new current distance of N. 
		â†’This called relaxation process. 
3. Mark the current node C as visited. 
4. If there are non-visited nodes, go to step 2.
### Dijkstraâ€™s Algorithm-Time Complexity
$Î˜(ğ‘š + ğ‘›^2) = Î˜(ğ‘›^2)$
### Improving the time bound
- Making a major improvement to Algorithm Dijkstra to lower its $Î˜(ğ‘›^2)$ time complexity to $ğ‘‚(ğ‘š\log{ğ‘›})$. 
- The basic idea is to use the min-heap data structure to maintain the vertices in the set Y so that the vertex y in Y closest to a vertex in ğ‘‰âˆ’ ğ‘Œcan be extracted in O(log n) time. 
- The key associated with each vertex v is its label `Î»[v]`.
### Dijkstraâ€™s Algorithm Applications 
â†’To find the shortest path. 
â†’In social networking applications. 
â†’In a telephone network. 
â†’To find the locations in the map.
## Kruskalâ€™s Algorithm for finding minimal spanning tree.
### Spanning Tree
- A spanning tree is a sub-graph of an undirected connected graph, which includes all the vertices of the graph with a minimum possible number of edges. 
- If a vertex is missed, then it is not a spanning tree. 
- The edges may or may not have weights assigned to them. 
- The total number of spanning trees with ğ‘› vertices that can be created from a complete graph is equal to $ğ‘›^{(ğ‘›âˆ’2)}$
### Kruskalâ€™s Algorithm for finding minimal Cost spanning tree
- Kruskalâ€™s Algorithm builds the spanning tree by adding edges one by one into a growing spanning tree. 
- Kruskalâ€™s algorithm follows the greedy approach as in each iteration it finds an edge that has the least weight and adds it to the growing spanning tree. 
- Kruskal's algorithm finds a minimum spanning tree by selecting a subset of a graph's edges to include every vertex. 
- The resulting tree will have the minimum sum of weights among all the trees that can be formed from the graph
### Kruskalâ€™s Algorithm â€“ Algorithm Steps:
1. Sort all the edges in non-decreasing order of their weight. 
2. Pick the smallest edge. Check if it forms a cycle with the spanning tree formed so far. If a cycle is not formed, include this edge. Else, discard it. 
3. Repeat step 2 until there are (ğ‘‰ âˆ’ 1) edges in the spanning tree
### Kruskalâ€™s Algorithm â€“ time Complexity
$ğ‘‚(ğ‘š \log{ğ‘›})$














