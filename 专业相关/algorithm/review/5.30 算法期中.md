Chapter 1-4
å¯èƒ½ä¼šæœ‰ä¼ªä»£ç 
# Chapter 1
## Lecture 1
### Several commonly used models: 
â†’Random Access Machine (RAM): Measures time and space, typical for traditional serial computers. 
â†’Parallel Random Access Machine (PRAM): Considers parallel time, number of processors, and read/write constraints. 
â†’Message Passing Model: Evaluates communication costs (number of messages) and computational costs. 
â†’Turing Machine: Focuses on time and space, representing an abstract theoretical model
### Algorithm analysis evaluates performance based on models and metrics such as: 
â†’Input Size: The length or amount of input data. 
â†’Running Time: The number of primitive operations executed. We typically focus on the worst-case running time unless stated otherwise. 
â†’Order of Growth: We analyze the growth rate of running time by considering only the leading term in a time formula (e.g., in $ğ‘›^2$ + 100ğ‘› + 5000, the leading term is $ğ‘›^2$).
â†’Theoretical analysis often uses asymptotic measures (Big O, Î©, Î˜) to estimate performance for large inputs.

### Steps for developing an Algorithm
1. Define the problem: State the problem you are trying to solve in clear and concise terms. 
2. List the inputs (Information needed to solve the problem) and the outputs (what the algorithm will produce as a result). 
3. Describe the steps needed to convert or manipulate the inputs to produce the outputs. Start at a high level first and keep refining the steps until they are effectively computable operations. 
4. Test the algorithm: choose datasets and verify that your algorithm works.

### Algorithm Phases
Design $\Rightarrow$ Analyse $\Rightarrow$ Implement $\Rightarrow$ Experiment

### Representing Algorithms

1. In terms of Natural Language. 
2. In terms of Formal Programming Language. 
3. In terms of Pseudocode. 
4. Flowcharts.

## Lecture 2

### Steps for finding Big-O 
1. Figure out what the input is and what ğ‘› represents. 
2. Express the maximum number of operations, the algorithm performs in terms of ğ‘›. 
3. Eliminate all excluding the highest order terms. 
4. Remove all the constant factors.

>ä¾‹1

$$f(n)=4n^2 + n +12$$

Solution:
- Big O for this function is: $O(n^2)$
- To prove this, we assume that:
$$\begin{align}
C &\ge 1 \\
C &= 5
\end{align}$$
$$\begin{align}
5n^2 &\ge 4n^2+n+12 \\
5n^2 &-4n^2-n-12=0\\
n^2 & -n-12=0 \\
(n - &4)(n+3)=0
\end{align}$$
- when $n_0$ = 4, both equations are equal
- when $n_0 > 4, 5n^2 > 4n^2 +n +12$

>ä¾‹2

```
void fun(int n){
   int i,j,k,count=0;
   for(i = n/2;i<=n; i++)
      for(j = 1; j<= n;2*j)
         for(k = 1;k<=n;k++)
            count++;
}
```
1. loop i
	Iteration 1        i=n/2
	Iteration 2        i=1+n/2
	Iteration 3        i=2+n/2
	Iteration 4        i=3+n/2
	Iteration k        i=k-1+n/2
k-1+n/2 = n $\Rightarrow$ k=n-n/2 = n/2+1
k = n/2 + 1, by removing the constant 1 and 1/2:
The time complexity of the first loop is **O(n)**
2. loop j
	Iteration 1        j=1
	Iteration 2        j=2
	Iteration 3        j=4
	Iteration 4        j=8
	Iteration k        j=$2^{k-1}$
$2^{k-1}$ = n $\Rightarrow$ k=$\log{n}$ +1, by removing the constant 1:
The time complexity of the second loop is **O(logn)**
3. loop k
	Iteration 1        k=1
	Iteration 2        k=2
	Iteration 3        k=3
	Iteration 4        k=4
	Iteration m       k=m
m = n
The time complexity of the third loop is **O(n)**

æ•´ä½“æ—¶é—´å¤æ‚åº¦å°±æ˜¯$O(n^2\log{n})$

# Chapter 2
## Part 1
### Probabilistic analysis definition
- é€šå¸¸ç”¨äºevaluateç®—æ³•è¿è¡Œæ—¶é—´ï¼Œæœ‰äº›ç®—æ³•åªèƒ½ç”¨æ¦‚ç‡åˆ†æï¼Œä¸èƒ½åªé deterministicç¡®å®šæ€§æ–¹æ³•ï¼ˆæ¯”å¦‚è¾“å…¥æ•°æ®æ˜¯éšæœºç”Ÿæˆçš„ï¼Œæ¯”å¦‚ç®—æ³•æœ¬èº«ä½¿ç”¨äº†éšæœºé€‰æ‹©ï¼ŒåƒéšæœºåŒ–å¿«é€Ÿæ’åºè¿™ç§ï¼‰
- Instead ofåªè€ƒè™‘æœ€å¥½æƒ…å†µå’Œæœ€åæƒ…å†µï¼Œprobabilistic analysis considers all cases by: 
	1. Assessing the probability distribution of each case.  è®¡ç®—æ¦‚ç‡ P(X)
	2. Calculating the expected runtime based on this distribution. æœŸæœ›è¿è¡Œæ—¶é—´ E(X) 
	 - This method can also be used to analyze various quantities.  æŸæ¬¡æ“ä½œæ¬¡æ•°ç­‰
	 - An example is estimating the hiring cost in the Hire-Assistant procedure. å…·ä½“ä¾‹å­è§£é‡Švarious quantitieså†…å®¹
### Hiring problem
>You are using an employment agency to hire a new assistant for your company. The agency works in such a way that it sends you ==one candidate per day.== You then interview the candidate after which you must immediately ==decide whether to hire== that person. Also, if you hire that person, you must ==fire your current office assistant==. Even if it is someone you have hired recently you must fire them before you hire a new assistant.

#### There are a few requirements to be fulfilled which are:

1. ä½ ä»»ä½•æ—¶å€™éƒ½æƒ³æ‹¥æœ‰æˆªæ­¢å½“å‰æœ€å¥½çš„å€™é€‰äºº
2. åªè¦è§‰å¾—é¢è¯•çš„å€™é€‰äººæ›´å¥½ï¼Œå¿…é¡»é›‡ç”¨è¿™ä¸ªå€™é€‰äººåŒæ—¶è§£é›‡ä¹‹å‰çš„å½“å‰çš„åŠ©ç†
3. ç¬¬ä¸€ä½å€™é€‰äººæ€»æ˜¯è¢«é›‡ç”¨
#### Here are a few points to be considered for the Cost Model:
1. æˆ‘ä»¬ä¸è€ƒè™‘é›‡ç”¨åŠ©æ‰‹çš„running time
2. We must determine the total cost of hiring the best candidate.
3. If ğ‘› candidates are interviewed and ğ‘š candidates are hired, then the cost would be $ğ‘›ğ‘_ğ‘– + ğ‘šğ‘_â„$, å…¶ä¸­ $nc_i$ æ˜¯æˆ‘ä»¬å¿…é¡»ä»˜å‡ºçš„æˆæœ¬ï¼Œä½† $mc_h$ æ˜¯å¯ä»¥æ§åˆ¶çš„ï¼Œméšç€å€™é€‰äººç­‰å¾…é¡ºåºçš„æ”¹å˜è€Œæ”¹å˜
#### Analysis
##### First
Cost to interview candidate is $ğ‘_ğ‘–$. 
Cost to hire a candidate is $ğ‘_h$.
Assume $ğ‘_ğ‘–$ is much less than $ğ‘_h$. 
Total cost: $O(ğ‘_ğ‘– \times n + c_h \times m)$, where ğ‘š=number of hirings

```pseudocode
Hire-Assistant(n) 

1 best = 0   // fictionalè™šæ„çš„ least qualified candidate 
2 for i= 1 to n 
3 interview candidate i       // paying cost ci 
4 if candidate i is better than candidate best 
5 best = i 
6 hire candidate i// paying cost ch
```

##### Second
é‚£ä¹ˆï¼Œå¯ä»¥é€šè¿‡æ£€æŸ¥æ¯ä¸ªå…ƒç´ å’Œæ”¹å˜the winner ğ‘š times æ¥find the maximum or minimum in a sequence.

$$cost=
\begin{cases}
\text{Best-case} : &O(c_in) \text{, when the agency sends us the best applicant on the first day}\\ 
\text{Worst-case} : &O(c_hn) \text{ when }c_h > c_i \text{, if each candidate is better than all who came before}\\
\text{Average-case} : & \text{Assume applicants come in random order. Each permutation of applicants is equally likely.}\\
\end{cases}$$

We can use a randomized algorithm to bring more control and ensure randomness in the sequence.


### Randomized Algorithms
- We can force all inputs to be equally likely by randomizing the input, ensuring a more balanced analysis
- An algorithm is randomized when part of its behavior is determined by a random number generator
- å¯¹äºhiring problemæ¥è¯´ï¼Œæˆ‘ä»¬éœ€è¦å»é€‰æ‹©é¢è¯•é¡ºåºï¼Œé‚£ä¹ˆæˆ‘ä»¬å†³å®šå»ä»¥éšæœºé¡ºåºé¢è¯•ä»–ä»¬ï¼Œthus making the average case the expected value
>In the hire-assistant problem, we can: 
> Randomly permute the list of candidates first. 
> Then, run the hiring algorithm
> This guarantees that for any input, the expected number of hires will be approximately $\ln{ğ‘›} +ğ‘‚(1)$

### Indicator Random Variables (IRV).
- Indicator Random Variables (IRV) are a useful tool in probabilistic analysis, especially when analyzing dependent events
- definition:
	An Indicator Random Variable is a variable that indicates whether a specific event has occurred. 
	It takes a value of: 
		1 if the event happens
		0 if the event does not happen.
- The expected value of $ğ¼_ğ´$ is the probability that event ğ´ occurs: ğ¸$(ğ¼_ğ´)$ =ğ‘ƒ(ğ´)

>ä¾‹1 Hiring Problem using IRV

Letâ€™s define $ğ¼_ğ‘–$ as the Indicator Random Variable for hiring candidate ğ‘–. 
$ğ¼_ğ‘–$ =1 if candidate ğ‘– is hired, and 0 otherwise.
The expected number of hires is: $$E[\text{number of hires}] = E [\sum\limits_{i=1}^{n}I_i] = \sum\limits_{i=1}^{n}E[I_i]$$
Since $$ğ¸[ğ¼_ğ‘–] = ğ‘ƒ \text{ (hiring candidate ğ‘–)} = \frac{1}{i}$$, the expected value gives us the average number of hires across all candidates
**Step 3: Total Expected Number of Hires**
$$\begin{align}
E[X] &= 1 + \frac{1}{2} + \frac{1}{3} + \dots + \frac{1}{n} \\
&= \sum\limits_{k=1}^{n}\frac{1}{k}   \text{ (harmonic series)} \\
&=\ln{n} + O(1)
\end{align}$$
where $\ln{n}$ is the natural logarithm of n, and O(1) is a constant.

>ä¾‹2 flipping a coin

åˆ†ä¸¤ç±»ï¼š
1. æŠ•æ·1æ¬¡ç¡¬å¸
only two possible outcomes: heads or tails.
**Step 1: Define the Indicator Random Variable (IRV)**

Letâ€™s define an indicator random variable ğ‘‹ for the event "the coin lands heads." The IRV will be defined as:
$$X = 
\begin{cases}
1, &\text{if head occurs}\\
0, &\text{if tail occurs}
\end{cases}$$

**Step 2: Compute the Expected Value**
$$E[X] = 1 \times P(heads) + 0 \times P(tails) = \frac{1}{2}$$
2. æŠ•æ· næ¬¡ç¡¬å¸
**Step 1: Define the Indicator Random Variable (IRV)**

Letâ€™s define an indicator random variable $ğ‘‹_ğ‘–$ for each flip ğ‘– (where ğ‘– = 1,2,â€¦,ğ‘›) to indicate whether heads occurs on the ğ‘– âˆ’ ğ‘¡â„ flip:
$$X_i = 
\begin{cases}
1, &\text{if i-th flip is heads}\\
0, &\text{if i-th flip is tails}
\end{cases}$$
**Step 2: Total Number of Heads**
$$X=X_1 + X_2 + \dots +X_n$$

**Step 3: Compute the Expected Value**
$$\begin{align}
E[X] &= E[X_1 + X_2 + \dots +X_n]=E[X_1] + E[X_2] + \dots + E[X_n]\\
&=\frac{1}{2} + \frac{1}{2} + \dots +\frac{1}{2}\\
&=\frac{n}{2}
\end{align}$$

>ä¾‹3 Using IRVs in Quicksort to Analyze Expected Comparisons 

In Quicksort, the key operation is comparing elements to a chosen pivot to partition the array into subarrays. The number of comparisons made directly affects the algorithm's performance.
Suppose we are sorting an array of ğ’ distinct elements using Quicksort.

**Step 1: Define the Indicator Random Variable (IRV)**
Letâ€™s define an Indicator Random Variable $ğ¼_{ğ‘–ğ‘—}$ for each pair of elements ğ‘– and ğ‘—, where ğ‘– < ğ‘—: 

$ğ¼_{ğ‘–ğ‘—}$ = 1 if elements ğ‘– and ğ‘— are compared at some point during the execution of Quicksort. 
$ğ¼_{ğ‘–ğ‘—}$ = 0 if elements ğ‘– and ğ‘— are never compared.

**Step 2: When Are Two Elements Compared?**
Two elements i and j are compared if and only if one of them is chosen as a pivot from the subarray containing both i and j.
å› æ­¤ï¼Œä¸¤å…ƒç´ è¢«æ¯”è¾ƒçš„æ¦‚ç‡å°±æ˜¯ä¸¤äººåœ¨åŒä¸€subarrayé‡Œä¸”ä¸€ä¸ªè¢«é€‰ä¸ºpivotçš„æ¦‚ç‡
ç”±äºæ¯ä¸ªå…ƒç´ éšæœºæˆä¸ºæ¢çº½ï¼Œ$$P(\text{i and j are compared}) = \frac{2}{j-i+1}$$
Thus, $$E[I_{ij}] = \frac{2}{j-i+1}$$
**Step 3: Total Expected Number of Comparisons**
$$E[\text{total comparisons}] = \sum\limits_{1\le i<j\le n} E[I_{ij}] = \sum\limits_{1\le i<j\le n} \frac{2}{j-i+1}$$
It turns out that this sum simplifies to: $$E[\text{total comparisons}] = 2n \ln{n} + O(n)$$

The expected number of comparisons in Quicksort is $2ğ‘› \ln{ğ‘›}$, which explains its average-case time complexity of $ğ‘‚(ğ‘› \log{ğ‘›})$ .

## Part 2

### Definition of Amortized Analysis 
- It calculates the average running time for each operation over the worst-case scenario.
- not a method to design algorithms, but rather a more accurate way to analyze algorithms under specific conditions
- ä¸åŒ…æ‹¬æ¦‚ç‡
$$\begin{cases}
\text{average-case analysis} &\text{considers all possible inputs}\\
\text{probabilistic analysis} &\text{involves all random choices}\\
\text{amortized analysis} &\text{looks at a sequence of operations}\\
\end{cases}$$
- assume worst-case input å¹¶ä¸”é€šå¸¸ä¸ç”¨randomness
### Three Methods of Amortized Analysis
1. **Aggregate Method**
æ‰¾åˆ°upper limit T(n), è®¡ç®—å¹³å‡cost as T(n)/n
2. **Accounting Method**
è®¡ç®—æ¯ä¸ªæ“ä½œçš„æˆæœ¬ï¼ˆæ—¢è¦è€ƒè™‘ç›´æ¥immediate timeå½±å“ï¼Œä¹Ÿè¦è€ƒè™‘å¯¹future operationsçš„å½±å“ï¼‰
assign æ¯ç§operation ä¸åŒçš„amortized cost
å¯¹æŸäº›æ“ä½œæ”¶å–è¿‡é«˜çš„è´¹ç”¨ï¼ˆoverchargeï¼‰ï¼Œå¹¶å°†å¤šä½™éƒ¨åˆ†å‚¨å­˜ä¸ºcredit
è¿™ä¸ªcreditå¯ä»¥åç»­ç”¨æ¥è¡¥å¿compensateæ˜‚è´µçš„æ“ä½œ
3. **Potential Method**
ç±»ä¼¼æ–¹æ³•2ä½†æ˜¯å‚¨å­˜creditå½“ä½œpotential energy
overcharge æ—©æœŸæ“ä½œæ¥cover futureæ˜‚è´µæ“ä½œ

### Examples: Dynamic Tables

- å¯¹äºthe size of a ==hash table== for insertions, éœ€è¦å¹³è¡¡ä¸¤å› ç´ ï¼šç©ºé—´å’Œæ—¶é—´
	If å“ˆå¸Œè¡¨æ¯”è¾ƒå¤§ï¼Œæ‰¾ä¸€ä¸ªitemä¼šæ›´å¿«ï¼Œä½†éœ€è¦æ›´å¤šmemoryï¼ˆspaceï¼‰
	If å“ˆå¸Œè¡¨æ¯”è¾ƒå°ï¼Œå ç”¨ç©ºé—´å°‘ï¼Œä½†éœ€è¦æ›´å¤šæ—¶é—´å»find/insert a itemï¼ˆå†²çªå¤ªå¤šå¯¼è‡´æ¯ä¸ªæ¡¶è£…äº†å¾ˆå¤šå…ƒç´ ï¼‰
- è§£å†³æ–¹æ³•ï¼šuse a ==dynamic table==, which grows in size when it becomes full

- Steps when the Dynamic table is ==full==: 
	Allocate memory for a table that is double the current size. 
	Copy all the contents from the old table to the new one. 
	Free the memory of the old table. 
	If space is available, simply insert the new item in the empty slot.

#### Simple analysis

If ç”¨==simple analysis==, an insertionçš„æœ€åæƒ…å†µæ˜¯ $O(n)$, total cost is $n\times O(n) = O(n^2)$
This analysis gives an upper bound, but not a tight upper bound for ğ‘› insertions as all insertions do not take ğœƒ(ğ‘›) time.

#### Aggregate Analysis
- let us try solving it by ==Aggregate Analysis==: $$\frac{Cost(\text{n opertaions})}{n} = \frac{Cost(\text{normal opertaions})}{n} + \frac{Cost(\text{expensive opertaions})}{n}$$
è¿™é‡Œnormal operationsæ˜¯ç›´æ¥æ’å…¥ï¼Œexpensive operationsæ˜¯double and copy

| No          | Table Size | Total Cost($C_i$) | Cost of Operations | Cost of Doubling and Copying |
| ----------- | :--------: | :---------------: | :----------------: | :--------------------------: |
| 1           |     1      |         1         |         1          |              0               |
| 2           |     2      |         2         |         1          |           1=$2^0$            |
| 3           |     4      |         3         |         1          |           2=$2^1$            |
| 4           |     4      |         1         |         1          |              0               |
| 5           |     8      |         5         |         1          |           4=$2^2$            |
| 6           |     8      |         1         |         1          |              0               |
| 7           |     8      |         1         |         1          |              0               |
| 8           |     8      |         1         |         1          |              0               |
| 9           |     16     |         9         |         1          |          8 = $2^3$           |
| ...         |    ...     |        ...        |        ...         |                              |
| $2^{n-1}$+1 |   $2^n$    |    1+$2^{n-1}$    |         1          |          $2^{n-1}$           |
$$
\begin{gather}
\text{Total cost } = \sum\limits_{i=1}^{n}c_i \\
\le n + \sum\limits_{j=0}^{\lfloor \log{n} \rfloor} 2^j \\
= n + \frac{2^{\lfloor \log{n} \rfloor +1}-1}{2-1} \\
= n + 2 \times 2^{\lfloor \log{n} \rfloor} - 1 \\
\le n + 2n -1 = 3n-1 = \Theta(n)
\end{gather}
$$
Thus, the average cost of each dynamic table operation is  Î˜(ğ‘›)/ ğ‘› = Î˜(1)
#### Accounting Method
- Idea: 
	Assign different charges to different operations. 
	The amount of the charge is called amortized cost. 
	Amortized cost is more or less than the actual cost. 
	When amortized cost >actual cost, the difference is saved in specific objects as credits. 
	The credits can be used by later operations whose amortized cost
- As a comparison, in aggregate analysis, all operations have same amortized costs
- The ==sum of all the amortized costs== in a sequence ==must be at least== the sum of ==all the actual costs== since we would like to bound the total cost of the sequence by the sum of amortized costs.
- Charge ğ‘–ğ‘¡â„ operation a fictitious amortized cost $ğ‘_ğ‘–'$ , where $1pays for 1unit of work (i.e., time)

- The bank balance must not go negative, we must ensure that, Let $ğ‘_ğ‘–'$ be the charge of ğ‘–ğ‘¡â„ operation then: $$\sum\limits_{i=1}^{n} c_i \le \sum\limits_{i=1}^{n} c_i'$$
  for all n.
  Thus, the total amortized costs provide an upper bound on the total true costs.
- Charge an amortized cost of $ğ‘_ğ‘–'$ = $3 for the ğ‘–ğ‘¡â„ insertion: 
	$1 pays for the immediate insertion. 
	$2 is stored for later table doubling.
	When the table doubles, $1 pays to insert a recent item, and $1 pays to move an old item.

|      **i**      |  1  |  2  |  3  |  4  |  5  |  6  |  7  |  8  |  9  | 10  |
| :-------------: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
|  **$size_i$**   |  1  |  2  |  4  |  4  |  8  |  8  |  8  |  8  | 16  | 16  |
|    **$c_i$**    |  1  |  2  |  3  |  1  |  5  |  1  |  1  |  1  |  9  |  1  |
| **$\hat{c_i}$** |  2  |  3  |  3  |  3  |  3  |  3  |  3  |  3  |  3  |  3  |
|  **$bank_i$**   |  1  |  2  |  2  |  4  |  2  |  4  |  6  |  8  |  2  |  4  |
The first operation costs only $2, not $3.
#### Potential Method
- Same as accounting method: something prepaid is used later
- Different from accounting method:
	The prepaid work not as credit, but as â€œpotential energyâ€, or â€œpotentialâ€. 
	The potential is linked to the entire data structure, not to individual elements within it
- The most powerful method ( and the hardest to use), it builds on a physical metaphor of potential energy
- Instead of associating costs or taxes with operations, we represent prepaid work as the potential that can be spent on later operations.
- let $D_i$ denote our data structure after i operations and let $\Phi_i$ denote its potential. Let $c_i$ denote the actual cost of the $ith$ operation (which changes $D_{i-1}$ into $D_1$). Then the amortized cost of the $ith$ operation, denoted $c_i'$ is defined to be the actual cost plus the increase in potential: $$c_i' = c_i + \Phi_i - \Phi_{i-1}$$
  å³ï¼ˆactual cost + potential changeï¼‰ ï¼Œå…¶ä¸­$\Phi_i - \Phi_{i-1}$ ä¸ºpotential difference
- æˆ‘ä»¬æ·»åŠ potentialæ˜¯å› ä¸ºæˆ‘ä»¬æƒ³è¦amortized costæ€»æ˜¯å¤§äºactual cost
- $$\begin{align}
 \sum\limits_{i=1}^{n}c_i' &= \sum\limits_{i=1}^{n}(c_i + \Phi(D_i) - \Phi(D_{i-1})) \\
 &= \sum\limits_{i=1}^{n}c_i + \Phi(D_n) - \Phi(D_0)
 \end{align}$$
    $\Phi(D_i)$ is called the potential of $D_i$
    a potential function is valid if $\Phi_i - \Phi_0 \ge 0$  for every i, then the total actual cost of any sequence of operations is always < the total amortized cost: $$\sum\limits_{i=1}^{n}c_i = \sum\limits_{i=1}^{n}c_i' - \Phi_n \le \sum\limits_{i=1}^{n}c_i'$$
    å®šä¹‰$\Phi(D_0) = 0$ æ˜¯å¾ˆæ–¹ä¾¿çš„ï¼Œè¿™æ ·  $\Phi(D_i) \ge 0$ é€‚ç”¨äºæ‰€æœ‰iï¼Œ$c_i'$ is an overcharge

 - $\Phi = 2 \times \text{no of items in the array - capacity of the array}$
 
| No          | Table <br>Size | Cost of <br>Operations | Cost of <br>Doubling<br>and <br>Copying | Total <br>Cost<br>($C_i$) |           $$\Phi$$           | Amortized Cost<br>$C_i +\Phi_i - \Phi_{i-1}$ |
| ----------- | :------------: | :--------------------: | :-------------------------------------: | :-----------------------: | :--------------------------: | :------------------------------------------: |
| 1           |       1        |           1            |                    0                    |             1             |     2 $\times$ 1 - 1 = 1     |                1 + 1 - 0 = 2                 |
| 2           |       2        |           1            |                    1                    |             2             |     2 $\times$ 2 - 2 = 2     |                2 + 2 - 1 = 3                 |
| 3           |       4        |           1            |                    2                    |             3             |     2 $\times$ 3 - 4 = 2     |                3 + 2 - 2 = 3                 |
| 4           |       4        |           1            |                    0                    |             1             |     2 $\times$ 4 - 4 = 4     |                1 + 4 - 2 = 3                 |
| 5           |       8        |           1            |                    4                    |             5             |     2 $\times$ 5 - 8 = 2     |                5 + 2 - 4 = 3                 |
| 6           |       8        |           1            |                    0                    |             1             |     2 $\times$ 6 - 8 = 4     |                1 + 4 - 2 = 3                 |
| 7           |       8        |           1            |                    0                    |             1             |     2 $\times$ 7 - 8 = 6     |                1 + 6 - 4 = 3                 |
| 8           |       8        |           1            |                    0                    |             1             |     2 $\times$ 8 - 8 = 8     |                 1 + 8 - 6= 3                 |
| 9           |       16       |           1            |                    8                    |             9             |     2 $\times$ 9 - 16= 2     |                9 + 2 - 8 = 3                 |
| ...         |      ...       |          ...           |                                         |            ...            |             ...              |                     ...                      |
| $2^{n-1}$+1 |     $2^n$      |           1            |                $2^{n-1}$                |        1+$2^{n-1}$        | 2 $\times 2^{n-1}+2-2^n$ = 2 |           1+$2^{n-1}+2-2^{n-1}$= 3           |

### Empirical Analysis 

 In practice, æˆ‘ä»¬ç»å¸¸ç”¨ç»éªŒåˆ†æè€Œä¸æ˜¯ç†è®ºåˆ†ææ¥æ¯”è¾ƒç®—æ³•


#### Issues to consider
- introduce many more factors:
	test platform (hardware, language, compiler)
	Measures of performance (what to compare) 
	Benchmark test set (what instances to test on) 
	Algorithmic parameters 
	Implementational details
- Practical considerations prevent complete testing.ï¼ˆèµ„æºæœ‰é™ï¼Œè€Œå› ç´ è¾ƒå¤šï¼Œæ— æ³•æŠŠæ‰€æœ‰ç»„åˆéƒ½æµ‹è¯•ä¸€éï¼‰

#### Measures of Performance
- ç›®å‰ï¼Œwe focus on sequential algorithms
- ç›®æ ‡ï¼š
	æ¯”è¾ƒä¸¤ç®—æ³•
	improveæ”¹è¿› the implementationå®ç°æ–¹å¼ of a single algorithm
- å¸¸è§çš„æ€§èƒ½è¡¡é‡æ–¹æ³•ï¼š
	empirical running time ï¼ˆCPU time, wall-clockï¼‰
	representative operation countsï¼ˆä¾‹å¦‚æ’åºä¸­çš„æ¯”è¾ƒæ¬¡æ•°ï¼‰

#### Measuring Time

| ç±»å‹              | æ„ä¹‰                 | ç”¨é€”            |
| --------------- | ------------------ | ------------- |
| User Time       | ç¨‹åºåœ¨ç”¨æˆ·æ¨¡å¼ä¸‹æ¶ˆè€—çš„ CPU æ—¶é—´ | ä¸€èˆ¬ç”¨äºç®—æ³•æ€§èƒ½æµ‹é‡    |
| System Time     | ç³»ç»Ÿè°ƒç”¨å ç”¨çš„æ—¶é—´ï¼ˆå†…æ ¸æ¨¡å¼ï¼‰    | å¦‚æœæœ‰å¤§é‡ I/O è¦æ³¨æ„ |
| Wall-clock Time | å®é™…ä»å¼€å§‹åˆ°ç»“æŸçš„æ—¶é—´        | å¹¶è¡Œç®—æ³•æ—¶æœ‰å‚è€ƒä»·å€¼    |
ä¸²è¡Œç®—æ³•æ›´å…³æ³¨ User Timeï¼Œä½†è¦å°å¿ƒå®ƒå¿½ç•¥äº† I/O ç­‰éƒ¨åˆ†

#### Representative Operation Counts

æœ‰äº›æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯èƒ½æƒ³è¦æ•°æ“ä½œæ¬¡æ•°ï¼Œè€Œä¸æ˜¯æ—¶é—´
	identify bottlenecks
	a counterpart to the theoretical analysis
å“ªäº›æ“ä½œå¯ä»¥æ•°ï¼š
	function calls and executions of individual lines of code to identify bottlenecks
	we may know a prioriå…ˆéªŒ çŸ¥é“æˆ‘ä»¬æƒ³è¦æµ‹é‡ä»€ä¹ˆï¼ˆä¾‹å¦‚æ’åºä¸­çš„æ¯”è¾ƒå’Œäº¤æ¢ï¼‰

#### Test Set
- é€‰å¯¹è®­ç»ƒé›†å¾ˆé‡è¦ï¼Œto allow proper conclusions to be drawn
- must pay attention to their size, inherent difficultyå›ºæœ‰éš¾åº¦ and å…¶ä»–ç»“æ„æ€§ç‰¹æ€§
- å¦‚æœæˆ‘ä»¬å°è¯•åˆ†æ¸…å¤šæ ·ç®—æ³•ï¼Œé‚£é€‰å¯¹è®­ç»ƒé›†å¾ˆé‡è¦

#### Comparing Algorithms

- Given a performance measure and a test set, the question still arises of how to decide which algorithm is â€œbetter.â€ 
- We can do the comparison using some sort of summary statistic. 
	Arithmetic mean 
	Geometric mean 
	Variance 
- These statistics hide information useful for comparison.

#### Scientific Method

- Empirical analysis:  Analyze running time based on observations and experiments. 
- Apply the scientific method: 
	1. Make observations and gather data. 
		- Make observations: 
			Run the program multiple times, systematically increasing the input size for each run.
		- Gather data: 
			Time each program run and record the elapsed time along with the associated input size
	2. Hypothesize an explanation for the observations. æå‡ºå‡è®¾ 
	3. Make predictions based on the hypothesis. 
	4. FormulateæŒ‡å®š an experiment to see if the predictions occur. 
	5. Analyze experimental results to confirm or falsify the hypothesis
#### Empirical versus Theoretical Analysis 

- For sequential algorithms, asymptotic analysis is often good enough for choosing between algorithms. It is less ideal with respect to the tuning of implementation details.
- For parallel algorithms, asymptotic analysis is far more problematic. 
	The details not captured by the model of computation can matter much more. There is an additional dimension on which we must compare algorithms: for example, scalability


# Chapter 3
## Part 1
### General introduction to recursion. 
- A recurrence is an ==equation== that describes a ==function== based on its value at ==smaller inputs==.
- Solving a recurrence means finding a function that works for ==all natural numbers== and satisfies the recurrence
- When solving recurrences, we often ==ignore== floors, ceilings, and boundary conditions
- A recursive function is a function that ==calls itself== until a â€œ==base condition==â€ is ==true==, and execution stops.
- [x] æ•´æ•°çš„é˜¶ä¹˜
- [x] æ•´æ•°çš„æ–æ³¢é‚£å¥‘
### The complexity of the recursive algorithm 
->simplify recurrence -> Obtain a formula without recurrence
#### Simplifying recurrences 
1. Iteration Method (-> recursion tree method)
2. Substitution Method (aka, induction method)
3. Master Method

#### The iteration method
- to ==expand== the recurrence and express it as a ==summation of terms== dependent only on ==*ğ‘›*== and the ==initial condition==
- Use back-substitution to express the recurrence in terms of ğ‘› and the initial (boundary) condition.
- [x] ä¾‹é¢˜ç»ƒä¹  ï¼ˆP16-23ï¼‰
#### The substitution method
- It involves ==guessing the form== of the solution and then using mathematical ==induction== to find the ==constants== and show that the solution works.
- The substitution method can be used to establish either ==upper or lower bounds== on a recurrence.
- Two-steps solution: 
	1. **Guess the exact form of the solution**: Typically, one would precompute a set of numerical results for different input  nâ€™ğ‘  and derive a formula. 
	2. **Prove the correctness of the guess**: Mathematical induction is the tool of choice in most cases.
- Making a good guess:
	Some heuristics can help us make a good guess.
	Prove the loose upper and lower bounds on the recurrence and then reduce the range of uncertainty.
	Revising the guess by subtracting a lower-order term often permits the math to go through.
- [x] ä¾‹é¢˜2é“ P27,P28
- [x] ä¾‹é¢˜ã€ç»ƒä¹  P30-37

## Part 2

### Recursion tree method 
- A recursion tree is a tree where each node represents the cost of a certain recursive subproblem.
>We would usually use a recursion tree to generate possible guesses for the runtime, and then use the substitution method to prove them.

- We always consider the second term in recurrence as the root.
- We sum the costs within each of the levels of the tree to obtain a set of pre-level costs and then sum all pre-level costs to determine the total cost of all levels of the recursion.
- Steps to Solve Recurrence Relations Using Recursion Tree Method:
	1. ==Draw== the recursion tree for the given recurrence relation. 
	2. Calculate the ==height== of the recursion tree formed. 
	3. Calculate the ==cost== (time required to solve all the subproblems at a level) at ==each level==. 
	4. Calculate the ==total number of nodes== at ==each level== in the recursion tree. 
	5. ==Sum up== the ==cost== of all the levels in the recursion tree
- [x]  Example 1 (ä¸¤ç§æ–¹æ³•) $T(n) = 2T(n/2) +cn$
- [x]  $T(n) = T(n/3) + T(2n/3) + O(n)$
- [x]  $T(n) = 3T(n/4) + cn^2$
- Geometric progression
$$a + ar + ar^2 + ar^3 + \dots = \sum\limits_{k=0}^{\infty}ar^k = \frac{a}{1-r},~{} for ~{}|r|<1$$
### Master Method 
#### Method 1
$$T(n) = aT(\frac{n}{b}) + f(n) \text{  with } a\ge 1 \text{  and } b > 1$$
â†’ğ’‚ represents how many recursive calls are made, (Binary search has 1 split, while Merge sort has 2 split, etc.) 
â†’ğ’ƒ represents the factor by which the work is reduced in each recursive call, (for example, Binary search and merge sort cut input in half). 
â†’ğ’‡(ğ’) reparents how much work is done by each call apart from the recursion, as a function of ğ‘›. for example ğ‘‚(ğ‘›),ğ‘‚(1).

$$\begin{cases}
T(n)= \theta(n^{\log_b{(a)}}) \text{ for }\epsilon > 0 &\text{if }f(n) = O(n^{\log_b{(a)}-\epsilon}) \\
T(n)= \theta(n^{\log_b{(a)}}\log{(n)})  &\text{if }f(n) = \Theta(n^{\log_b{(a)}}) \\
T(n) = \theta(f(n)) &\text{if }f(n) = \Omega(n^{\log_b{(a)}+\epsilon})
\end{cases}$$
>for case 3, f(n) should satisfy the regularity condition: $$af(\frac{n}{b}) \le cf(n), \text{ğ‘¤â„ğ‘’ğ‘Ÿğ‘’ ğ‘ <1(This is always holds for polynomials)}$$

- Steps:
	**Extract** a, b and f(n) from the given recurrence. 
	**Determine** $n^{\log_b{(a)}}$ . 
	**Compare** ğ‘“(ğ‘›) and $n^{\log_b{(a)}}$ asymptomatically. 
	**Determine** the appropriate Master Method case and apply it.
- [ ] $T(n) = 2T(\frac{n}{2}) + n$
- [ ] $T(n) = 9T(\frac{n}{3}) + n$
- [ ] $T(n) = 3T(\frac{n}{4}) + n\log{n}$
- [ ] What the runtime of this recursion? (P32)
#### Method 2
$$T(n) = aT(\frac{n}{b}) + f(n)$$
We assume that: $$a\ge 1, b>1, f(n) = \Theta(n^k \log^p{n})$$
Case 1: $\text{if }\log_b{a} > k, \text{ then } \Theta(n^{\log_b{a}})$
Case 2: if $\log_b{a}$ = ğ‘˜,then we will have 3 sub-cases:
	a) If ğ‘ > âˆ’1, then $\Theta(n^k \log^{p+1}{n})$
	b) If p = -1, then  $\Theta(n^k \log{\log{n}})$
	c) If p < -1, then  $\Theta(n^k)$
Case 3: if $\log_b{a}$ < ğ‘˜,then we will have 2 sub-cases:
	a) If p $\ge$ 0, then $\Theta(n^k \log^p{n})$
	b) If p < 0, then  $\Theta(n^k)$
- [x] $T(n) = 2T(\frac{n}{2}) +1$
- [x] 6é“ç»ƒä¹ é¢˜ï¼ˆP37ï¼‰

# Chapter 4

## Part 1
### Divide and Conquer Algorithms
- In this approach, we solve a problem recursively by applying 3 steps: 
	1. ==DIVIDE==-break the problem into several sub-problems of smaller size. 
	2. ==CONQUER==-solve the problem recursively. 
	3. ==COMBINE==-combine these solutions to create a solution to the original problem
-  top-down approach
-  Recursion is usually adopted
- Algorithms using Divide and conquer Technique:
	1. Binary Search
	2. Merge Sort
	3. Quick Sort
	4. Finding maximum and minimum
	5. Strassen's matrix multiplication

### Linear (Sequential) Search
- ä¸æ˜¯åˆ†è€Œæ²»ä¹‹ç®—æ³•
- 3 popular search algorithms:
	linear search
	binary search
	jump search
- Every item is checked and if a match is found then that item is returned, otherwise, the search continues till the end of the data collection.
- input:
	search value(key)
	data set
- time complexity:
	best case O(1)
	worst case O(n)
	average case O(n/2)
### Binary Search
- éœ€è¦the list of elements æœ‰åºæ’åˆ—
##### Pros and cons
- advantage:
	1. searches several times faster than the linear search
	2. In each iteration, it reduces the number of elements to be searched from n to n/2
- Disadvantage:
	åªèƒ½ç”¨äºæœ‰åºlist
##### Analysis
let ğ‘‡(ğ‘›) denote the number of comparisons performed by Algorithm binary-search in the worst case on an array of size ğ‘›, then ğ‘‡(ğ‘›) can be expressed by the recurrence: $$T(n) = \begin{cases}
1 & n = 1\\
T(\frac{n}{2})+1 & n> 1
\end{cases}$$
##### Rewrite
- [ ] ä¼ªä»£ç 
- [ ] C++ program
### Merge Sort
- Efficient sorting is important for optimizingä¼˜åŒ– the efficiency of other algorithms(such as search and merge algorithms)that require input data to be in sorted lists
- The output of any sorting algorithm must satisfy two conditions: 
	1. The output is in nondecreasing order(each item is no smaller than the previous item). é€’å¢æˆ–é€’å¢è¶‹åŠ¿
	2. The output is a permutation(a reordering yet retaining all the original items)of the input. ä¸ä¸¢å¤±ä¸æ·»åŠ å…ƒç´ 
- top-down approach
- [ ] ä¼ªä»£ç 
- [ ] C++ program
- implicit binary tree éšå¼äºŒå‰æ ‘ ä¸éœ€è¦æŒ‡é’ˆæ¥è®°å½•å…ƒç´ é—´å…³ç³»ï¼Œå…ƒç´ é—´å…³ç³»æ˜¯æœ‰å…ƒç´ æœ¬èº«å†³å®šçš„ï¼Œéšè—çš„
- This chain of calls corresponds to a ==preorder== traversal of the tree: Visit the **root**, the **left subtree**, and then the **right subtree**.
- The computation, **however**, corresponds to a ==post-order== traversal of the tree: Visit the **left subtree**, the **right subtree**, and then the **root**.
- å®æ–½ traversal, éœ€è¦ç”¨stack
- mergesort å·¦&å³ ç„¶åmergeä¸¤ä¸ªç»“æœ
##### Time complexity
- the number of comparison: at least n1? at most n-1
- the number of assignment: 2n
- å’ŒäºŒå…ƒæŸ¥æ‰¾ä¸€æ ·ï¼Œbasic operationä¹Ÿæ˜¯element comparisonï¼Œä¹Ÿå°±æ˜¯è¯´è¿è¡Œæ—¶é—´æ˜¯å’Œå…ƒç´ æ¯”è¾ƒæ¬¡æ•°æˆæ­£æ¯”
- we wish to compute the number of ==element comparisons== ğ¶(ğ‘›)ï¼šæœ€å°æ¯”è¾ƒæ•°ï¼Ÿ$$C(n) = \begin{cases}
 0 &\text{if }n = 1, \\
2C(n/2)+n/2 &\text{if }n \ge 2
\end{cases}$$
   æ ¹æ®Corollary 1.2ï¼Œd=0, a=c=2, and b=1/2 å¯å¾—$$C(n) = \frac{n\log{n}}{2}$$
	Corollary 1.2 $\text{Let a and c be nonnegative integers, b and d nonnegative}$ $\text{constants, and let n = }c^k,\text{ for some nonegative integer k. Then, the}$ $\text{solution to the recurrence}$
	$$f(n) = \begin{cases}
	d & \text{if n = 1,} \\
	af(n/c)+bn &\text{if n}\ge 2
	\end{cases}$$
	$$\begin{align}
	&f(n) = bn\log_{c}{n} + dn  \text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ } \text{ if }a = c \\
	&f(n) = (d + \frac{bc}{a-c})n^{\log_{c}{a}} - (\frac{bc}{a-c})n \text{ }\text{ }\text{ if a} \ne c
	\end{align}$$
- The ==maximum number of comparisons== done by the algorithm is given by the recurrence $$C(n) = \begin{cases}
 0 &\text{if }n = 1, \\
2C(n/2)+n-1 &\text{if }n \ge 2
\end{cases}$$
  ç”¨iteration methodï¼Œ
   $$\begin{align}
   C(n)&=2C(n/2)+n-1 \\
   C(n)&=2[2C(n/4)+n/2+1]+n-1 \\
   C(n)&=4C(n/4)+n+2+n-1 \\
   &=2^k C(n/2^k)+ kn -(2^k-1)\\
   (n = &2^k -> k = \log{n})\\
   &=nC(1)+n\log{n}-n+1\\
   &=n\log{n}-n+1
   \end{align}$$
- The total number of element comparisons performed by algorithm mergesort to æ’åºä¸€ä¸ªå¤§å°ä¸ºnçš„æ•°ç»„ï¼Œwhere n is a power of 2, is between ä¸Šé¢ä¸¤ç§ç»“æœï¼ˆ$\frac{n\log{n}}{2}$, $n\log{n} -n +1$ï¼‰ä¹‹é—´
- å¦‚æœ n æ˜¯ä»»æ„arbitraryæ­£æ•´æ•°ï¼ˆä¸ä¸€å®šæ˜¯ 2 çš„å¹‚ï¼‰,æ¯”è¾ƒæ¬¡æ•°æ»¡è¶³ä»¥ä¸‹é€’æ¨å…³ç³»ï¼š$$C(n) = \begin{cases}
 0 &\text{if }n = 1, \\
C(\lfloor n/2\rfloor)+C(\lceil n/2\rceil)+ bn &\text{if }n \ge 2
\end{cases}$$
where b is nonnegative
é‚£ä¹ˆç”¨ä¸»å®šç†å³å¯å¾—$C(n) = \Theta(n\log{n})$
- ç”±äºå…ƒç´ æ¯”è¾ƒçš„æ“ä½œæ˜¯basic operationï¼Œå› æ­¤mergesortç®—æ³•çš„è¿è¡Œæ—¶é—´æ˜¯ï¼š$$T(n) = \Theta(n \log{n})$$
- å½’å¹¶æ’åºçš„è¿è¡Œæ—¶é—´(æœ€åæƒ…å†µä¸‹)è¾¾åˆ°äº†æ‰€æœ‰æ¯”è¾ƒæ’åºç®—æ³•çš„æœ€ä¼˜ä¸‹ç•Œ $\Omega(n \log n)$ï¼Œå› æ­¤å®ƒæ˜¯**ç†è®ºä¸Šæœ€ä¼˜çš„æ¯”è¾ƒæ’åº**(Insertion, Bubble, Quicksort, Mergesort, Heapsort)**ç®—æ³•ä¹‹ä¸€**
##### Space complexity
- not a ==in-place== sort,éœ€è¦é¢å¤–ç©ºé—´
- total number of extra array items is $n + \frac{n}{2} + \frac{n}{4} +\dots =2n$
- the space needed for the recursive calls is Î˜(ğ‘›)
##### Pros and Cons
1. advantage:
	Suitable for large-size lists. 
	Suitable for linked lists. (Can sort and merge 2 linked lists to create 1 linked list) Extended sorting.(Partitioning large data and sort it if the space not enough in the memory) 
	Stable.(Works in case of there is data duplicate in two files, the algorithm will preserve their order)
2. Disadvantage:
	éœ€è¦extra space
	No small problem except when we have only one element.
	recursive

- [ ] C++ç»ƒä¹ 

## Part 2
### Quicksort
- the basis for Algorithm quicksort å°±æ˜¯partitionç®—æ³•
- the action of arrangement is also called splitting or partitioning around x, which is called the pivot or splitting element
- [ ] partition ç®—æ³•ä¼ªä»£ç 
- number of element comparisons of algorithm SPLIT is exactly n-1, æ—¶é—´å¤æ‚åº¦ä¸º $\Theta(n)$
- The only extra space the algorithm uses is needed to hold its local variables. Therefore, the space complexity of the algorithm is Î˜(1).
- [ ] quicksort ç®—æ³•ä¼ªä»£ç 
- splitç®—æ³•å’ŒQuicksortç®—æ³•çš„å…³ç³»ç±»ä¼¼mergeå’Œmergesortç®—æ³•å…³ç³»ã€‚ä½†æ˜¯ï¼Œmergingå±äºåˆ†è€Œæ²»ä¹‹çš„combiningéƒ¨åˆ†ï¼Œsplitingå±äºdividingéƒ¨åˆ†
- The combine step in Algorithm quicksort is nonexistent.
#### pivot to be the first element
- [ ] quicksort ä¼ªä»£ç 
- [ ] partition ä¼ªä»£ç 
- [ ] ä¸¾ä¾‹è¯´æ˜
#### pivot to be the last element
- [ ] quicksort ä¼ªä»£ç 
- [ ] partition ä¼ªä»£ç 
- [ ] ä¸¾ä¾‹è¯´æ˜
#### pivot to be the middle element
- [ ] quicksort ä¼ªä»£ç 
- [ ] partition ä¼ªä»£ç 
- [ ] ä¸¾ä¾‹è¯´æ˜
#### time complexity
##### Best case
- Quicksort's best case occurs when the partitions are as evenly balanced as possible: their sizes either are equal or are within 1 of each other.
	ç¬¬ä¸€ç§æ˜¯subarrayå…ƒç´ ä¸ªæ•°ä¸ºå¥‡æ•°ï¼Œpivotæ­£å¥½åœ¨ä¸­é—´after partitioningï¼Œæ¯ä¸ªpartitioné‡Œæœ‰ï¼ˆn-1)/2 ä¸ªå…ƒç´ 
	ç¬¬äºŒç§æƒ…å†µå‘ç”Ÿåœ¨å­æ•°ç»„å…ƒç´ ä¸ªæ•°ä¸ºå¶æ•°ï¼Œone partition has n/2 elements with the other having ğ‘›/ 2 âˆ’1
##### Worst case
- only need to find one situation in which the algorithm exhibits the longest running time for each value of ğ‘›.ä¹Ÿå°±æ˜¯input arrayæ˜¯ä»¥nondecreasingé¡ºåºæ’åˆ—çš„
- total up the partitioning times for each levelï¼š $$n+(n-1)+(n-2)+\dots +2+1 = \frac{n(n+1)}{2}$$
- In big-O notation, quicksort's worst-case running time is $ğ‘‚(ğ‘›^2)$.
- In this case, the **smallest element** will always be chosen as the pivot
- The worst-case running time can be improved to ==Î˜(ğ‘›logğ‘›)== by always selecting the **median** as the pivot.
##### average case
- æœ€åæƒ…å†µä¸å¤šè§
- on the average, the time complexity is $\Theta(n\log{n})$ 
- for simplicityï¼Œæˆ‘ä»¬å‡è®¾input elements are distinct,æ’åˆ—çš„æ¦‚ç‡ç›¸åŒ
- any element will be picked as the pivot is 1/n
- Let ğ¶(ğ‘›) denote the number of comparisons done by the algorithm on the average on an input of size n $$C(n) = (n-1) + \frac{1}{n} \sum\limits_{w=1}^{n}(C(w-1)+C(n-w))$$
  since, $$\frac{1}{n} \sum\limits_{w=1}^{n} C(n-w) = C(n-1) + C(n-2) +\dots +C(0) = \sum\limits_{w=1}^{n}C(w-1)$$
  the equation can be simplified to $$C(n) = (n-1) \frac{2}{n} \sum\limits_{w=1}^{n}C(w-1)$$
 $$C(n) =(n+1)D(n) \approx 1.44n\log{n}$$
- The average number of comparisons performed by algorithm QUICKSORT to sort an array of n elements is $\Theta(n\log{n})$ 
- å°½ç®¡æœ€åæƒ…å†µä¸‹ï¼ŒQuicksortæ—¶é—´å¤æ‚åº¦æ¯”å¾ˆå¤šå…¶ä»–æ’åºç®—æ³•å¤§ï¼Œä½†æ˜¯å®é™…åº”ç”¨æ›´å¿«ï¼Œå› ä¸ºinner loop can be efficiently implemented on most architectures, and in most real-world data
- åŒæ—¶å¯ä»¥é€šè¿‡æ”¹å˜pivotçš„é€‰æ‹©æ¥ä»¥ä¸åŒæ–¹å¼å®æ–½ï¼Œå› æ­¤å¯¹äºæ—¢å®šç±»å‹çš„æ•°æ®ï¼Œæœ€åæƒ…å†µå¾ˆå°‘å‘ç”Ÿ
- ==However==ï¼Œå½“æ•°æ®å¾ˆå¤§ä¸”å‚¨å­˜in external storage, mergesorté€šå¸¸è¢«è®¤ä¸ºæ›´å¥½
##### avoid the worst case
 é€‰æ‹©ä¸€ä¸ªappropriate pivot
	 1. ä¸­é—´å…ƒç´ 
	 2. éšæœºé€‰æ‹©
#### space complexity
- in-place
- index of the pivot item is created in each recursion call
- using a stack
- the space taken depends on the stack size, the maximum size of the stack in the worst case is *n* and in the average and best case is *log n*

#### Pros and Cons
Quicksort is a representative of three types of sorting algorithms: in-place, divide and conquer, and unstable
1. in-place:
	- does not create any copies of the array or any of its subarrays. 
	- It does however require stack memory for all recursive calls it makes

2. divide and conquer:
	 splits the array into smaller arrays until it ends with an empty array, or one that has only one element, before recursively sorting the larger arrays

3. unstable:
	ä¸€ä¸ªstableçš„ç®—æ³•ç›¸åŒå€¼çš„å…ƒç´ åœ¨æ’åºåçš„æ•°ç»„ä¸­å‡ºç°çš„relative orderä¸æ’åºå‰çš„é¡ºåºç›¸åŒï¼ˆå¦‚æœæœ‰ä¸¤ä¸ªå…·æœ‰ç›¸åŒå€¼çš„å…ƒç´ ï¼Œåˆ™åŸå§‹æ•°ç»„ä¸­çš„ç¬¬ä¸€ä¸ªå…ƒç´ å°†åœ¨æ’åºåçš„æ•°ç»„ä¸­é¦–å…ˆå‡ºç°ï¼‰
	ä¸ç¨³å®šçš„æ’åºç®—æ³•ä¸ä¿è¯è¿™ä¸€ç‚¹ï¼Œä½†æ˜¯è¿™ç§æƒ…å†µå¯èƒ½å‘ç”Ÿ

- [ ] C++å†™Quicksort

### Strassenâ€™s Matrix Multiplication Algorithm
- The standard method of matrix multiplication of two (ğ‘› Ã— n) matrices takes $O(n^3)$ operations
- Strassen's algorithm is a Divide-and-Conquer algorithm that is asymptotically faster
- é€šå¸¸$2\times 2$ çŸ©é˜µä¹˜æ³•éœ€è¦8 multiplications & 4 additions
- æœ¬ç®—æ³•using only 7 multiplications and 18 additions
- ä¹˜æ³•æ“ä½œéœ€è¦ $O(n^3)$ï¼ŒåŠ æ³•æ“ä½œéœ€è¦  $O(n^2)$
- multiplications are much more expensive, and it makes sense to trade one multiplication operation for 18 additions
- [ ] The pseudo code to multiply two Matrices
- Parallelizing the Algorithm: æŠŠçŸ©é˜µåˆ†æˆå››ä»½ $$T(n) = 8T(n/2) + O(n^2)$$
  by the master theorem, $T(n) = O(n^{\log_{2}{8}}) = O(n^3)$
- To apply divide and conquer the matrices dimensions should be a power of 2
- å¦‚æœnä¸æ˜¯2çš„powerï¼Œé‚£ä¹ˆone simple modification is to åŠ ä¸Šè¶³å¤Ÿè¡Œå’Œåˆ—çš„0
- Useful in practice for large matrices but would be slower than the fastest known algorithms for extremely large matrices
#### time complexity of matrix multiplication
$$T(n) = \begin{cases}
1&n\le 2 \\
8T(\frac{n}{2})+ n^2 &n >2
\end{cases}$$
 æ ¹æ®ä¸»å®šç†ï¼Œ $T(n) = \Theta(n^3)$
ç”¨åˆ†è€Œæ²»ä¹‹æˆ–for loop in matrix multiplication ä¼šæœ‰ç›¸åŒçš„æ—¶é—´å¤æ‚åº¦
#### How it worksï¼Ÿ
å‡è®¾æˆ‘ä»¬æƒ³multiply two matrices A and B $$A=\begin{pmatrix}
a_{11} &a_{12} \\
a_{21} &a_{22}
\end{pmatrix} \text{ }\text{ }\text{ }\text{ }\text{ }
B=\begin{pmatrix}
b_{11} &b_{12} \\
b_{21} &b_{22}
\end{pmatrix} $$
$$c=\begin{pmatrix}
c_{11} &c_{12} \\
c_{21} &c_{22}
\end{pmatrix}\text{ } = \begin{pmatrix}
a_{11} &a_{12} \\
a_{21} &a_{22}
\end{pmatrix} \text{ } \begin{pmatrix}
b_{11} &b_{12} \\
b_{21} &b_{22}
\end{pmatrix}$$
we first compute the following products:
$$\begin{align}
d_1 &= (a_{11} + a_{22})(b_{11} + b_{22}) \\
d_2 &= (a_{21} + a_{22})b_{11} \\
d_3 &= a_{11}(b_{12} - b_{22}) \\
d_4 &= a_{22}(b_{21} - b_{11}) \\
d_5 &= (a_{11} + a_{22})b_{22} \\
d_6 &= (a_{21} - a_{11})(b_{11} + b_{22}) \\
d_7 &= (a_{12} - a_{22})(b_{21} + b_{22}) 
\end{align}$$
Next, we compute C from the equation $$C=\begin{pmatrix}
d_1 +d_4 -d_5+d_7 &d_3 + d_5 \\
d_2+d_4 &d_1 + d_3 -d_2 +d_6
\end{pmatrix}$$
- [ ] Strassenâ€™s Matrix Multiplication â€“ Pseudocode
#### Time complexity
In order to count the number of scalar operations, let ğ‘ and ğ‘š denote the costs of scalar addition and multiplication, respectively 
the total cost of multiplying two ğ‘› Ã— ğ‘› matrices is governed by the recurrence
$$T(n) = \begin{cases}
m&n=1 \\
7T(\frac{n}{2})+ 18(\frac{n}{2})^2a &n \ge2 \\
\text{ }=mn^{\log{7}}+6an^{\log{7}}-6an^2
\end{cases}$$
 the running time is Î˜($ğ‘›^{\log{7}} = Î˜(ğ‘›^{2.81})$
 - [ ] ä¹ é¢˜ä¸€ä»½

## Part 3
### Tiling a defective chessboard Algorithm
#### Definition
- A chessboard is an ğ‘›Ã—ğ‘› grid, where ğ‘› is a power of 2.
- A defective chessboard is a chessboard that has one unavailable (defective) position.
- A triomino is an L-shaped object that can cover three squares of a chessboard. ï¼ˆ4 orientationsï¼‰

- åœ¨ $n \times n$ çš„defective chessboardé‡Œæ”¾å…¥ $(n^2-1)/3$ çš„triominos so that all $n^2-1$ çš„æ— ç¼ºé™·çš„ä½ç½®è¢«è¦†ç›–
#### given conditions
- We have a chessboard of size ğ‘› Ã— ğ‘›, where ğ‘› = $2^ğ‘˜$. 
- Exactly one square is defective in the chessboard. 
- The tiles(triominoes) are in L-shape i.e., 3 squares. 
#### Objectives
- Cover all the chessboard with L-shape tiles(triominoes), except the defective square

#### ğŸ–Ã—ğŸ– Defective Chessboard
1. One of the cells is defective
2. We divide the chessboard into equal sub-halves.
3. Trick to cover the chessboard with tiles
4. Again creation of defective boxes as we divide the chessboard.
5. As we have finally divided the problem into 2 Ã— 2 board we will put the tiles.
6. The procedure will continue until all the sub board are covered with the tiles.
7. The final chessboard has been covered by all the tiles and only left the defectives which we created.
8. Here we will cover the defectives which we have created as in the last, there should be only one defective left

- [ ] Pseudocode

#### Time complexity
The recurrence equation of the defective chessboard problem is given as follows: $$T(n) = \begin{cases}
c & for \text{ }k = 0 \\
4T(\frac{n}{2}) + c & for \text{ }k > 0 
\end{cases}$$
  æ ¹æ®ä¸»å®šç†ï¼Œthe time complexity of this algorithm is ğ‘¶($ğ’^ğŸ$)

### Finding minimum and Maximum Element Algorithm
#### Problem Description:
Given an array `A[]` of size ğ‘›, you need to find the maximum and minimum element present in the array.
#### This problem can be solved by these methods: 
1. Searching linearly 
2. Divide and Conquer

#### searching linearly
- [ ] ä¼ªä»£ç 
##### Complexity Analysis
- At every step of the loop, we are doing 2 comparisons in the worst case. 
- Total number of comparisons (in worst case) = 2 Ã—(ğ‘› âˆ’1) = 2ğ‘› âˆ’ 2 
- Time complexity = O(n), Space complexity = O(1)
#### Divide and Conquer algorithm
å°†æ•°ç»„äºŒåˆ†ï¼Œåˆ†åˆ«é€’å½’æ‰¾æœ€å¤§æœ€å°å€¼,ç„¶ååˆå¹¶ä¸¤ä¸ªå­é—®é¢˜çš„ç»“æœ
##### time complexity
$$T(n) = \begin{cases}
0 &if \text{ } n=1\\
1 &if \text{ } n=2\\
2T(\frac{n}{2}) + 2 &if \text{ } n >2
\end{cases}$$
using iteration method: $$\begin{align}
T(n)&=2T(\frac{n}{2}) + 2 \\
T(\frac{n}{2})=2(2T(\frac{n}{4})+2)&+2 =2^2T(\frac{n}{2^2})+2^2+2
\end{align}$$
the last element is k, thus: $$\begin{align}
T(n) &= 2^kT(\frac{n}{2^k})+\sum\limits_{i=1}^{k}2^i \\
= &2^kT(\frac{n}{2^k})+2^k-2\\
& \text{ } Let \text{ } 2^k = n \\
T(n) &=nT(1)+n-2 \\
=nT(1) + n-&2=n\times 1+n-2=2n-2
\end{align}$$
Thus, T(n)=O(n)
- [ ] exercise
## Part 4
### Sorting algorithm
- A sorting algorithm is used to arrange elements of an array/list in a specific order.
- Different Sorting Algorithms:
	Bubble Sort 
	Selection Sort 
	Insertion Sort 
	Merge Sort 
	Quicksort 
	Counting Sort 
	Radix Sort 
	Bucket Sort 
	Heap Sort 
	Shell Sort

#### Complexity analysis of different sorting algorithms

- The efficiency of any sorting algorithm is determined by the time complexity and space complexity of the algorithm. 
- **Time** Complexity: Time complexity refers to the ==time== taken by an algorithm to complete its execution with ==respect to the size of the input==. 
- It can be represented in different forms: 
	Big-O notation (O) 
	Omega notation (Î©) 
	Theta notation (Î˜)
- **Space** Complexity: Space complexity refers to the ==total amount of memory== used by the algorithm for a complete execution. It includes both the ==auxiliary memory== and the ==input==. 
- The auxiliary memory is the ==additional space== occupied by the algorithm apart from the input data. 
- Usually, auxiliary memory is considered for calculating the space complexity of an algorithm.

| Sorting Algorithm | Time Complexity-Best | Time Complexity-Worst | Time Complexity-Average | Space Complexity |
| ----------------- | :------------------: | :-------------------: | :---------------------: | :--------------: |
| Bubble Sort       |          n           |         $n^2$         |          $n^2$          |        1         |
| Selection Sort    |        $n^2$         |         $n^2$         |          $n^2$          |        1         |
| Insertion Sort    |          n           |         $n^2$         |          $n^2$          |        1         |
| Merge Sort        |        nlogn         |         nlogn         |          nlogn          |        n         |
| Quicksort         |        nlogn         |         $n^2$         |          nlogn          |       logn       |
| Counting Sort     |         n+k          |          n+k          |           n+k           |       max        |
| Radix Sort        |         n+k          |          n+k          |           n+k           |       max        |
| Bucket Sort       |         n+k          |         $n^2$         |            n            |       n+k        |
| Heap Sort         |        nlogn         |         nlogn         |          nlogn          |        1         |
| Shell Sort        |        nlogn         |         $n^2$         |          nlogn          |        1         |
#### Stability of Sorting Algorithm
- A sorting algorithm is considered stable if the ==two or more items== with the ==same value== maintain the ==same relative positions== even ==after sorting==.

| Sorting Algorithm | Stability |
| ----------------- | :-------: |
| Bubble Sort       |    Yes    |
| Selection Sort    |    No     |
| Insertion Sort    |    Yes    |
| Merge Sort        |    Yes    |
| Quicksort         |    No     |
| Counting Sort     |    Yes    |
| Radix Sort        |    Yes    |
| Bucket Sort       |    Yes    |
| Heap Sort         |    No     |
| Shell Sort        |    No     |
### Radix Sort
- ä¸æ˜¯åˆ†è€Œæ²»ä¹‹ç®—æ³•
- Radix = â€œThe base of a number systemâ€
- In practice, radix sort is fast for large inputs, as well as simple to code and maintain
- Digit-by-digit sort. 
- Hollerithâ€™s original (==bad==) idea: sort on the **most-significant digit** first. 
- ==Good idea==: Sort on the **least-significant digi**t first with an ==auxiliary stable sort==æ¯è½®ä½¿ç”¨ä¸€ä¸ªç¨³å®šæ’åºç®—æ³•
- This sorting method is called radix sort because the information used to sort the keys is a particular radix(base).
- radixå¯ä»¥æ˜¯ä»»ä½•æ•°å­—åŸºåº•æˆ–è€…æ˜¯å­—æ¯
- æ’åºè¿‡ç¨‹ä¸­ï¼Œä¼šç”¨â€œæ¡¶â€ï¼ˆpileï¼‰æ¥åˆ†ç±»ä¸åŒçš„ä½æ•°ã€‚The number of piles is the same as the radix.
	If we were sorting numbers represented in hexadecimal, the number of piles would be 16. 
	If we were sorting alpha keys represented in the English alphabet, the number of piles would be 26
#### Sorting by the most significant digit
- We have two choices. The **first** choice is to sort each list using ==another sorting algorithm== and then ==concatenate the resulting lists== into one sorted list. 
- Observe that in the **worst case** ==all== numbers may have the ==same most significant digit==, which means that they will all end up in one list and the other nine lists will be empty.
- Hence, if the sorting algorithm used runs in Î˜(ğ‘› logğ‘›) time, the running time of this method will be Î˜(ğ‘› logğ‘›). 
- **Another** possibility is to ==recursively sort each list== on digit $d_k$ âˆ’ 1. But this approach will result in the addition of more and more lists, which is undesirable.
#### Sorting by the least significant digit
- If the numbers are first distributed into the lists by their least significant digit, then a very efficient algorithm results.
- This algorithm is commonly known as radix sort. It is straightforward to derive the algorithm using induction on k.
- The implementation of the algorithm does ==not require== any ==other sorting algorithm==. Nor does it require ==recursion==.
#### How does it work?
1. First, distribute the numbers into 10 lists ğ¿0, ğ¿1 , â€¦ , ğ¿9 according to digit ğ‘‘1, so that those numbers with ğ‘‘1 = 0 constitute list ğ¿0, those with ğ‘‘1 = 1 constitute list ğ¿1, and so on. 
2. Next, the lists åˆå¹¶coalesce in the order ğ¿0, ğ¿1 , â€¦, ğ¿9. 
3. Then, they are distributed into 10 lists according to digit ğ‘‘2, coalesced in order, and so on. 
4. After distributing them according to $d_k$ and collecting them in order, all numbers will be sorted.
- [ ] ä¼ªä»£ç 
#### Time and Space complexity
- There are ğ‘˜ passes, and each pass costs Î˜(ğ‘›) time. 
- Thus, the running time of the algorithm is Î˜(ğ‘˜ğ‘›). 
- If ğ‘˜ is constant, the running time is simply Î˜(ğ‘›). 
- The algorithm uses Î˜(ğ‘›) space, as there are 10 lists needed and the overall size of this list is $\Theta(n)$
### Bubble Sort
#### How it works?
- Suppose we are trying to sort the elements in ascending order.
- First Iteration (Compare and Swap) 
	Starting from the first index, compare the first and the second elements. 
	If the first element is greater than the second element, they are swapped. 
	Now, compare the second and the third elements. Swap them if they are not in order.
	The above process goes on until the last element.
#### Optimized Bubble Sort Algorithm
- In the previous algorithm, all the comparisons are made even if the array is already sorted. This increases the execution time. 
- To solve this, we can introduce an extra variable swapped.
- The value of swapped is set true if there occurs swapping of elements. Otherwise, it is set false. After an iteration, if there is no swapping, the value of swapped will be false. This means elements are already sorted and there is no need to perform further iterations. 
- This will reduce the execution time and helps to optimize the bubble sort
- [ ] ä¼ªä»£ç 
- [ ] ç»ƒä¹ é¢˜
#### Complexity
1. Time complexityï¼š
	Best  O(n) â†’If the array is already sorted, then there is no need for sorting
	Worst  $O(n^2)$   â†’If we want to sort in ascending order and the array is in descending order then the worst case occurs.
	Average  $O(n^2)$  â†’It occurs when the elements of the array are in jumbled order (neither ascending nor descending).
2. Space complexityï¼š O(1) 
	Space complexity is O(1) because an extra variable is used for swapping. 
	In the optimized bubble sort algorithm, two extra variables are used. Hence, the space complexity will be O(2).
3. Stabilityï¼š Yes
### Selection Sort
- an in-place comparison-based algorithm
- Initially the sorted part is empty, and the unsorted part is the entire list. 
- The smallest element is selected from the unsorted array and swapped with the leftmost element, and that element becomes a part of the sorted array. 
- This process continues moving unsorted array boundary by one element to the right.
- [ ] ä¼ªä»£ç 
#### complexity
- It is easy to see that the number of element comparisons performed by the algorithm is exactly: $$\sum\limits_{i=1}^{n-1}(n-i)=(n-1)+(n-2)+\dots +(n-n+1)=\sum\limits_{i=1}^{n-1}i=\frac{(n-1)n}{2}$$
- It is also easy to see that the number of element interchanges is between 0 and ğ‘› âˆ’ 1. 
- Since each interchange requires three element assignments, the number of element assignments is between 0 and 3(ğ‘› âˆ’ 1)

1. Time complexityï¼š
	Best $O(n^2)$   â†’It occurs when the array is already sorted
	Worst  $O(n^2)$  â†’If we want to sort in ascending order and the array is in descending order then, the worst case occurs
	Average  $O(n^2)$  â†’It occurs when the elements of the array are in jumbled order (neither ascending nor descending).
2. Space complexityï¼š O(1) 
	an extra variable temp is used
3. Stabilityï¼š No
- [ ] C++ program









